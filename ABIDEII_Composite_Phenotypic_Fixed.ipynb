{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yphFevKUNcZO"
      },
      "source": [
        "# Autism Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "ag5bFE5YNcZR"
      },
      "outputs": [],
      "source": [
        "# Importing the libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelBinarizer, StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, KFold\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from pprint import pprint\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmFfIIPMNcZS"
      },
      "source": [
        "**EDA**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP - 0\n",
        "# # Import the dataset\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "cpn61iWSOzOY",
        "outputId": "21e501dd-cdb4-4635-87a9-2e6d20c758f4"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3c472a0e-fa32-444b-a49e-0c19440c9ac7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3c472a0e-fa32-444b-a49e-0c19440c9ac7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Phenotypic_V1_0b_preprocessed1.csv to Phenotypic_V1_0b_preprocessed1 (5).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "id": "WJU-qg4WNcZS",
        "outputId": "244da28b-18e2-4598-d9e0-2678b07affee"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b4a3986d-a38e-4adc-a69e-8b0c260cc34d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>SUB_ID</th>\n",
              "      <th>X</th>\n",
              "      <th>subject</th>\n",
              "      <th>SITE_ID</th>\n",
              "      <th>FILE_ID</th>\n",
              "      <th>DX_GROUP</th>\n",
              "      <th>DSM_IV_TR</th>\n",
              "      <th>AGE_AT_SCAN</th>\n",
              "      <th>SEX</th>\n",
              "      <th>HANDEDNESS_CATEGORY</th>\n",
              "      <th>HANDEDNESS_SCORES</th>\n",
              "      <th>FIQ</th>\n",
              "      <th>VIQ</th>\n",
              "      <th>PIQ</th>\n",
              "      <th>FIQ_TEST_TYPE</th>\n",
              "      <th>VIQ_TEST_TYPE</th>\n",
              "      <th>PIQ_TEST_TYPE</th>\n",
              "      <th>ADI_R_SOCIAL_TOTAL_A</th>\n",
              "      <th>ADI_R_VERBAL_TOTAL_BV</th>\n",
              "      <th>ADI_RRB_TOTAL_C</th>\n",
              "      <th>ADI_R_ONSET_TOTAL_D</th>\n",
              "      <th>ADI_R_RSRCH_RELIABLE</th>\n",
              "      <th>ADOS_MODULE</th>\n",
              "      <th>ADOS_TOTAL</th>\n",
              "      <th>ADOS_COMM</th>\n",
              "      <th>ADOS_SOCIAL</th>\n",
              "      <th>ADOS_STEREO_BEHAV</th>\n",
              "      <th>ADOS_RSRCH_RELIABLE</th>\n",
              "      <th>ADOS_GOTHAM_SOCAFFECT</th>\n",
              "      <th>ADOS_GOTHAM_RRB</th>\n",
              "      <th>ADOS_GOTHAM_TOTAL</th>\n",
              "      <th>ADOS_GOTHAM_SEVERITY</th>\n",
              "      <th>SRS_VERSION</th>\n",
              "      <th>SRS_RAW_TOTAL</th>\n",
              "      <th>SRS_AWARENESS</th>\n",
              "      <th>SRS_COGNITION</th>\n",
              "      <th>SRS_COMMUNICATION</th>\n",
              "      <th>SRS_MOTIVATION</th>\n",
              "      <th>...</th>\n",
              "      <th>WISC_IV_SIM_SCALED</th>\n",
              "      <th>WISC_IV_VOCAB_SCALED</th>\n",
              "      <th>WISC_IV_INFO_SCALED</th>\n",
              "      <th>WISC_IV_BLK_DSN_SCALED</th>\n",
              "      <th>WISC_IV_PIC_CON_SCALED</th>\n",
              "      <th>WISC_IV_MATRIX_SCALED</th>\n",
              "      <th>WISC_IV_DIGIT_SPAN_SCALED</th>\n",
              "      <th>WISC_IV_LET_NUM_SCALED</th>\n",
              "      <th>WISC_IV_CODING_SCALED</th>\n",
              "      <th>WISC_IV_SYM_SCALED</th>\n",
              "      <th>EYE_STATUS_AT_SCAN</th>\n",
              "      <th>AGE_AT_MPRAGE</th>\n",
              "      <th>BMI</th>\n",
              "      <th>anat_cnr</th>\n",
              "      <th>anat_efc</th>\n",
              "      <th>anat_fber</th>\n",
              "      <th>anat_fwhm</th>\n",
              "      <th>anat_qi1</th>\n",
              "      <th>anat_snr</th>\n",
              "      <th>func_efc</th>\n",
              "      <th>func_fber</th>\n",
              "      <th>func_fwhm</th>\n",
              "      <th>func_dvars</th>\n",
              "      <th>func_outlier</th>\n",
              "      <th>func_quality</th>\n",
              "      <th>func_mean_fd</th>\n",
              "      <th>func_num_fd</th>\n",
              "      <th>func_perc_fd</th>\n",
              "      <th>func_gsr</th>\n",
              "      <th>qc_rater_1</th>\n",
              "      <th>qc_notes_rater_1</th>\n",
              "      <th>qc_anat_rater_2</th>\n",
              "      <th>qc_anat_notes_rater_2</th>\n",
              "      <th>qc_func_rater_2</th>\n",
              "      <th>qc_func_notes_rater_2</th>\n",
              "      <th>qc_anat_rater_3</th>\n",
              "      <th>qc_anat_notes_rater_3</th>\n",
              "      <th>qc_func_rater_3</th>\n",
              "      <th>qc_func_notes_rater_3</th>\n",
              "      <th>SUB_IN_SMP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>50002</td>\n",
              "      <td>1</td>\n",
              "      <td>50002</td>\n",
              "      <td>PITT</td>\n",
              "      <td>no_filename</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>16.77</td>\n",
              "      <td>1</td>\n",
              "      <td>Ambi</td>\n",
              "      <td>NaN</td>\n",
              "      <td>103.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>WASI</td>\n",
              "      <td>WASI</td>\n",
              "      <td>WASI</td>\n",
              "      <td>16.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.201539</td>\n",
              "      <td>1.194664</td>\n",
              "      <td>16.223458</td>\n",
              "      <td>3.878000</td>\n",
              "      <td>0.152711</td>\n",
              "      <td>12.072452</td>\n",
              "      <td>0.613128</td>\n",
              "      <td>45.446551</td>\n",
              "      <td>1.873339</td>\n",
              "      <td>1.054931</td>\n",
              "      <td>0.000641</td>\n",
              "      <td>0.011443</td>\n",
              "      <td>0.116828</td>\n",
              "      <td>8.0</td>\n",
              "      <td>3.980100</td>\n",
              "      <td>0.054346</td>\n",
              "      <td>fail</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>fail</td>\n",
              "      <td>ic-parietal-cerebellum</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>fail</td>\n",
              "      <td>ERROR #24</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>50003</td>\n",
              "      <td>2</td>\n",
              "      <td>50003</td>\n",
              "      <td>PITT</td>\n",
              "      <td>Pitt_0050003</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>24.45</td>\n",
              "      <td>1</td>\n",
              "      <td>R</td>\n",
              "      <td>NaN</td>\n",
              "      <td>124.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>WASI</td>\n",
              "      <td>WASI</td>\n",
              "      <td>WASI</td>\n",
              "      <td>27.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.165701</td>\n",
              "      <td>1.126752</td>\n",
              "      <td>10.460008</td>\n",
              "      <td>4.282238</td>\n",
              "      <td>0.161716</td>\n",
              "      <td>9.241155</td>\n",
              "      <td>0.578301</td>\n",
              "      <td>56.286350</td>\n",
              "      <td>2.012112</td>\n",
              "      <td>0.949857</td>\n",
              "      <td>0.000474</td>\n",
              "      <td>0.031781</td>\n",
              "      <td>0.322092</td>\n",
              "      <td>135.0</td>\n",
              "      <td>67.164179</td>\n",
              "      <td>0.041862</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>50004</td>\n",
              "      <td>3</td>\n",
              "      <td>50004</td>\n",
              "      <td>PITT</td>\n",
              "      <td>Pitt_0050004</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>19.09</td>\n",
              "      <td>1</td>\n",
              "      <td>R</td>\n",
              "      <td>NaN</td>\n",
              "      <td>113.0</td>\n",
              "      <td>108.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>WASI</td>\n",
              "      <td>WASI</td>\n",
              "      <td>WASI</td>\n",
              "      <td>19.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.698144</td>\n",
              "      <td>1.226218</td>\n",
              "      <td>9.725750</td>\n",
              "      <td>3.881684</td>\n",
              "      <td>0.174186</td>\n",
              "      <td>9.323463</td>\n",
              "      <td>0.578960</td>\n",
              "      <td>63.317943</td>\n",
              "      <td>1.866104</td>\n",
              "      <td>1.180605</td>\n",
              "      <td>0.008262</td>\n",
              "      <td>0.014260</td>\n",
              "      <td>0.127745</td>\n",
              "      <td>29.0</td>\n",
              "      <td>14.427861</td>\n",
              "      <td>0.046745</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>50005</td>\n",
              "      <td>4</td>\n",
              "      <td>50005</td>\n",
              "      <td>PITT</td>\n",
              "      <td>Pitt_0050005</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>13.73</td>\n",
              "      <td>2</td>\n",
              "      <td>R</td>\n",
              "      <td>NaN</td>\n",
              "      <td>119.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>WASI</td>\n",
              "      <td>WASI</td>\n",
              "      <td>WASI</td>\n",
              "      <td>23.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.071807</td>\n",
              "      <td>1.256278</td>\n",
              "      <td>11.198226</td>\n",
              "      <td>3.628667</td>\n",
              "      <td>0.119269</td>\n",
              "      <td>10.814200</td>\n",
              "      <td>0.556064</td>\n",
              "      <td>70.800354</td>\n",
              "      <td>1.918278</td>\n",
              "      <td>1.092030</td>\n",
              "      <td>0.001711</td>\n",
              "      <td>0.019205</td>\n",
              "      <td>0.128136</td>\n",
              "      <td>22.0</td>\n",
              "      <td>10.945274</td>\n",
              "      <td>0.027963</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>maybe</td>\n",
              "      <td>ic-parietal-cerebellum</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>50006</td>\n",
              "      <td>5</td>\n",
              "      <td>50006</td>\n",
              "      <td>PITT</td>\n",
              "      <td>Pitt_0050006</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>13.37</td>\n",
              "      <td>1</td>\n",
              "      <td>L</td>\n",
              "      <td>NaN</td>\n",
              "      <td>109.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>WASI</td>\n",
              "      <td>WASI</td>\n",
              "      <td>WASI</td>\n",
              "      <td>13.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.026798</td>\n",
              "      <td>1.407166</td>\n",
              "      <td>6.282055</td>\n",
              "      <td>3.674539</td>\n",
              "      <td>0.130647</td>\n",
              "      <td>10.123574</td>\n",
              "      <td>0.562942</td>\n",
              "      <td>75.364679</td>\n",
              "      <td>2.213873</td>\n",
              "      <td>1.086830</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.006919</td>\n",
              "      <td>0.070143</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.492537</td>\n",
              "      <td>0.054006</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>maybe</td>\n",
              "      <td>ic-parietal slight</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 106 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b4a3986d-a38e-4adc-a69e-8b0c260cc34d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b4a3986d-a38e-4adc-a69e-8b0c260cc34d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b4a3986d-a38e-4adc-a69e-8b0c260cc34d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  ...  qc_func_notes_rater_3  SUB_IN_SMP\n",
              "0           0             1  ...              ERROR #24           1\n",
              "1           1             2  ...                    NaN           1\n",
              "2           2             3  ...                    NaN           1\n",
              "3           3             4  ...                    NaN           0\n",
              "4           4             5  ...                    NaN           1\n",
              "\n",
              "[5 rows x 106 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "columns 106\n",
            "total 117872\n",
            "Total number of records: 1112\n",
            "Individuals diagonised with ASD: 539\n",
            "Individuals not diagonised with ASD: 573\n",
            "Percentage of individuals diagonised with ASD: 48.47%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0933f85f-6172-48f3-b3d0-7f7bd5def8cd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>SUB_ID</th>\n",
              "      <th>X</th>\n",
              "      <th>subject</th>\n",
              "      <th>DX_GROUP</th>\n",
              "      <th>DSM_IV_TR</th>\n",
              "      <th>AGE_AT_SCAN</th>\n",
              "      <th>SEX</th>\n",
              "      <th>HANDEDNESS_SCORES</th>\n",
              "      <th>FIQ</th>\n",
              "      <th>VIQ</th>\n",
              "      <th>PIQ</th>\n",
              "      <th>ADI_R_SOCIAL_TOTAL_A</th>\n",
              "      <th>ADI_R_VERBAL_TOTAL_BV</th>\n",
              "      <th>ADI_RRB_TOTAL_C</th>\n",
              "      <th>ADI_R_ONSET_TOTAL_D</th>\n",
              "      <th>ADI_R_RSRCH_RELIABLE</th>\n",
              "      <th>ADOS_MODULE</th>\n",
              "      <th>ADOS_TOTAL</th>\n",
              "      <th>ADOS_COMM</th>\n",
              "      <th>ADOS_SOCIAL</th>\n",
              "      <th>ADOS_STEREO_BEHAV</th>\n",
              "      <th>ADOS_RSRCH_RELIABLE</th>\n",
              "      <th>ADOS_GOTHAM_SOCAFFECT</th>\n",
              "      <th>ADOS_GOTHAM_RRB</th>\n",
              "      <th>ADOS_GOTHAM_TOTAL</th>\n",
              "      <th>ADOS_GOTHAM_SEVERITY</th>\n",
              "      <th>SRS_VERSION</th>\n",
              "      <th>SRS_RAW_TOTAL</th>\n",
              "      <th>SRS_AWARENESS</th>\n",
              "      <th>SRS_COGNITION</th>\n",
              "      <th>SRS_COMMUNICATION</th>\n",
              "      <th>SRS_MOTIVATION</th>\n",
              "      <th>SRS_MANNERISMS</th>\n",
              "      <th>SCQ_TOTAL</th>\n",
              "      <th>AQ_TOTAL</th>\n",
              "      <th>CURRENT_MED_STATUS</th>\n",
              "      <th>OFF_STIMULANTS_AT_SCAN</th>\n",
              "      <th>VINELAND_RECEPTIVE_V_SCALED</th>\n",
              "      <th>...</th>\n",
              "      <th>VINELAND_PLAY_V_SCALED</th>\n",
              "      <th>VINELAND_COPING_V_SCALED</th>\n",
              "      <th>VINELAND_SOCIAL_STANDARD</th>\n",
              "      <th>VINELAND_SUM_SCORES</th>\n",
              "      <th>VINELAND_ABC_STANDARD</th>\n",
              "      <th>VINELAND_INFORMANT</th>\n",
              "      <th>WISC_IV_VCI</th>\n",
              "      <th>WISC_IV_PRI</th>\n",
              "      <th>WISC_IV_WMI</th>\n",
              "      <th>WISC_IV_PSI</th>\n",
              "      <th>WISC_IV_SIM_SCALED</th>\n",
              "      <th>WISC_IV_VOCAB_SCALED</th>\n",
              "      <th>WISC_IV_INFO_SCALED</th>\n",
              "      <th>WISC_IV_BLK_DSN_SCALED</th>\n",
              "      <th>WISC_IV_PIC_CON_SCALED</th>\n",
              "      <th>WISC_IV_MATRIX_SCALED</th>\n",
              "      <th>WISC_IV_DIGIT_SPAN_SCALED</th>\n",
              "      <th>WISC_IV_LET_NUM_SCALED</th>\n",
              "      <th>WISC_IV_CODING_SCALED</th>\n",
              "      <th>WISC_IV_SYM_SCALED</th>\n",
              "      <th>EYE_STATUS_AT_SCAN</th>\n",
              "      <th>AGE_AT_MPRAGE</th>\n",
              "      <th>BMI</th>\n",
              "      <th>anat_cnr</th>\n",
              "      <th>anat_efc</th>\n",
              "      <th>anat_fber</th>\n",
              "      <th>anat_fwhm</th>\n",
              "      <th>anat_qi1</th>\n",
              "      <th>anat_snr</th>\n",
              "      <th>func_efc</th>\n",
              "      <th>func_fber</th>\n",
              "      <th>func_fwhm</th>\n",
              "      <th>func_dvars</th>\n",
              "      <th>func_outlier</th>\n",
              "      <th>func_quality</th>\n",
              "      <th>func_mean_fd</th>\n",
              "      <th>func_num_fd</th>\n",
              "      <th>func_perc_fd</th>\n",
              "      <th>func_gsr</th>\n",
              "      <th>SUB_IN_SMP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1112.000000</td>\n",
              "      <td>1112.000000</td>\n",
              "      <td>1112.000000</td>\n",
              "      <td>1112.000000</td>\n",
              "      <td>1112.000000</td>\n",
              "      <td>1112.000000</td>\n",
              "      <td>1040.000000</td>\n",
              "      <td>1112.000000</td>\n",
              "      <td>1112.000000</td>\n",
              "      <td>364.000000</td>\n",
              "      <td>1040.000000</td>\n",
              "      <td>917.000000</td>\n",
              "      <td>931.000000</td>\n",
              "      <td>378.000000</td>\n",
              "      <td>379.000000</td>\n",
              "      <td>378.000000</td>\n",
              "      <td>297.000000</td>\n",
              "      <td>391.000000</td>\n",
              "      <td>507.000000</td>\n",
              "      <td>415.000000</td>\n",
              "      <td>390.000000</td>\n",
              "      <td>391.000000</td>\n",
              "      <td>331.000000</td>\n",
              "      <td>376.000000</td>\n",
              "      <td>265.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>273.000000</td>\n",
              "      <td>273.000000</td>\n",
              "      <td>231.000000</td>\n",
              "      <td>365.000000</td>\n",
              "      <td>64.000000</td>\n",
              "      <td>64.000000</td>\n",
              "      <td>64.000000</td>\n",
              "      <td>64.00000</td>\n",
              "      <td>64.000000</td>\n",
              "      <td>124.000000</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>809.000000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>1112.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>1099.000000</td>\n",
              "      <td>1099.000000</td>\n",
              "      <td>1099.000000</td>\n",
              "      <td>1099.000000</td>\n",
              "      <td>1099.000000</td>\n",
              "      <td>1099.000000</td>\n",
              "      <td>1099.000000</td>\n",
              "      <td>1099.000000</td>\n",
              "      <td>1099.000000</td>\n",
              "      <td>1099.000000</td>\n",
              "      <td>1099.000000</td>\n",
              "      <td>1099.000000</td>\n",
              "      <td>1099.000000</td>\n",
              "      <td>1099.000000</td>\n",
              "      <td>1099.000000</td>\n",
              "      <td>1099.000000</td>\n",
              "      <td>1112.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>555.500000</td>\n",
              "      <td>556.500000</td>\n",
              "      <td>50752.684353</td>\n",
              "      <td>556.500000</td>\n",
              "      <td>50752.684353</td>\n",
              "      <td>1.515288</td>\n",
              "      <td>0.639423</td>\n",
              "      <td>17.048864</td>\n",
              "      <td>1.147482</td>\n",
              "      <td>60.567225</td>\n",
              "      <td>108.380962</td>\n",
              "      <td>107.812432</td>\n",
              "      <td>106.625134</td>\n",
              "      <td>19.767196</td>\n",
              "      <td>15.791557</td>\n",
              "      <td>6.084656</td>\n",
              "      <td>3.218855</td>\n",
              "      <td>0.910486</td>\n",
              "      <td>3.341223</td>\n",
              "      <td>11.062651</td>\n",
              "      <td>3.553846</td>\n",
              "      <td>7.539642</td>\n",
              "      <td>1.921450</td>\n",
              "      <td>0.928191</td>\n",
              "      <td>9.071698</td>\n",
              "      <td>2.840741</td>\n",
              "      <td>11.912088</td>\n",
              "      <td>6.758242</td>\n",
              "      <td>1.199134</td>\n",
              "      <td>57.882192</td>\n",
              "      <td>7.546875</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>17.406250</td>\n",
              "      <td>10.37500</td>\n",
              "      <td>8.937500</td>\n",
              "      <td>11.596774</td>\n",
              "      <td>21.155172</td>\n",
              "      <td>0.168109</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>12.883929</td>\n",
              "      <td>...</td>\n",
              "      <td>12.616071</td>\n",
              "      <td>14.794643</td>\n",
              "      <td>89.089286</td>\n",
              "      <td>272.035714</td>\n",
              "      <td>89.187500</td>\n",
              "      <td>1.035714</td>\n",
              "      <td>112.109091</td>\n",
              "      <td>106.618182</td>\n",
              "      <td>100.690909</td>\n",
              "      <td>96.690909</td>\n",
              "      <td>12.381818</td>\n",
              "      <td>11.818182</td>\n",
              "      <td>12.127273</td>\n",
              "      <td>11.636364</td>\n",
              "      <td>9.909091</td>\n",
              "      <td>11.545455</td>\n",
              "      <td>10.127273</td>\n",
              "      <td>10.363636</td>\n",
              "      <td>8.763636</td>\n",
              "      <td>9.890909</td>\n",
              "      <td>1.312050</td>\n",
              "      <td>12.951700</td>\n",
              "      <td>20.519255</td>\n",
              "      <td>11.641527</td>\n",
              "      <td>2.079628</td>\n",
              "      <td>72.804971</td>\n",
              "      <td>3.558793</td>\n",
              "      <td>0.072209</td>\n",
              "      <td>48.185793</td>\n",
              "      <td>0.508707</td>\n",
              "      <td>100.548498</td>\n",
              "      <td>2.116388</td>\n",
              "      <td>1.103531</td>\n",
              "      <td>0.005525</td>\n",
              "      <td>0.012778</td>\n",
              "      <td>0.131138</td>\n",
              "      <td>27.218380</td>\n",
              "      <td>12.952826</td>\n",
              "      <td>0.028416</td>\n",
              "      <td>0.686151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>321.151055</td>\n",
              "      <td>321.151055</td>\n",
              "      <td>447.680666</td>\n",
              "      <td>321.151055</td>\n",
              "      <td>447.680666</td>\n",
              "      <td>0.499991</td>\n",
              "      <td>0.828849</td>\n",
              "      <td>8.036419</td>\n",
              "      <td>0.354745</td>\n",
              "      <td>47.495811</td>\n",
              "      <td>15.072062</td>\n",
              "      <td>16.244305</td>\n",
              "      <td>15.339533</td>\n",
              "      <td>5.527245</td>\n",
              "      <td>4.633822</td>\n",
              "      <td>2.577855</td>\n",
              "      <td>1.261132</td>\n",
              "      <td>0.285850</td>\n",
              "      <td>0.498948</td>\n",
              "      <td>4.638208</td>\n",
              "      <td>1.750776</td>\n",
              "      <td>3.337870</td>\n",
              "      <td>1.619912</td>\n",
              "      <td>0.258515</td>\n",
              "      <td>4.223688</td>\n",
              "      <td>1.857519</td>\n",
              "      <td>5.321948</td>\n",
              "      <td>2.486762</td>\n",
              "      <td>0.400216</td>\n",
              "      <td>42.799043</td>\n",
              "      <td>4.120909</td>\n",
              "      <td>7.690439</td>\n",
              "      <td>12.852915</td>\n",
              "      <td>7.54142</td>\n",
              "      <td>7.028005</td>\n",
              "      <td>9.602608</td>\n",
              "      <td>11.575536</td>\n",
              "      <td>0.374194</td>\n",
              "      <td>0.474713</td>\n",
              "      <td>3.256997</td>\n",
              "      <td>...</td>\n",
              "      <td>4.020769</td>\n",
              "      <td>3.785078</td>\n",
              "      <td>23.033291</td>\n",
              "      <td>58.131093</td>\n",
              "      <td>19.839233</td>\n",
              "      <td>0.186411</td>\n",
              "      <td>17.142834</td>\n",
              "      <td>13.967543</td>\n",
              "      <td>15.143600</td>\n",
              "      <td>14.309894</td>\n",
              "      <td>3.094146</td>\n",
              "      <td>3.646870</td>\n",
              "      <td>3.103490</td>\n",
              "      <td>3.014555</td>\n",
              "      <td>3.187201</td>\n",
              "      <td>2.651472</td>\n",
              "      <td>3.232099</td>\n",
              "      <td>2.914610</td>\n",
              "      <td>2.821634</td>\n",
              "      <td>2.753266</td>\n",
              "      <td>0.463539</td>\n",
              "      <td>2.193555</td>\n",
              "      <td>5.244446</td>\n",
              "      <td>5.612932</td>\n",
              "      <td>11.435278</td>\n",
              "      <td>155.033640</td>\n",
              "      <td>0.676312</td>\n",
              "      <td>0.052283</td>\n",
              "      <td>282.736653</td>\n",
              "      <td>0.059832</td>\n",
              "      <td>43.552970</td>\n",
              "      <td>0.345282</td>\n",
              "      <td>0.117803</td>\n",
              "      <td>0.006464</td>\n",
              "      <td>0.011212</td>\n",
              "      <td>0.157728</td>\n",
              "      <td>42.757583</td>\n",
              "      <td>18.216671</td>\n",
              "      <td>0.021070</td>\n",
              "      <td>0.464265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>50002.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>50002.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.470000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-100.000000</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>42.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>128.000000</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>63.000000</td>\n",
              "      <td>79.000000</td>\n",
              "      <td>56.000000</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.290000</td>\n",
              "      <td>6.530000</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>-217.560433</td>\n",
              "      <td>-3.123743</td>\n",
              "      <td>2.533930</td>\n",
              "      <td>0.000451</td>\n",
              "      <td>0.001400</td>\n",
              "      <td>0.352391</td>\n",
              "      <td>33.836513</td>\n",
              "      <td>1.581108</td>\n",
              "      <td>0.709671</td>\n",
              "      <td>0.000182</td>\n",
              "      <td>0.001693</td>\n",
              "      <td>0.016117</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.015812</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>277.750000</td>\n",
              "      <td>278.750000</td>\n",
              "      <td>50354.750000</td>\n",
              "      <td>278.750000</td>\n",
              "      <td>50354.750000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>11.658575</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>52.750000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>98.000000</td>\n",
              "      <td>97.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>4.750000</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>3.75000</td>\n",
              "      <td>2.750000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>75.000000</td>\n",
              "      <td>228.750000</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>97.000000</td>\n",
              "      <td>92.500000</td>\n",
              "      <td>84.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>9.500000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>7.500000</td>\n",
              "      <td>9.500000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>11.265000</td>\n",
              "      <td>17.422500</td>\n",
              "      <td>8.613273</td>\n",
              "      <td>0.755764</td>\n",
              "      <td>3.898699</td>\n",
              "      <td>3.088846</td>\n",
              "      <td>0.040015</td>\n",
              "      <td>12.262756</td>\n",
              "      <td>0.471226</td>\n",
              "      <td>69.047209</td>\n",
              "      <td>1.862772</td>\n",
              "      <td>1.026977</td>\n",
              "      <td>0.001124</td>\n",
              "      <td>0.006333</td>\n",
              "      <td>0.050888</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.995025</td>\n",
              "      <td>0.013075</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>555.500000</td>\n",
              "      <td>556.500000</td>\n",
              "      <td>50724.500000</td>\n",
              "      <td>556.500000</td>\n",
              "      <td>50724.500000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14.660000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>75.000000</td>\n",
              "      <td>109.000000</td>\n",
              "      <td>108.000000</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>51.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>7.500000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>18.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>14.500000</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>269.500000</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>116.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>102.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>12.945000</td>\n",
              "      <td>19.945000</td>\n",
              "      <td>10.966648</td>\n",
              "      <td>1.675090</td>\n",
              "      <td>10.341742</td>\n",
              "      <td>3.401205</td>\n",
              "      <td>0.060916</td>\n",
              "      <td>15.601842</td>\n",
              "      <td>0.517312</td>\n",
              "      <td>87.681334</td>\n",
              "      <td>2.004123</td>\n",
              "      <td>1.080727</td>\n",
              "      <td>0.003441</td>\n",
              "      <td>0.010043</td>\n",
              "      <td>0.083163</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>4.972376</td>\n",
              "      <td>0.031102</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>833.250000</td>\n",
              "      <td>834.250000</td>\n",
              "      <td>51153.250000</td>\n",
              "      <td>834.250000</td>\n",
              "      <td>51153.250000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>20.085000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>91.000000</td>\n",
              "      <td>119.000000</td>\n",
              "      <td>119.000000</td>\n",
              "      <td>117.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>93.000000</td>\n",
              "      <td>10.250000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>16.00000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>106.250000</td>\n",
              "      <td>308.000000</td>\n",
              "      <td>102.250000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>125.000000</td>\n",
              "      <td>116.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>107.500000</td>\n",
              "      <td>14.500000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>10.500000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>14.462500</td>\n",
              "      <td>22.645000</td>\n",
              "      <td>13.181021</td>\n",
              "      <td>3.197335</td>\n",
              "      <td>86.028008</td>\n",
              "      <td>3.831771</td>\n",
              "      <td>0.093534</td>\n",
              "      <td>21.528386</td>\n",
              "      <td>0.550617</td>\n",
              "      <td>125.779269</td>\n",
              "      <td>2.328370</td>\n",
              "      <td>1.172068</td>\n",
              "      <td>0.006977</td>\n",
              "      <td>0.015189</td>\n",
              "      <td>0.143479</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>16.977985</td>\n",
              "      <td>0.042821</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1111.000000</td>\n",
              "      <td>1112.000000</td>\n",
              "      <td>51607.000000</td>\n",
              "      <td>1112.000000</td>\n",
              "      <td>51607.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>64.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>148.000000</td>\n",
              "      <td>180.000000</td>\n",
              "      <td>157.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>164.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>51.000000</td>\n",
              "      <td>29.00000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>42.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>138.000000</td>\n",
              "      <td>469.000000</td>\n",
              "      <td>131.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>144.000000</td>\n",
              "      <td>137.000000</td>\n",
              "      <td>126.000000</td>\n",
              "      <td>131.000000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>17.940000</td>\n",
              "      <td>35.430000</td>\n",
              "      <td>51.691800</td>\n",
              "      <td>33.318370</td>\n",
              "      <td>1734.146859</td>\n",
              "      <td>5.938324</td>\n",
              "      <td>0.259048</td>\n",
              "      <td>5957.198529</td>\n",
              "      <td>0.668279</td>\n",
              "      <td>326.544102</td>\n",
              "      <td>3.753481</td>\n",
              "      <td>1.571101</td>\n",
              "      <td>0.080158</td>\n",
              "      <td>0.098614</td>\n",
              "      <td>1.434912</td>\n",
              "      <td>288.000000</td>\n",
              "      <td>91.735537</td>\n",
              "      <td>0.104225</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows Ã— 88 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0933f85f-6172-48f3-b3d0-7f7bd5def8cd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0933f85f-6172-48f3-b3d0-7f7bd5def8cd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0933f85f-6172-48f3-b3d0-7f7bd5def8cd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        Unnamed: 0  Unnamed: 0.1  ...     func_gsr   SUB_IN_SMP\n",
              "count  1112.000000   1112.000000  ...  1099.000000  1112.000000\n",
              "mean    555.500000    556.500000  ...     0.028416     0.686151\n",
              "std     321.151055    321.151055  ...     0.021070     0.464265\n",
              "min       0.000000      1.000000  ...    -0.015812     0.000000\n",
              "25%     277.750000    278.750000  ...     0.013075     0.000000\n",
              "50%     555.500000    556.500000  ...     0.031102     1.000000\n",
              "75%     833.250000    834.250000  ...     0.042821     1.000000\n",
              "max    1111.000000   1112.000000  ...     0.104225     1.000000\n",
              "\n",
              "[8 rows x 88 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ],
      "source": [
        "import io\n",
        "df_pheno = pd.read_csv(io.BytesIO(uploaded['Phenotypic_V1_0b_preprocessed1.csv']))\n",
        "# Dataset is now stored in a Pandas Dataframe\n",
        "\n",
        "#df_pheno = pd.read_csv('Phenotypic_V1_0b_preprocessed1.csv', encoding='unicode_escape')\n",
        "display(df_pheno.head(n=5))\n",
        "#Total Coulmns\n",
        "columns_pheno = df_pheno.columns\n",
        "print(\"columns\", columns_pheno.size)\n",
        "print(\"total\", df_pheno.size)\n",
        "\n",
        "# Total number of records\n",
        "n_records = len(df_pheno.index)\n",
        "\n",
        "# Number of records where individual's with ASD\n",
        "n_asd_yes = len(df_pheno[df_pheno['DX_GROUP'] == 1])\n",
        "\n",
        "# Number of records where individual's with no ASD\n",
        "n_asd_no = len(df_pheno[df_pheno['DX_GROUP'] == 2])\n",
        "\n",
        "# Percentage of individuals whose are with ASD\n",
        "yes_percent = float(n_asd_yes) / n_records *100\n",
        "\n",
        "\n",
        "\n",
        "# Print the results\n",
        "print(\"Total number of records:\", n_records)\n",
        "print (\"Individuals diagonised with ASD: {}\".format(n_asd_yes))\n",
        "print (\"Individuals not diagonised with ASD: {}\".format(n_asd_no))\n",
        "print (\"Percentage of individuals diagonised with ASD: {:.2f}%\".format(yes_percent))\n",
        "\n",
        "df_pheno.describe()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "0N7B8XgjNcZU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f605c7d-dff9-43ac-9897-2201dba6596a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "males: 948\n",
            "males_autistic: 474\n",
            "males_control: 474\n",
            "females: 164\n",
            "females_autistic: 65\n",
            "females_controls: 99\n"
          ]
        }
      ],
      "source": [
        "#Total Males\n",
        "males = df_pheno[df_pheno['SEX'] == 1]\n",
        "print(\"males:\", len(males))\n",
        "## Autistic Males\n",
        "males_autistic = males[males['DX_GROUP'] == 1]\n",
        "print(\"males_autistic:\", len(males_autistic))\n",
        "## Control Males\n",
        "males_control = males[males['DX_GROUP'] == 2]\n",
        "print(\"males_control:\", len(males_control))\n",
        "\n",
        "#Total Females\n",
        "females = df_pheno[df_pheno['SEX'] == 2]\n",
        "print(\"females:\", len(females))\n",
        "## Autistic Females\n",
        "females_autistic = females[females['DX_GROUP'] == 1]\n",
        "print(\"females_autistic:\", len(females_autistic))\n",
        "## Control Females\n",
        "females_controls = females[females['DX_GROUP'] == 2]\n",
        "print(\"females_controls:\", len(females_controls))\n",
        "\n",
        "# # Creating dataset with selected features only\n",
        "# pheno_data = dataset.loc[:,['SUB_ID','SEX','HANDEDNESS_CATEGORY','AGE_AT_SCAN','FIQ','VIQ','PIQ','ADOS_TOTAL',\n",
        "#                             'ADI_R_SOCIAL_TOTAL_A','ADI_R_VERBAL_TOTAL_BV','ADI_RRB_TOTAL_C','ADI_R_ONSET_TOTAL_D',\n",
        "#                             'SUB_IN_SMP','CURRENT_MED_STATUS','DSM_IV_TR']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu0Y91QoNcZV"
      },
      "source": [
        "** Filtering **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "hghxzwtwNcZV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "ea5a3747-e292-4267-f2b9-2c010bc3b8b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "females 149\n",
            "males 833\n",
            "females_autistic 57\n",
            "males_autistic 401\n",
            "males_control 432\n",
            "Total: 982\n",
            "ASD: 458\n",
            "ASD percent 41.18705035971223\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e3b7a926-99a7-41d4-b323-8762ac452978\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>SUB_ID</th>\n",
              "      <th>X</th>\n",
              "      <th>subject</th>\n",
              "      <th>SITE_ID</th>\n",
              "      <th>FILE_ID</th>\n",
              "      <th>DX_GROUP</th>\n",
              "      <th>DSM_IV_TR</th>\n",
              "      <th>AGE_AT_SCAN</th>\n",
              "      <th>SEX</th>\n",
              "      <th>HANDEDNESS_CATEGORY</th>\n",
              "      <th>HANDEDNESS_SCORES</th>\n",
              "      <th>FIQ</th>\n",
              "      <th>VIQ</th>\n",
              "      <th>PIQ</th>\n",
              "      <th>FIQ_TEST_TYPE</th>\n",
              "      <th>VIQ_TEST_TYPE</th>\n",
              "      <th>PIQ_TEST_TYPE</th>\n",
              "      <th>ADI_R_SOCIAL_TOTAL_A</th>\n",
              "      <th>ADI_R_VERBAL_TOTAL_BV</th>\n",
              "      <th>ADI_RRB_TOTAL_C</th>\n",
              "      <th>ADI_R_ONSET_TOTAL_D</th>\n",
              "      <th>ADI_R_RSRCH_RELIABLE</th>\n",
              "      <th>ADOS_MODULE</th>\n",
              "      <th>ADOS_TOTAL</th>\n",
              "      <th>ADOS_COMM</th>\n",
              "      <th>ADOS_SOCIAL</th>\n",
              "      <th>ADOS_STEREO_BEHAV</th>\n",
              "      <th>ADOS_RSRCH_RELIABLE</th>\n",
              "      <th>ADOS_GOTHAM_SOCAFFECT</th>\n",
              "      <th>ADOS_GOTHAM_RRB</th>\n",
              "      <th>ADOS_GOTHAM_TOTAL</th>\n",
              "      <th>ADOS_GOTHAM_SEVERITY</th>\n",
              "      <th>SRS_VERSION</th>\n",
              "      <th>SRS_RAW_TOTAL</th>\n",
              "      <th>SRS_AWARENESS</th>\n",
              "      <th>SRS_COGNITION</th>\n",
              "      <th>SRS_COMMUNICATION</th>\n",
              "      <th>SRS_MOTIVATION</th>\n",
              "      <th>...</th>\n",
              "      <th>WISC_IV_SIM_SCALED</th>\n",
              "      <th>WISC_IV_VOCAB_SCALED</th>\n",
              "      <th>WISC_IV_INFO_SCALED</th>\n",
              "      <th>WISC_IV_BLK_DSN_SCALED</th>\n",
              "      <th>WISC_IV_PIC_CON_SCALED</th>\n",
              "      <th>WISC_IV_MATRIX_SCALED</th>\n",
              "      <th>WISC_IV_DIGIT_SPAN_SCALED</th>\n",
              "      <th>WISC_IV_LET_NUM_SCALED</th>\n",
              "      <th>WISC_IV_CODING_SCALED</th>\n",
              "      <th>WISC_IV_SYM_SCALED</th>\n",
              "      <th>EYE_STATUS_AT_SCAN</th>\n",
              "      <th>AGE_AT_MPRAGE</th>\n",
              "      <th>BMI</th>\n",
              "      <th>anat_cnr</th>\n",
              "      <th>anat_efc</th>\n",
              "      <th>anat_fber</th>\n",
              "      <th>anat_fwhm</th>\n",
              "      <th>anat_qi1</th>\n",
              "      <th>anat_snr</th>\n",
              "      <th>func_efc</th>\n",
              "      <th>func_fber</th>\n",
              "      <th>func_fwhm</th>\n",
              "      <th>func_dvars</th>\n",
              "      <th>func_outlier</th>\n",
              "      <th>func_quality</th>\n",
              "      <th>func_mean_fd</th>\n",
              "      <th>func_num_fd</th>\n",
              "      <th>func_perc_fd</th>\n",
              "      <th>func_gsr</th>\n",
              "      <th>qc_rater_1</th>\n",
              "      <th>qc_notes_rater_1</th>\n",
              "      <th>qc_anat_rater_2</th>\n",
              "      <th>qc_anat_notes_rater_2</th>\n",
              "      <th>qc_func_rater_2</th>\n",
              "      <th>qc_func_notes_rater_2</th>\n",
              "      <th>qc_anat_rater_3</th>\n",
              "      <th>qc_anat_notes_rater_3</th>\n",
              "      <th>qc_func_rater_3</th>\n",
              "      <th>qc_func_notes_rater_3</th>\n",
              "      <th>SUB_IN_SMP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>50002</td>\n",
              "      <td>1</td>\n",
              "      <td>50002</td>\n",
              "      <td>PITT</td>\n",
              "      <td>no_filename</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>16.77</td>\n",
              "      <td>1</td>\n",
              "      <td>Ambi</td>\n",
              "      <td>NaN</td>\n",
              "      <td>103.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>WASI</td>\n",
              "      <td>WASI</td>\n",
              "      <td>WASI</td>\n",
              "      <td>16.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.201539</td>\n",
              "      <td>1.194664</td>\n",
              "      <td>16.223458</td>\n",
              "      <td>3.878000</td>\n",
              "      <td>0.152711</td>\n",
              "      <td>12.072452</td>\n",
              "      <td>0.613128</td>\n",
              "      <td>45.446551</td>\n",
              "      <td>1.873339</td>\n",
              "      <td>1.054931</td>\n",
              "      <td>0.000641</td>\n",
              "      <td>0.011443</td>\n",
              "      <td>0.116828</td>\n",
              "      <td>8.0</td>\n",
              "      <td>3.980100</td>\n",
              "      <td>0.054346</td>\n",
              "      <td>fail</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>fail</td>\n",
              "      <td>ic-parietal-cerebellum</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>fail</td>\n",
              "      <td>ERROR #24</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>50003</td>\n",
              "      <td>2</td>\n",
              "      <td>50003</td>\n",
              "      <td>PITT</td>\n",
              "      <td>Pitt_0050003</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>24.45</td>\n",
              "      <td>1</td>\n",
              "      <td>R</td>\n",
              "      <td>NaN</td>\n",
              "      <td>124.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>WASI</td>\n",
              "      <td>WASI</td>\n",
              "      <td>WASI</td>\n",
              "      <td>27.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.165701</td>\n",
              "      <td>1.126752</td>\n",
              "      <td>10.460008</td>\n",
              "      <td>4.282238</td>\n",
              "      <td>0.161716</td>\n",
              "      <td>9.241155</td>\n",
              "      <td>0.578301</td>\n",
              "      <td>56.286350</td>\n",
              "      <td>2.012112</td>\n",
              "      <td>0.949857</td>\n",
              "      <td>0.000474</td>\n",
              "      <td>0.031781</td>\n",
              "      <td>0.322092</td>\n",
              "      <td>135.0</td>\n",
              "      <td>67.164179</td>\n",
              "      <td>0.041862</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>50004</td>\n",
              "      <td>3</td>\n",
              "      <td>50004</td>\n",
              "      <td>PITT</td>\n",
              "      <td>Pitt_0050004</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>19.09</td>\n",
              "      <td>1</td>\n",
              "      <td>R</td>\n",
              "      <td>NaN</td>\n",
              "      <td>113.0</td>\n",
              "      <td>108.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>WASI</td>\n",
              "      <td>WASI</td>\n",
              "      <td>WASI</td>\n",
              "      <td>19.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.698144</td>\n",
              "      <td>1.226218</td>\n",
              "      <td>9.725750</td>\n",
              "      <td>3.881684</td>\n",
              "      <td>0.174186</td>\n",
              "      <td>9.323463</td>\n",
              "      <td>0.578960</td>\n",
              "      <td>63.317943</td>\n",
              "      <td>1.866104</td>\n",
              "      <td>1.180605</td>\n",
              "      <td>0.008262</td>\n",
              "      <td>0.014260</td>\n",
              "      <td>0.127745</td>\n",
              "      <td>29.0</td>\n",
              "      <td>14.427861</td>\n",
              "      <td>0.046745</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>50005</td>\n",
              "      <td>4</td>\n",
              "      <td>50005</td>\n",
              "      <td>PITT</td>\n",
              "      <td>Pitt_0050005</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>13.73</td>\n",
              "      <td>2</td>\n",
              "      <td>R</td>\n",
              "      <td>NaN</td>\n",
              "      <td>119.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>WASI</td>\n",
              "      <td>WASI</td>\n",
              "      <td>WASI</td>\n",
              "      <td>23.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.071807</td>\n",
              "      <td>1.256278</td>\n",
              "      <td>11.198226</td>\n",
              "      <td>3.628667</td>\n",
              "      <td>0.119269</td>\n",
              "      <td>10.814200</td>\n",
              "      <td>0.556064</td>\n",
              "      <td>70.800354</td>\n",
              "      <td>1.918278</td>\n",
              "      <td>1.092030</td>\n",
              "      <td>0.001711</td>\n",
              "      <td>0.019205</td>\n",
              "      <td>0.128136</td>\n",
              "      <td>22.0</td>\n",
              "      <td>10.945274</td>\n",
              "      <td>0.027963</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>maybe</td>\n",
              "      <td>ic-parietal-cerebellum</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>50006</td>\n",
              "      <td>5</td>\n",
              "      <td>50006</td>\n",
              "      <td>PITT</td>\n",
              "      <td>Pitt_0050006</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>13.37</td>\n",
              "      <td>1</td>\n",
              "      <td>L</td>\n",
              "      <td>NaN</td>\n",
              "      <td>109.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>WASI</td>\n",
              "      <td>WASI</td>\n",
              "      <td>WASI</td>\n",
              "      <td>13.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.026798</td>\n",
              "      <td>1.407166</td>\n",
              "      <td>6.282055</td>\n",
              "      <td>3.674539</td>\n",
              "      <td>0.130647</td>\n",
              "      <td>10.123574</td>\n",
              "      <td>0.562942</td>\n",
              "      <td>75.364679</td>\n",
              "      <td>2.213873</td>\n",
              "      <td>1.086830</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.006919</td>\n",
              "      <td>0.070143</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.492537</td>\n",
              "      <td>0.054006</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>maybe</td>\n",
              "      <td>ic-parietal slight</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 106 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3b7a926-99a7-41d4-b323-8762ac452978')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e3b7a926-99a7-41d4-b323-8762ac452978 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e3b7a926-99a7-41d4-b323-8762ac452978');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  ...  qc_func_notes_rater_3  SUB_IN_SMP\n",
              "0           0             1  ...              ERROR #24           1\n",
              "1           1             2  ...                    NaN           1\n",
              "2           2             3  ...                    NaN           1\n",
              "3           3             4  ...                    NaN           0\n",
              "4           4             5  ...                    NaN           1\n",
              "\n",
              "[5 rows x 106 columns]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Filter Data Set\n",
        "#selection = filter_data[(filter_data['AGE_AT_SCAN '] <= 40) & (filter_data['FIQ'] >= 80)]    # add check for FIQ = NaN\n",
        "selection = df_pheno[(df_pheno['AGE_AT_SCAN'] <= 40) & (df_pheno['FIQ'] >= 80)]    # add check for FIQ = NaN\n",
        "selection = selection.dropna(subset=['FIQ','DX_GROUP'], axis=0) # drop the row if FIQ or DX_GROUP is null\n",
        "\n",
        "# columns_sel = selection.columns\n",
        "# print(\"columns\", columns_sel.size) # 2 null FIQ (351-349)\n",
        "# selection = selection[['SEX','DX_GROUP','SUB_ID']]\n",
        "## Total Females-Males\n",
        "females = selection[selection['SEX'] == 2]\n",
        "males = selection[selection['SEX'] == 1]\n",
        "print(\"females\", len(females))\n",
        "print(\"males\", len(males))\n",
        "## Autistic Females-Males\n",
        "females_autistic = females[females['DX_GROUP'] == 1]\n",
        "males_autistic = males[males['DX_GROUP'] == 1]\n",
        "males_control = males[males['DX_GROUP'] == 2]\n",
        "print(\"females_autistic\", len(females_autistic))\n",
        "print(\"males_autistic\", len(males_autistic))\n",
        "print(\"males_control\", len(males_control))\n",
        "# # therefore we need len(females_autistic) control males and len(females)-len(females_autistic) autistic males\n",
        "# # remove: #males-#autistic_females and #males\n",
        "# ma_to_remove = np.random.choice(males_autistic.index, size=240, replace=False)\n",
        "# mc_to_remove = np.random.choice(males_control.index, size=330, replace=False)\n",
        "# selection = selection.drop(ma_to_remove)\n",
        "# selection = selection.drop(mc_to_remove)\n",
        "# labels = selection[['SUB_ID','DX_GROUP']]\n",
        "# labels_dict = labels.set_index('SUB_ID').T.to_dict('list')\n",
        "records = len(selection.index)\n",
        "print(\"Total:\",records)\n",
        "select_yes = len(selection[selection['DX_GROUP'] == 1])\n",
        "print(\"ASD:\",select_yes)\n",
        "y_percent = float(select_yes) / n_records *100\n",
        "print(\"ASD percent\",y_percent)\n",
        "display(selection.head(n=5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "7N5SsyjFNcZW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "9da0c6f2-8cb5-4f58-9503-7abcee77cf3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-16b14b3f-855a-44af-abed-27f87f714e79\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>SUB_ID</th>\n",
              "      <th>X</th>\n",
              "      <th>subject</th>\n",
              "      <th>DX_GROUP</th>\n",
              "      <th>DSM_IV_TR</th>\n",
              "      <th>AGE_AT_SCAN</th>\n",
              "      <th>SEX</th>\n",
              "      <th>HANDEDNESS_SCORES</th>\n",
              "      <th>FIQ</th>\n",
              "      <th>VIQ</th>\n",
              "      <th>PIQ</th>\n",
              "      <th>ADI_R_SOCIAL_TOTAL_A</th>\n",
              "      <th>ADI_R_VERBAL_TOTAL_BV</th>\n",
              "      <th>ADI_RRB_TOTAL_C</th>\n",
              "      <th>ADI_R_ONSET_TOTAL_D</th>\n",
              "      <th>ADI_R_RSRCH_RELIABLE</th>\n",
              "      <th>ADOS_MODULE</th>\n",
              "      <th>ADOS_TOTAL</th>\n",
              "      <th>ADOS_COMM</th>\n",
              "      <th>ADOS_SOCIAL</th>\n",
              "      <th>ADOS_STEREO_BEHAV</th>\n",
              "      <th>ADOS_RSRCH_RELIABLE</th>\n",
              "      <th>ADOS_GOTHAM_SOCAFFECT</th>\n",
              "      <th>ADOS_GOTHAM_RRB</th>\n",
              "      <th>ADOS_GOTHAM_TOTAL</th>\n",
              "      <th>ADOS_GOTHAM_SEVERITY</th>\n",
              "      <th>SRS_VERSION</th>\n",
              "      <th>SRS_RAW_TOTAL</th>\n",
              "      <th>SRS_AWARENESS</th>\n",
              "      <th>SRS_COGNITION</th>\n",
              "      <th>SRS_COMMUNICATION</th>\n",
              "      <th>SRS_MOTIVATION</th>\n",
              "      <th>SRS_MANNERISMS</th>\n",
              "      <th>SCQ_TOTAL</th>\n",
              "      <th>AQ_TOTAL</th>\n",
              "      <th>CURRENT_MED_STATUS</th>\n",
              "      <th>OFF_STIMULANTS_AT_SCAN</th>\n",
              "      <th>VINELAND_RECEPTIVE_V_SCALED</th>\n",
              "      <th>...</th>\n",
              "      <th>VINELAND_PLAY_V_SCALED</th>\n",
              "      <th>VINELAND_COPING_V_SCALED</th>\n",
              "      <th>VINELAND_SOCIAL_STANDARD</th>\n",
              "      <th>VINELAND_SUM_SCORES</th>\n",
              "      <th>VINELAND_ABC_STANDARD</th>\n",
              "      <th>VINELAND_INFORMANT</th>\n",
              "      <th>WISC_IV_VCI</th>\n",
              "      <th>WISC_IV_PRI</th>\n",
              "      <th>WISC_IV_WMI</th>\n",
              "      <th>WISC_IV_PSI</th>\n",
              "      <th>WISC_IV_SIM_SCALED</th>\n",
              "      <th>WISC_IV_VOCAB_SCALED</th>\n",
              "      <th>WISC_IV_INFO_SCALED</th>\n",
              "      <th>WISC_IV_BLK_DSN_SCALED</th>\n",
              "      <th>WISC_IV_PIC_CON_SCALED</th>\n",
              "      <th>WISC_IV_MATRIX_SCALED</th>\n",
              "      <th>WISC_IV_DIGIT_SPAN_SCALED</th>\n",
              "      <th>WISC_IV_LET_NUM_SCALED</th>\n",
              "      <th>WISC_IV_CODING_SCALED</th>\n",
              "      <th>WISC_IV_SYM_SCALED</th>\n",
              "      <th>EYE_STATUS_AT_SCAN</th>\n",
              "      <th>AGE_AT_MPRAGE</th>\n",
              "      <th>BMI</th>\n",
              "      <th>anat_cnr</th>\n",
              "      <th>anat_efc</th>\n",
              "      <th>anat_fber</th>\n",
              "      <th>anat_fwhm</th>\n",
              "      <th>anat_qi1</th>\n",
              "      <th>anat_snr</th>\n",
              "      <th>func_efc</th>\n",
              "      <th>func_fber</th>\n",
              "      <th>func_fwhm</th>\n",
              "      <th>func_dvars</th>\n",
              "      <th>func_outlier</th>\n",
              "      <th>func_quality</th>\n",
              "      <th>func_mean_fd</th>\n",
              "      <th>func_num_fd</th>\n",
              "      <th>func_perc_fd</th>\n",
              "      <th>func_gsr</th>\n",
              "      <th>SUB_IN_SMP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996802</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996802</td>\n",
              "      <td>0.061703</td>\n",
              "      <td>0.047760</td>\n",
              "      <td>0.181828</td>\n",
              "      <td>0.014324</td>\n",
              "      <td>-0.054216</td>\n",
              "      <td>-0.018130</td>\n",
              "      <td>-0.014932</td>\n",
              "      <td>0.016984</td>\n",
              "      <td>-0.031584</td>\n",
              "      <td>-0.027844</td>\n",
              "      <td>-0.055876</td>\n",
              "      <td>-0.039102</td>\n",
              "      <td>-0.062540</td>\n",
              "      <td>-0.105054</td>\n",
              "      <td>0.046482</td>\n",
              "      <td>-0.019601</td>\n",
              "      <td>0.067449</td>\n",
              "      <td>0.045814</td>\n",
              "      <td>-0.038527</td>\n",
              "      <td>0.121982</td>\n",
              "      <td>0.009640</td>\n",
              "      <td>0.099288</td>\n",
              "      <td>0.128881</td>\n",
              "      <td>-0.188254</td>\n",
              "      <td>-0.072971</td>\n",
              "      <td>0.138917</td>\n",
              "      <td>0.147769</td>\n",
              "      <td>0.128280</td>\n",
              "      <td>0.159167</td>\n",
              "      <td>0.156117</td>\n",
              "      <td>-0.258679</td>\n",
              "      <td>0.241644</td>\n",
              "      <td>-0.052009</td>\n",
              "      <td>0.748119</td>\n",
              "      <td>0.515331</td>\n",
              "      <td>...</td>\n",
              "      <td>0.527677</td>\n",
              "      <td>0.637955</td>\n",
              "      <td>0.630992</td>\n",
              "      <td>0.574395</td>\n",
              "      <td>0.591557</td>\n",
              "      <td>-0.144162</td>\n",
              "      <td>-0.121505</td>\n",
              "      <td>0.036496</td>\n",
              "      <td>-0.115563</td>\n",
              "      <td>-0.124800</td>\n",
              "      <td>-0.072100</td>\n",
              "      <td>-0.126149</td>\n",
              "      <td>-0.107639</td>\n",
              "      <td>0.205908</td>\n",
              "      <td>-0.095745</td>\n",
              "      <td>-0.019618</td>\n",
              "      <td>-0.040055</td>\n",
              "      <td>-0.170137</td>\n",
              "      <td>-0.051207</td>\n",
              "      <td>-0.153658</td>\n",
              "      <td>0.181235</td>\n",
              "      <td>-0.134917</td>\n",
              "      <td>0.254167</td>\n",
              "      <td>-0.117261</td>\n",
              "      <td>-0.237504</td>\n",
              "      <td>0.123073</td>\n",
              "      <td>-0.179085</td>\n",
              "      <td>-0.169309</td>\n",
              "      <td>0.163762</td>\n",
              "      <td>0.173439</td>\n",
              "      <td>-0.049783</td>\n",
              "      <td>-0.185545</td>\n",
              "      <td>-0.022723</td>\n",
              "      <td>-0.037161</td>\n",
              "      <td>-0.120263</td>\n",
              "      <td>-0.060259</td>\n",
              "      <td>-0.130511</td>\n",
              "      <td>-0.047600</td>\n",
              "      <td>-0.096649</td>\n",
              "      <td>-0.152313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996802</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996802</td>\n",
              "      <td>0.061703</td>\n",
              "      <td>0.047760</td>\n",
              "      <td>0.181828</td>\n",
              "      <td>0.014324</td>\n",
              "      <td>-0.054216</td>\n",
              "      <td>-0.018130</td>\n",
              "      <td>-0.014932</td>\n",
              "      <td>0.016984</td>\n",
              "      <td>-0.031584</td>\n",
              "      <td>-0.027844</td>\n",
              "      <td>-0.055876</td>\n",
              "      <td>-0.039102</td>\n",
              "      <td>-0.062540</td>\n",
              "      <td>-0.105054</td>\n",
              "      <td>0.046482</td>\n",
              "      <td>-0.019601</td>\n",
              "      <td>0.067449</td>\n",
              "      <td>0.045814</td>\n",
              "      <td>-0.038527</td>\n",
              "      <td>0.121982</td>\n",
              "      <td>0.009640</td>\n",
              "      <td>0.099288</td>\n",
              "      <td>0.128881</td>\n",
              "      <td>-0.188254</td>\n",
              "      <td>-0.072971</td>\n",
              "      <td>0.138917</td>\n",
              "      <td>0.147769</td>\n",
              "      <td>0.128280</td>\n",
              "      <td>0.159167</td>\n",
              "      <td>0.156117</td>\n",
              "      <td>-0.258679</td>\n",
              "      <td>0.241644</td>\n",
              "      <td>-0.052009</td>\n",
              "      <td>0.748119</td>\n",
              "      <td>0.515331</td>\n",
              "      <td>...</td>\n",
              "      <td>0.527677</td>\n",
              "      <td>0.637955</td>\n",
              "      <td>0.630992</td>\n",
              "      <td>0.574395</td>\n",
              "      <td>0.591557</td>\n",
              "      <td>-0.144162</td>\n",
              "      <td>-0.121505</td>\n",
              "      <td>0.036496</td>\n",
              "      <td>-0.115563</td>\n",
              "      <td>-0.124800</td>\n",
              "      <td>-0.072100</td>\n",
              "      <td>-0.126149</td>\n",
              "      <td>-0.107639</td>\n",
              "      <td>0.205908</td>\n",
              "      <td>-0.095745</td>\n",
              "      <td>-0.019618</td>\n",
              "      <td>-0.040055</td>\n",
              "      <td>-0.170137</td>\n",
              "      <td>-0.051207</td>\n",
              "      <td>-0.153658</td>\n",
              "      <td>0.181235</td>\n",
              "      <td>-0.134917</td>\n",
              "      <td>0.254167</td>\n",
              "      <td>-0.117261</td>\n",
              "      <td>-0.237504</td>\n",
              "      <td>0.123073</td>\n",
              "      <td>-0.179085</td>\n",
              "      <td>-0.169309</td>\n",
              "      <td>0.163762</td>\n",
              "      <td>0.173439</td>\n",
              "      <td>-0.049783</td>\n",
              "      <td>-0.185545</td>\n",
              "      <td>-0.022723</td>\n",
              "      <td>-0.037161</td>\n",
              "      <td>-0.120263</td>\n",
              "      <td>-0.060259</td>\n",
              "      <td>-0.130511</td>\n",
              "      <td>-0.047600</td>\n",
              "      <td>-0.096649</td>\n",
              "      <td>-0.152313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SUB_ID</th>\n",
              "      <td>0.996802</td>\n",
              "      <td>0.996802</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996802</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.053670</td>\n",
              "      <td>0.056924</td>\n",
              "      <td>0.180474</td>\n",
              "      <td>0.017500</td>\n",
              "      <td>-0.071095</td>\n",
              "      <td>-0.008870</td>\n",
              "      <td>-0.006089</td>\n",
              "      <td>0.026679</td>\n",
              "      <td>-0.037152</td>\n",
              "      <td>-0.030402</td>\n",
              "      <td>-0.059036</td>\n",
              "      <td>-0.042714</td>\n",
              "      <td>-0.078726</td>\n",
              "      <td>-0.095660</td>\n",
              "      <td>0.048947</td>\n",
              "      <td>-0.019705</td>\n",
              "      <td>0.071645</td>\n",
              "      <td>0.059460</td>\n",
              "      <td>-0.045728</td>\n",
              "      <td>0.116648</td>\n",
              "      <td>0.011459</td>\n",
              "      <td>0.096052</td>\n",
              "      <td>0.125661</td>\n",
              "      <td>-0.195366</td>\n",
              "      <td>-0.054225</td>\n",
              "      <td>0.086588</td>\n",
              "      <td>0.097897</td>\n",
              "      <td>0.069067</td>\n",
              "      <td>0.101796</td>\n",
              "      <td>0.094620</td>\n",
              "      <td>-0.131374</td>\n",
              "      <td>0.223997</td>\n",
              "      <td>-0.054655</td>\n",
              "      <td>0.734246</td>\n",
              "      <td>0.519395</td>\n",
              "      <td>...</td>\n",
              "      <td>0.532840</td>\n",
              "      <td>0.642177</td>\n",
              "      <td>0.636457</td>\n",
              "      <td>0.579289</td>\n",
              "      <td>0.597081</td>\n",
              "      <td>-0.144298</td>\n",
              "      <td>-0.121505</td>\n",
              "      <td>0.036496</td>\n",
              "      <td>-0.115563</td>\n",
              "      <td>-0.124800</td>\n",
              "      <td>-0.072100</td>\n",
              "      <td>-0.126149</td>\n",
              "      <td>-0.107639</td>\n",
              "      <td>0.205908</td>\n",
              "      <td>-0.095745</td>\n",
              "      <td>-0.019618</td>\n",
              "      <td>-0.040055</td>\n",
              "      <td>-0.170137</td>\n",
              "      <td>-0.051207</td>\n",
              "      <td>-0.153658</td>\n",
              "      <td>0.191990</td>\n",
              "      <td>-0.135887</td>\n",
              "      <td>0.259552</td>\n",
              "      <td>-0.144752</td>\n",
              "      <td>-0.244675</td>\n",
              "      <td>0.128319</td>\n",
              "      <td>-0.197718</td>\n",
              "      <td>-0.157199</td>\n",
              "      <td>0.166623</td>\n",
              "      <td>0.160089</td>\n",
              "      <td>-0.037783</td>\n",
              "      <td>-0.194439</td>\n",
              "      <td>-0.038690</td>\n",
              "      <td>-0.034133</td>\n",
              "      <td>-0.118905</td>\n",
              "      <td>-0.068008</td>\n",
              "      <td>-0.139321</td>\n",
              "      <td>-0.055767</td>\n",
              "      <td>-0.137094</td>\n",
              "      <td>-0.158667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996802</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996802</td>\n",
              "      <td>0.061703</td>\n",
              "      <td>0.047760</td>\n",
              "      <td>0.181828</td>\n",
              "      <td>0.014324</td>\n",
              "      <td>-0.054216</td>\n",
              "      <td>-0.018130</td>\n",
              "      <td>-0.014932</td>\n",
              "      <td>0.016984</td>\n",
              "      <td>-0.031584</td>\n",
              "      <td>-0.027844</td>\n",
              "      <td>-0.055876</td>\n",
              "      <td>-0.039102</td>\n",
              "      <td>-0.062540</td>\n",
              "      <td>-0.105054</td>\n",
              "      <td>0.046482</td>\n",
              "      <td>-0.019601</td>\n",
              "      <td>0.067449</td>\n",
              "      <td>0.045814</td>\n",
              "      <td>-0.038527</td>\n",
              "      <td>0.121982</td>\n",
              "      <td>0.009640</td>\n",
              "      <td>0.099288</td>\n",
              "      <td>0.128881</td>\n",
              "      <td>-0.188254</td>\n",
              "      <td>-0.072971</td>\n",
              "      <td>0.138917</td>\n",
              "      <td>0.147769</td>\n",
              "      <td>0.128280</td>\n",
              "      <td>0.159167</td>\n",
              "      <td>0.156117</td>\n",
              "      <td>-0.258679</td>\n",
              "      <td>0.241644</td>\n",
              "      <td>-0.052009</td>\n",
              "      <td>0.748119</td>\n",
              "      <td>0.515331</td>\n",
              "      <td>...</td>\n",
              "      <td>0.527677</td>\n",
              "      <td>0.637955</td>\n",
              "      <td>0.630992</td>\n",
              "      <td>0.574395</td>\n",
              "      <td>0.591557</td>\n",
              "      <td>-0.144162</td>\n",
              "      <td>-0.121505</td>\n",
              "      <td>0.036496</td>\n",
              "      <td>-0.115563</td>\n",
              "      <td>-0.124800</td>\n",
              "      <td>-0.072100</td>\n",
              "      <td>-0.126149</td>\n",
              "      <td>-0.107639</td>\n",
              "      <td>0.205908</td>\n",
              "      <td>-0.095745</td>\n",
              "      <td>-0.019618</td>\n",
              "      <td>-0.040055</td>\n",
              "      <td>-0.170137</td>\n",
              "      <td>-0.051207</td>\n",
              "      <td>-0.153658</td>\n",
              "      <td>0.181235</td>\n",
              "      <td>-0.134917</td>\n",
              "      <td>0.254167</td>\n",
              "      <td>-0.117261</td>\n",
              "      <td>-0.237504</td>\n",
              "      <td>0.123073</td>\n",
              "      <td>-0.179085</td>\n",
              "      <td>-0.169309</td>\n",
              "      <td>0.163762</td>\n",
              "      <td>0.173439</td>\n",
              "      <td>-0.049783</td>\n",
              "      <td>-0.185545</td>\n",
              "      <td>-0.022723</td>\n",
              "      <td>-0.037161</td>\n",
              "      <td>-0.120263</td>\n",
              "      <td>-0.060259</td>\n",
              "      <td>-0.130511</td>\n",
              "      <td>-0.047600</td>\n",
              "      <td>-0.096649</td>\n",
              "      <td>-0.152313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>subject</th>\n",
              "      <td>0.996802</td>\n",
              "      <td>0.996802</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996802</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.053670</td>\n",
              "      <td>0.056924</td>\n",
              "      <td>0.180474</td>\n",
              "      <td>0.017500</td>\n",
              "      <td>-0.071095</td>\n",
              "      <td>-0.008870</td>\n",
              "      <td>-0.006089</td>\n",
              "      <td>0.026679</td>\n",
              "      <td>-0.037152</td>\n",
              "      <td>-0.030402</td>\n",
              "      <td>-0.059036</td>\n",
              "      <td>-0.042714</td>\n",
              "      <td>-0.078726</td>\n",
              "      <td>-0.095660</td>\n",
              "      <td>0.048947</td>\n",
              "      <td>-0.019705</td>\n",
              "      <td>0.071645</td>\n",
              "      <td>0.059460</td>\n",
              "      <td>-0.045728</td>\n",
              "      <td>0.116648</td>\n",
              "      <td>0.011459</td>\n",
              "      <td>0.096052</td>\n",
              "      <td>0.125661</td>\n",
              "      <td>-0.195366</td>\n",
              "      <td>-0.054225</td>\n",
              "      <td>0.086588</td>\n",
              "      <td>0.097897</td>\n",
              "      <td>0.069067</td>\n",
              "      <td>0.101796</td>\n",
              "      <td>0.094620</td>\n",
              "      <td>-0.131374</td>\n",
              "      <td>0.223997</td>\n",
              "      <td>-0.054655</td>\n",
              "      <td>0.734246</td>\n",
              "      <td>0.519395</td>\n",
              "      <td>...</td>\n",
              "      <td>0.532840</td>\n",
              "      <td>0.642177</td>\n",
              "      <td>0.636457</td>\n",
              "      <td>0.579289</td>\n",
              "      <td>0.597081</td>\n",
              "      <td>-0.144298</td>\n",
              "      <td>-0.121505</td>\n",
              "      <td>0.036496</td>\n",
              "      <td>-0.115563</td>\n",
              "      <td>-0.124800</td>\n",
              "      <td>-0.072100</td>\n",
              "      <td>-0.126149</td>\n",
              "      <td>-0.107639</td>\n",
              "      <td>0.205908</td>\n",
              "      <td>-0.095745</td>\n",
              "      <td>-0.019618</td>\n",
              "      <td>-0.040055</td>\n",
              "      <td>-0.170137</td>\n",
              "      <td>-0.051207</td>\n",
              "      <td>-0.153658</td>\n",
              "      <td>0.191990</td>\n",
              "      <td>-0.135887</td>\n",
              "      <td>0.259552</td>\n",
              "      <td>-0.144752</td>\n",
              "      <td>-0.244675</td>\n",
              "      <td>0.128319</td>\n",
              "      <td>-0.197718</td>\n",
              "      <td>-0.157199</td>\n",
              "      <td>0.166623</td>\n",
              "      <td>0.160089</td>\n",
              "      <td>-0.037783</td>\n",
              "      <td>-0.194439</td>\n",
              "      <td>-0.038690</td>\n",
              "      <td>-0.034133</td>\n",
              "      <td>-0.118905</td>\n",
              "      <td>-0.068008</td>\n",
              "      <td>-0.139321</td>\n",
              "      <td>-0.055767</td>\n",
              "      <td>-0.137094</td>\n",
              "      <td>-0.158667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>func_mean_fd</th>\n",
              "      <td>-0.060259</td>\n",
              "      <td>-0.060259</td>\n",
              "      <td>-0.068008</td>\n",
              "      <td>-0.060259</td>\n",
              "      <td>-0.068008</td>\n",
              "      <td>-0.188098</td>\n",
              "      <td>0.114166</td>\n",
              "      <td>-0.092335</td>\n",
              "      <td>-0.063382</td>\n",
              "      <td>0.007383</td>\n",
              "      <td>-0.122773</td>\n",
              "      <td>-0.126543</td>\n",
              "      <td>-0.066165</td>\n",
              "      <td>-0.079939</td>\n",
              "      <td>-0.045771</td>\n",
              "      <td>0.030512</td>\n",
              "      <td>0.058685</td>\n",
              "      <td>0.001952</td>\n",
              "      <td>-0.136684</td>\n",
              "      <td>0.092311</td>\n",
              "      <td>0.065725</td>\n",
              "      <td>0.110532</td>\n",
              "      <td>0.117953</td>\n",
              "      <td>-0.015403</td>\n",
              "      <td>0.140908</td>\n",
              "      <td>0.079231</td>\n",
              "      <td>0.107522</td>\n",
              "      <td>0.096594</td>\n",
              "      <td>-0.116990</td>\n",
              "      <td>0.201941</td>\n",
              "      <td>0.107436</td>\n",
              "      <td>0.146364</td>\n",
              "      <td>0.168551</td>\n",
              "      <td>0.127458</td>\n",
              "      <td>0.143944</td>\n",
              "      <td>0.210679</td>\n",
              "      <td>0.007265</td>\n",
              "      <td>0.235046</td>\n",
              "      <td>0.173849</td>\n",
              "      <td>-0.210616</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.159431</td>\n",
              "      <td>-0.211027</td>\n",
              "      <td>-0.122878</td>\n",
              "      <td>-0.078373</td>\n",
              "      <td>-0.097690</td>\n",
              "      <td>-0.081639</td>\n",
              "      <td>-0.152280</td>\n",
              "      <td>-0.137014</td>\n",
              "      <td>-0.241973</td>\n",
              "      <td>-0.309809</td>\n",
              "      <td>-0.010830</td>\n",
              "      <td>-0.165017</td>\n",
              "      <td>-0.231921</td>\n",
              "      <td>0.047050</td>\n",
              "      <td>-0.214863</td>\n",
              "      <td>-0.155420</td>\n",
              "      <td>-0.236055</td>\n",
              "      <td>-0.197581</td>\n",
              "      <td>-0.301144</td>\n",
              "      <td>-0.269522</td>\n",
              "      <td>-0.026221</td>\n",
              "      <td>-0.271617</td>\n",
              "      <td>0.366732</td>\n",
              "      <td>-0.069697</td>\n",
              "      <td>0.059188</td>\n",
              "      <td>-0.026702</td>\n",
              "      <td>-0.037423</td>\n",
              "      <td>0.035346</td>\n",
              "      <td>-0.046110</td>\n",
              "      <td>0.091803</td>\n",
              "      <td>-0.003365</td>\n",
              "      <td>0.089862</td>\n",
              "      <td>0.043171</td>\n",
              "      <td>0.541888</td>\n",
              "      <td>0.736170</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.809481</td>\n",
              "      <td>0.865454</td>\n",
              "      <td>0.154814</td>\n",
              "      <td>-0.259634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>func_num_fd</th>\n",
              "      <td>-0.130511</td>\n",
              "      <td>-0.130511</td>\n",
              "      <td>-0.139321</td>\n",
              "      <td>-0.130511</td>\n",
              "      <td>-0.139321</td>\n",
              "      <td>-0.175619</td>\n",
              "      <td>0.101150</td>\n",
              "      <td>0.015559</td>\n",
              "      <td>-0.051133</td>\n",
              "      <td>0.059331</td>\n",
              "      <td>-0.077902</td>\n",
              "      <td>-0.078496</td>\n",
              "      <td>-0.035282</td>\n",
              "      <td>-0.051958</td>\n",
              "      <td>-0.049576</td>\n",
              "      <td>0.029655</td>\n",
              "      <td>0.012128</td>\n",
              "      <td>-0.011148</td>\n",
              "      <td>-0.050542</td>\n",
              "      <td>0.094643</td>\n",
              "      <td>0.081989</td>\n",
              "      <td>0.100438</td>\n",
              "      <td>0.102787</td>\n",
              "      <td>-0.038643</td>\n",
              "      <td>0.091364</td>\n",
              "      <td>0.072114</td>\n",
              "      <td>0.069365</td>\n",
              "      <td>0.063067</td>\n",
              "      <td>-0.136426</td>\n",
              "      <td>0.163893</td>\n",
              "      <td>0.055271</td>\n",
              "      <td>0.117453</td>\n",
              "      <td>0.137197</td>\n",
              "      <td>0.092617</td>\n",
              "      <td>0.144967</td>\n",
              "      <td>0.240150</td>\n",
              "      <td>-0.004576</td>\n",
              "      <td>0.196758</td>\n",
              "      <td>-0.069332</td>\n",
              "      <td>-0.249363</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.154504</td>\n",
              "      <td>-0.220884</td>\n",
              "      <td>-0.136833</td>\n",
              "      <td>-0.103156</td>\n",
              "      <td>-0.122015</td>\n",
              "      <td>-0.072238</td>\n",
              "      <td>-0.193913</td>\n",
              "      <td>-0.136482</td>\n",
              "      <td>-0.280848</td>\n",
              "      <td>-0.355986</td>\n",
              "      <td>-0.061757</td>\n",
              "      <td>-0.192821</td>\n",
              "      <td>-0.258255</td>\n",
              "      <td>0.018026</td>\n",
              "      <td>-0.194419</td>\n",
              "      <td>-0.141016</td>\n",
              "      <td>-0.254754</td>\n",
              "      <td>-0.244882</td>\n",
              "      <td>-0.367986</td>\n",
              "      <td>-0.281149</td>\n",
              "      <td>0.069038</td>\n",
              "      <td>-0.282691</td>\n",
              "      <td>0.402948</td>\n",
              "      <td>0.015227</td>\n",
              "      <td>0.092107</td>\n",
              "      <td>-0.019233</td>\n",
              "      <td>-0.017033</td>\n",
              "      <td>0.039110</td>\n",
              "      <td>-0.052551</td>\n",
              "      <td>0.195242</td>\n",
              "      <td>-0.112762</td>\n",
              "      <td>0.021923</td>\n",
              "      <td>0.035093</td>\n",
              "      <td>0.375929</td>\n",
              "      <td>0.690024</td>\n",
              "      <td>0.809481</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.939095</td>\n",
              "      <td>0.238193</td>\n",
              "      <td>-0.258560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>func_perc_fd</th>\n",
              "      <td>-0.047600</td>\n",
              "      <td>-0.047600</td>\n",
              "      <td>-0.055767</td>\n",
              "      <td>-0.047600</td>\n",
              "      <td>-0.055767</td>\n",
              "      <td>-0.189374</td>\n",
              "      <td>0.128284</td>\n",
              "      <td>-0.011289</td>\n",
              "      <td>-0.068794</td>\n",
              "      <td>0.054083</td>\n",
              "      <td>-0.106733</td>\n",
              "      <td>-0.115714</td>\n",
              "      <td>-0.045305</td>\n",
              "      <td>-0.053875</td>\n",
              "      <td>-0.048358</td>\n",
              "      <td>0.038214</td>\n",
              "      <td>0.053662</td>\n",
              "      <td>-0.041268</td>\n",
              "      <td>-0.094856</td>\n",
              "      <td>0.091475</td>\n",
              "      <td>0.062039</td>\n",
              "      <td>0.111750</td>\n",
              "      <td>0.120532</td>\n",
              "      <td>-0.071919</td>\n",
              "      <td>0.107676</td>\n",
              "      <td>0.042990</td>\n",
              "      <td>0.078231</td>\n",
              "      <td>0.087040</td>\n",
              "      <td>-0.176487</td>\n",
              "      <td>0.168636</td>\n",
              "      <td>0.055271</td>\n",
              "      <td>0.117453</td>\n",
              "      <td>0.137197</td>\n",
              "      <td>0.092617</td>\n",
              "      <td>0.144967</td>\n",
              "      <td>0.249951</td>\n",
              "      <td>0.002215</td>\n",
              "      <td>0.207518</td>\n",
              "      <td>0.092299</td>\n",
              "      <td>-0.249363</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.154504</td>\n",
              "      <td>-0.220884</td>\n",
              "      <td>-0.136833</td>\n",
              "      <td>-0.103156</td>\n",
              "      <td>-0.122015</td>\n",
              "      <td>-0.072238</td>\n",
              "      <td>-0.199629</td>\n",
              "      <td>-0.140865</td>\n",
              "      <td>-0.257270</td>\n",
              "      <td>-0.363744</td>\n",
              "      <td>-0.063637</td>\n",
              "      <td>-0.200423</td>\n",
              "      <td>-0.266873</td>\n",
              "      <td>0.025411</td>\n",
              "      <td>-0.209853</td>\n",
              "      <td>-0.148002</td>\n",
              "      <td>-0.240873</td>\n",
              "      <td>-0.217354</td>\n",
              "      <td>-0.371410</td>\n",
              "      <td>-0.296683</td>\n",
              "      <td>0.062679</td>\n",
              "      <td>-0.282691</td>\n",
              "      <td>0.406308</td>\n",
              "      <td>-0.024464</td>\n",
              "      <td>0.072387</td>\n",
              "      <td>-0.057528</td>\n",
              "      <td>-0.028669</td>\n",
              "      <td>0.088452</td>\n",
              "      <td>-0.058272</td>\n",
              "      <td>0.156700</td>\n",
              "      <td>-0.050046</td>\n",
              "      <td>0.018357</td>\n",
              "      <td>0.048893</td>\n",
              "      <td>0.419313</td>\n",
              "      <td>0.622091</td>\n",
              "      <td>0.865454</td>\n",
              "      <td>0.939095</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.221054</td>\n",
              "      <td>-0.243603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>func_gsr</th>\n",
              "      <td>-0.096649</td>\n",
              "      <td>-0.096649</td>\n",
              "      <td>-0.137094</td>\n",
              "      <td>-0.096649</td>\n",
              "      <td>-0.137094</td>\n",
              "      <td>-0.047518</td>\n",
              "      <td>0.016872</td>\n",
              "      <td>0.215673</td>\n",
              "      <td>-0.081625</td>\n",
              "      <td>0.098368</td>\n",
              "      <td>-0.039404</td>\n",
              "      <td>-0.042354</td>\n",
              "      <td>-0.036278</td>\n",
              "      <td>0.061802</td>\n",
              "      <td>0.033680</td>\n",
              "      <td>0.045302</td>\n",
              "      <td>-0.052665</td>\n",
              "      <td>0.120729</td>\n",
              "      <td>0.073469</td>\n",
              "      <td>-0.019326</td>\n",
              "      <td>0.029710</td>\n",
              "      <td>-0.050929</td>\n",
              "      <td>-0.191460</td>\n",
              "      <td>0.009198</td>\n",
              "      <td>-0.030204</td>\n",
              "      <td>-0.153577</td>\n",
              "      <td>-0.074249</td>\n",
              "      <td>-0.085207</td>\n",
              "      <td>-0.056475</td>\n",
              "      <td>0.041512</td>\n",
              "      <td>-0.060449</td>\n",
              "      <td>-0.036526</td>\n",
              "      <td>-0.025990</td>\n",
              "      <td>0.077877</td>\n",
              "      <td>0.011192</td>\n",
              "      <td>-0.087813</td>\n",
              "      <td>0.117148</td>\n",
              "      <td>0.099604</td>\n",
              "      <td>-0.646554</td>\n",
              "      <td>-0.132869</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001121</td>\n",
              "      <td>0.010276</td>\n",
              "      <td>-0.001843</td>\n",
              "      <td>-0.044835</td>\n",
              "      <td>-0.033893</td>\n",
              "      <td>0.040918</td>\n",
              "      <td>-0.143467</td>\n",
              "      <td>-0.087267</td>\n",
              "      <td>0.023914</td>\n",
              "      <td>0.013151</td>\n",
              "      <td>-0.056715</td>\n",
              "      <td>-0.172201</td>\n",
              "      <td>-0.119670</td>\n",
              "      <td>-0.062523</td>\n",
              "      <td>-0.064961</td>\n",
              "      <td>-0.100756</td>\n",
              "      <td>0.032626</td>\n",
              "      <td>0.012200</td>\n",
              "      <td>0.088694</td>\n",
              "      <td>-0.085753</td>\n",
              "      <td>0.209869</td>\n",
              "      <td>0.424177</td>\n",
              "      <td>0.282411</td>\n",
              "      <td>0.032070</td>\n",
              "      <td>0.043538</td>\n",
              "      <td>-0.098114</td>\n",
              "      <td>0.185789</td>\n",
              "      <td>0.162296</td>\n",
              "      <td>0.054803</td>\n",
              "      <td>0.551418</td>\n",
              "      <td>-0.580066</td>\n",
              "      <td>0.092056</td>\n",
              "      <td>0.156227</td>\n",
              "      <td>-0.011656</td>\n",
              "      <td>0.028626</td>\n",
              "      <td>0.154814</td>\n",
              "      <td>0.238193</td>\n",
              "      <td>0.221054</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.078080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SUB_IN_SMP</th>\n",
              "      <td>-0.152313</td>\n",
              "      <td>-0.152313</td>\n",
              "      <td>-0.158667</td>\n",
              "      <td>-0.152313</td>\n",
              "      <td>-0.158667</td>\n",
              "      <td>0.038137</td>\n",
              "      <td>-0.078134</td>\n",
              "      <td>-0.128226</td>\n",
              "      <td>-0.614989</td>\n",
              "      <td>-0.020809</td>\n",
              "      <td>0.056624</td>\n",
              "      <td>0.067021</td>\n",
              "      <td>0.038470</td>\n",
              "      <td>0.067278</td>\n",
              "      <td>0.057674</td>\n",
              "      <td>0.099542</td>\n",
              "      <td>0.103194</td>\n",
              "      <td>0.122730</td>\n",
              "      <td>-0.020170</td>\n",
              "      <td>-0.088624</td>\n",
              "      <td>-0.067782</td>\n",
              "      <td>-0.091903</td>\n",
              "      <td>-0.092916</td>\n",
              "      <td>0.077553</td>\n",
              "      <td>-0.120841</td>\n",
              "      <td>-0.083614</td>\n",
              "      <td>-0.119836</td>\n",
              "      <td>-0.106920</td>\n",
              "      <td>0.171398</td>\n",
              "      <td>-0.006492</td>\n",
              "      <td>0.067531</td>\n",
              "      <td>-0.081439</td>\n",
              "      <td>-0.005234</td>\n",
              "      <td>0.025304</td>\n",
              "      <td>0.023323</td>\n",
              "      <td>0.049658</td>\n",
              "      <td>-0.172785</td>\n",
              "      <td>-0.040563</td>\n",
              "      <td>0.023623</td>\n",
              "      <td>-0.177902</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.094271</td>\n",
              "      <td>-0.034905</td>\n",
              "      <td>-0.075527</td>\n",
              "      <td>-0.114159</td>\n",
              "      <td>-0.146903</td>\n",
              "      <td>-0.127923</td>\n",
              "      <td>0.029774</td>\n",
              "      <td>0.183387</td>\n",
              "      <td>-0.084573</td>\n",
              "      <td>-0.039554</td>\n",
              "      <td>0.031966</td>\n",
              "      <td>0.086114</td>\n",
              "      <td>0.014888</td>\n",
              "      <td>0.300556</td>\n",
              "      <td>0.117788</td>\n",
              "      <td>0.034035</td>\n",
              "      <td>-0.120842</td>\n",
              "      <td>0.002477</td>\n",
              "      <td>-0.183197</td>\n",
              "      <td>0.204003</td>\n",
              "      <td>-0.159329</td>\n",
              "      <td>0.137280</td>\n",
              "      <td>0.133207</td>\n",
              "      <td>-0.017998</td>\n",
              "      <td>0.172056</td>\n",
              "      <td>-0.178976</td>\n",
              "      <td>0.036132</td>\n",
              "      <td>-0.041233</td>\n",
              "      <td>-0.163617</td>\n",
              "      <td>0.049602</td>\n",
              "      <td>-0.014234</td>\n",
              "      <td>-0.069887</td>\n",
              "      <td>-0.006979</td>\n",
              "      <td>-0.146339</td>\n",
              "      <td>-0.105156</td>\n",
              "      <td>-0.259634</td>\n",
              "      <td>-0.258560</td>\n",
              "      <td>-0.243603</td>\n",
              "      <td>-0.078080</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>88 rows Ã— 88 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-16b14b3f-855a-44af-abed-27f87f714e79')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-16b14b3f-855a-44af-abed-27f87f714e79 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-16b14b3f-855a-44af-abed-27f87f714e79');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "              Unnamed: 0  Unnamed: 0.1  ...  func_gsr  SUB_IN_SMP\n",
              "Unnamed: 0      1.000000      1.000000  ... -0.096649   -0.152313\n",
              "Unnamed: 0.1    1.000000      1.000000  ... -0.096649   -0.152313\n",
              "SUB_ID          0.996802      0.996802  ... -0.137094   -0.158667\n",
              "X               1.000000      1.000000  ... -0.096649   -0.152313\n",
              "subject         0.996802      0.996802  ... -0.137094   -0.158667\n",
              "...                  ...           ...  ...       ...         ...\n",
              "func_mean_fd   -0.060259     -0.060259  ...  0.154814   -0.259634\n",
              "func_num_fd    -0.130511     -0.130511  ...  0.238193   -0.258560\n",
              "func_perc_fd   -0.047600     -0.047600  ...  0.221054   -0.243603\n",
              "func_gsr       -0.096649     -0.096649  ...  1.000000   -0.078080\n",
              "SUB_IN_SMP     -0.152313     -0.152313  ... -0.078080    1.000000\n",
              "\n",
              "[88 rows x 88 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "#corelation between the orignal data\n",
        "correlation_o = df_pheno.corr()\n",
        "correlation_o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "NUvu114uNcZW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "50934ff4-051e-437a-e5df-8aae29e660be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1649136b-7aa6-4434-9a1f-11bef80ca317\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SUB_ID</th>\n",
              "      <th>X</th>\n",
              "      <th>subject</th>\n",
              "      <th>DX_GROUP</th>\n",
              "      <th>DSM_IV_TR</th>\n",
              "      <th>AGE_AT_SCAN</th>\n",
              "      <th>SEX</th>\n",
              "      <th>HANDEDNESS_SCORES</th>\n",
              "      <th>FIQ</th>\n",
              "      <th>VIQ</th>\n",
              "      <th>PIQ</th>\n",
              "      <th>ADI_R_SOCIAL_TOTAL_A</th>\n",
              "      <th>ADI_R_VERBAL_TOTAL_BV</th>\n",
              "      <th>ADI_RRB_TOTAL_C</th>\n",
              "      <th>ADI_R_ONSET_TOTAL_D</th>\n",
              "      <th>ADI_R_RSRCH_RELIABLE</th>\n",
              "      <th>ADOS_MODULE</th>\n",
              "      <th>ADOS_TOTAL</th>\n",
              "      <th>ADOS_COMM</th>\n",
              "      <th>ADOS_SOCIAL</th>\n",
              "      <th>ADOS_STEREO_BEHAV</th>\n",
              "      <th>ADOS_RSRCH_RELIABLE</th>\n",
              "      <th>ADOS_GOTHAM_SOCAFFECT</th>\n",
              "      <th>ADOS_GOTHAM_RRB</th>\n",
              "      <th>ADOS_GOTHAM_TOTAL</th>\n",
              "      <th>ADOS_GOTHAM_SEVERITY</th>\n",
              "      <th>SRS_VERSION</th>\n",
              "      <th>SRS_RAW_TOTAL</th>\n",
              "      <th>SRS_AWARENESS</th>\n",
              "      <th>SRS_COGNITION</th>\n",
              "      <th>SRS_COMMUNICATION</th>\n",
              "      <th>SRS_MOTIVATION</th>\n",
              "      <th>SRS_MANNERISMS</th>\n",
              "      <th>SCQ_TOTAL</th>\n",
              "      <th>AQ_TOTAL</th>\n",
              "      <th>CURRENT_MED_STATUS</th>\n",
              "      <th>OFF_STIMULANTS_AT_SCAN</th>\n",
              "      <th>VINELAND_RECEPTIVE_V_SCALED</th>\n",
              "      <th>VINELAND_EXPRESSIVE_V_SCALED</th>\n",
              "      <th>VINELAND_WRITTEN_V_SCALED</th>\n",
              "      <th>...</th>\n",
              "      <th>VINELAND_PLAY_V_SCALED</th>\n",
              "      <th>VINELAND_COPING_V_SCALED</th>\n",
              "      <th>VINELAND_SOCIAL_STANDARD</th>\n",
              "      <th>VINELAND_SUM_SCORES</th>\n",
              "      <th>VINELAND_ABC_STANDARD</th>\n",
              "      <th>VINELAND_INFORMANT</th>\n",
              "      <th>WISC_IV_VCI</th>\n",
              "      <th>WISC_IV_PRI</th>\n",
              "      <th>WISC_IV_WMI</th>\n",
              "      <th>WISC_IV_PSI</th>\n",
              "      <th>WISC_IV_SIM_SCALED</th>\n",
              "      <th>WISC_IV_VOCAB_SCALED</th>\n",
              "      <th>WISC_IV_INFO_SCALED</th>\n",
              "      <th>WISC_IV_BLK_DSN_SCALED</th>\n",
              "      <th>WISC_IV_PIC_CON_SCALED</th>\n",
              "      <th>WISC_IV_MATRIX_SCALED</th>\n",
              "      <th>WISC_IV_DIGIT_SPAN_SCALED</th>\n",
              "      <th>WISC_IV_LET_NUM_SCALED</th>\n",
              "      <th>WISC_IV_CODING_SCALED</th>\n",
              "      <th>WISC_IV_SYM_SCALED</th>\n",
              "      <th>EYE_STATUS_AT_SCAN</th>\n",
              "      <th>AGE_AT_MPRAGE</th>\n",
              "      <th>BMI</th>\n",
              "      <th>anat_cnr</th>\n",
              "      <th>anat_efc</th>\n",
              "      <th>anat_fber</th>\n",
              "      <th>anat_fwhm</th>\n",
              "      <th>anat_qi1</th>\n",
              "      <th>anat_snr</th>\n",
              "      <th>func_efc</th>\n",
              "      <th>func_fber</th>\n",
              "      <th>func_fwhm</th>\n",
              "      <th>func_dvars</th>\n",
              "      <th>func_outlier</th>\n",
              "      <th>func_quality</th>\n",
              "      <th>func_mean_fd</th>\n",
              "      <th>func_num_fd</th>\n",
              "      <th>func_perc_fd</th>\n",
              "      <th>func_gsr</th>\n",
              "      <th>SUB_IN_SMP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>SUB_ID</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996938</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.061534</td>\n",
              "      <td>0.005388</td>\n",
              "      <td>0.017788</td>\n",
              "      <td>0.041123</td>\n",
              "      <td>-0.107386</td>\n",
              "      <td>-0.019436</td>\n",
              "      <td>0.008824</td>\n",
              "      <td>0.010219</td>\n",
              "      <td>0.008794</td>\n",
              "      <td>0.014432</td>\n",
              "      <td>-0.045434</td>\n",
              "      <td>0.016150</td>\n",
              "      <td>0.056463</td>\n",
              "      <td>-0.147022</td>\n",
              "      <td>0.089541</td>\n",
              "      <td>0.019450</td>\n",
              "      <td>0.113110</td>\n",
              "      <td>0.046731</td>\n",
              "      <td>0.112174</td>\n",
              "      <td>0.147594</td>\n",
              "      <td>0.026776</td>\n",
              "      <td>0.124033</td>\n",
              "      <td>0.149724</td>\n",
              "      <td>-0.218425</td>\n",
              "      <td>-0.048111</td>\n",
              "      <td>0.238681</td>\n",
              "      <td>0.128181</td>\n",
              "      <td>0.323434</td>\n",
              "      <td>0.345196</td>\n",
              "      <td>0.230688</td>\n",
              "      <td>-0.733100</td>\n",
              "      <td>0.499556</td>\n",
              "      <td>-0.052422</td>\n",
              "      <td>0.830689</td>\n",
              "      <td>0.508347</td>\n",
              "      <td>0.636436</td>\n",
              "      <td>0.320192</td>\n",
              "      <td>...</td>\n",
              "      <td>0.521023</td>\n",
              "      <td>0.649602</td>\n",
              "      <td>0.633794</td>\n",
              "      <td>0.572617</td>\n",
              "      <td>0.590972</td>\n",
              "      <td>-0.153806</td>\n",
              "      <td>-0.152362</td>\n",
              "      <td>0.058235</td>\n",
              "      <td>-0.144000</td>\n",
              "      <td>-0.128108</td>\n",
              "      <td>-0.113697</td>\n",
              "      <td>-0.162745</td>\n",
              "      <td>-0.113980</td>\n",
              "      <td>0.215041</td>\n",
              "      <td>-0.092078</td>\n",
              "      <td>-0.003277</td>\n",
              "      <td>-0.054556</td>\n",
              "      <td>-0.198592</td>\n",
              "      <td>-0.037467</td>\n",
              "      <td>-0.185474</td>\n",
              "      <td>0.101219</td>\n",
              "      <td>-0.121469</td>\n",
              "      <td>0.030873</td>\n",
              "      <td>-0.074936</td>\n",
              "      <td>-0.232204</td>\n",
              "      <td>0.144431</td>\n",
              "      <td>-0.188087</td>\n",
              "      <td>-0.190360</td>\n",
              "      <td>0.165186</td>\n",
              "      <td>0.179996</td>\n",
              "      <td>-0.042194</td>\n",
              "      <td>-0.186669</td>\n",
              "      <td>-0.063495</td>\n",
              "      <td>0.000774</td>\n",
              "      <td>-0.107835</td>\n",
              "      <td>-0.103853</td>\n",
              "      <td>-0.187568</td>\n",
              "      <td>-0.114884</td>\n",
              "      <td>-0.170916</td>\n",
              "      <td>-0.092060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X</th>\n",
              "      <td>0.996938</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996938</td>\n",
              "      <td>0.071853</td>\n",
              "      <td>-0.004486</td>\n",
              "      <td>0.027769</td>\n",
              "      <td>0.036261</td>\n",
              "      <td>-0.089955</td>\n",
              "      <td>-0.028026</td>\n",
              "      <td>0.003239</td>\n",
              "      <td>0.002724</td>\n",
              "      <td>0.010749</td>\n",
              "      <td>0.013005</td>\n",
              "      <td>-0.043626</td>\n",
              "      <td>0.014929</td>\n",
              "      <td>0.064725</td>\n",
              "      <td>-0.152517</td>\n",
              "      <td>0.080590</td>\n",
              "      <td>0.013095</td>\n",
              "      <td>0.102982</td>\n",
              "      <td>0.027596</td>\n",
              "      <td>0.109021</td>\n",
              "      <td>0.145718</td>\n",
              "      <td>0.017836</td>\n",
              "      <td>0.119289</td>\n",
              "      <td>0.146261</td>\n",
              "      <td>-0.212479</td>\n",
              "      <td>-0.067111</td>\n",
              "      <td>0.235207</td>\n",
              "      <td>0.123985</td>\n",
              "      <td>0.323751</td>\n",
              "      <td>0.340992</td>\n",
              "      <td>0.225573</td>\n",
              "      <td>-0.728229</td>\n",
              "      <td>0.513151</td>\n",
              "      <td>-0.051953</td>\n",
              "      <td>0.848588</td>\n",
              "      <td>0.504225</td>\n",
              "      <td>0.632527</td>\n",
              "      <td>0.315737</td>\n",
              "      <td>...</td>\n",
              "      <td>0.515889</td>\n",
              "      <td>0.644997</td>\n",
              "      <td>0.628199</td>\n",
              "      <td>0.567543</td>\n",
              "      <td>0.585247</td>\n",
              "      <td>-0.153647</td>\n",
              "      <td>-0.152362</td>\n",
              "      <td>0.058235</td>\n",
              "      <td>-0.144000</td>\n",
              "      <td>-0.128108</td>\n",
              "      <td>-0.113697</td>\n",
              "      <td>-0.162745</td>\n",
              "      <td>-0.113980</td>\n",
              "      <td>0.215041</td>\n",
              "      <td>-0.092078</td>\n",
              "      <td>-0.003277</td>\n",
              "      <td>-0.054556</td>\n",
              "      <td>-0.198592</td>\n",
              "      <td>-0.037467</td>\n",
              "      <td>-0.185474</td>\n",
              "      <td>0.089991</td>\n",
              "      <td>-0.120941</td>\n",
              "      <td>0.036560</td>\n",
              "      <td>-0.050977</td>\n",
              "      <td>-0.223516</td>\n",
              "      <td>0.136775</td>\n",
              "      <td>-0.169921</td>\n",
              "      <td>-0.201817</td>\n",
              "      <td>0.161516</td>\n",
              "      <td>0.193866</td>\n",
              "      <td>-0.056876</td>\n",
              "      <td>-0.176549</td>\n",
              "      <td>-0.045614</td>\n",
              "      <td>-0.004144</td>\n",
              "      <td>-0.109964</td>\n",
              "      <td>-0.093991</td>\n",
              "      <td>-0.174198</td>\n",
              "      <td>-0.101969</td>\n",
              "      <td>-0.126022</td>\n",
              "      <td>-0.089751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>subject</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996938</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.061534</td>\n",
              "      <td>0.005388</td>\n",
              "      <td>0.017788</td>\n",
              "      <td>0.041123</td>\n",
              "      <td>-0.107386</td>\n",
              "      <td>-0.019436</td>\n",
              "      <td>0.008824</td>\n",
              "      <td>0.010219</td>\n",
              "      <td>0.008794</td>\n",
              "      <td>0.014432</td>\n",
              "      <td>-0.045434</td>\n",
              "      <td>0.016150</td>\n",
              "      <td>0.056463</td>\n",
              "      <td>-0.147022</td>\n",
              "      <td>0.089541</td>\n",
              "      <td>0.019450</td>\n",
              "      <td>0.113110</td>\n",
              "      <td>0.046731</td>\n",
              "      <td>0.112174</td>\n",
              "      <td>0.147594</td>\n",
              "      <td>0.026776</td>\n",
              "      <td>0.124033</td>\n",
              "      <td>0.149724</td>\n",
              "      <td>-0.218425</td>\n",
              "      <td>-0.048111</td>\n",
              "      <td>0.238681</td>\n",
              "      <td>0.128181</td>\n",
              "      <td>0.323434</td>\n",
              "      <td>0.345196</td>\n",
              "      <td>0.230688</td>\n",
              "      <td>-0.733100</td>\n",
              "      <td>0.499556</td>\n",
              "      <td>-0.052422</td>\n",
              "      <td>0.830689</td>\n",
              "      <td>0.508347</td>\n",
              "      <td>0.636436</td>\n",
              "      <td>0.320192</td>\n",
              "      <td>...</td>\n",
              "      <td>0.521023</td>\n",
              "      <td>0.649602</td>\n",
              "      <td>0.633794</td>\n",
              "      <td>0.572617</td>\n",
              "      <td>0.590972</td>\n",
              "      <td>-0.153806</td>\n",
              "      <td>-0.152362</td>\n",
              "      <td>0.058235</td>\n",
              "      <td>-0.144000</td>\n",
              "      <td>-0.128108</td>\n",
              "      <td>-0.113697</td>\n",
              "      <td>-0.162745</td>\n",
              "      <td>-0.113980</td>\n",
              "      <td>0.215041</td>\n",
              "      <td>-0.092078</td>\n",
              "      <td>-0.003277</td>\n",
              "      <td>-0.054556</td>\n",
              "      <td>-0.198592</td>\n",
              "      <td>-0.037467</td>\n",
              "      <td>-0.185474</td>\n",
              "      <td>0.101219</td>\n",
              "      <td>-0.121469</td>\n",
              "      <td>0.030873</td>\n",
              "      <td>-0.074936</td>\n",
              "      <td>-0.232204</td>\n",
              "      <td>0.144431</td>\n",
              "      <td>-0.188087</td>\n",
              "      <td>-0.190360</td>\n",
              "      <td>0.165186</td>\n",
              "      <td>0.179996</td>\n",
              "      <td>-0.042194</td>\n",
              "      <td>-0.186669</td>\n",
              "      <td>-0.063495</td>\n",
              "      <td>0.000774</td>\n",
              "      <td>-0.107835</td>\n",
              "      <td>-0.103853</td>\n",
              "      <td>-0.187568</td>\n",
              "      <td>-0.114884</td>\n",
              "      <td>-0.170916</td>\n",
              "      <td>-0.092060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DX_GROUP</th>\n",
              "      <td>0.061534</td>\n",
              "      <td>0.071853</td>\n",
              "      <td>0.061534</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.811268</td>\n",
              "      <td>0.009793</td>\n",
              "      <td>0.071082</td>\n",
              "      <td>0.177723</td>\n",
              "      <td>0.147581</td>\n",
              "      <td>0.176438</td>\n",
              "      <td>0.045226</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.098919</td>\n",
              "      <td>-0.639783</td>\n",
              "      <td>-0.551117</td>\n",
              "      <td>-0.633477</td>\n",
              "      <td>-0.373408</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.501635</td>\n",
              "      <td>-0.364098</td>\n",
              "      <td>-0.521204</td>\n",
              "      <td>-0.585461</td>\n",
              "      <td>-0.185306</td>\n",
              "      <td>-0.808617</td>\n",
              "      <td>-0.389744</td>\n",
              "      <td>-0.613455</td>\n",
              "      <td>-0.583561</td>\n",
              "      <td>-0.602387</td>\n",
              "      <td>-0.544225</td>\n",
              "      <td>-0.794246</td>\n",
              "      <td>-0.595627</td>\n",
              "      <td>-0.448767</td>\n",
              "      <td>-0.483046</td>\n",
              "      <td>0.628458</td>\n",
              "      <td>0.770905</td>\n",
              "      <td>0.483257</td>\n",
              "      <td>...</td>\n",
              "      <td>0.614030</td>\n",
              "      <td>0.784141</td>\n",
              "      <td>0.778149</td>\n",
              "      <td>0.742125</td>\n",
              "      <td>0.772192</td>\n",
              "      <td>-0.157542</td>\n",
              "      <td>0.386499</td>\n",
              "      <td>0.007469</td>\n",
              "      <td>0.424915</td>\n",
              "      <td>0.464502</td>\n",
              "      <td>0.296531</td>\n",
              "      <td>0.391092</td>\n",
              "      <td>0.356194</td>\n",
              "      <td>-0.163912</td>\n",
              "      <td>0.243837</td>\n",
              "      <td>-0.002884</td>\n",
              "      <td>0.232693</td>\n",
              "      <td>0.514152</td>\n",
              "      <td>0.448588</td>\n",
              "      <td>0.469217</td>\n",
              "      <td>-0.008906</td>\n",
              "      <td>-0.028866</td>\n",
              "      <td>-0.098064</td>\n",
              "      <td>0.062730</td>\n",
              "      <td>0.034984</td>\n",
              "      <td>0.009036</td>\n",
              "      <td>-0.053019</td>\n",
              "      <td>-0.002342</td>\n",
              "      <td>-0.009302</td>\n",
              "      <td>-0.061583</td>\n",
              "      <td>0.036858</td>\n",
              "      <td>-0.007409</td>\n",
              "      <td>-0.024250</td>\n",
              "      <td>-0.094878</td>\n",
              "      <td>-0.161366</td>\n",
              "      <td>-0.188701</td>\n",
              "      <td>-0.178633</td>\n",
              "      <td>-0.189190</td>\n",
              "      <td>-0.046589</td>\n",
              "      <td>0.009193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DSM_IV_TR</th>\n",
              "      <td>0.005388</td>\n",
              "      <td>-0.004486</td>\n",
              "      <td>0.005388</td>\n",
              "      <td>-0.811268</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.013170</td>\n",
              "      <td>-0.035672</td>\n",
              "      <td>-0.099813</td>\n",
              "      <td>-0.100984</td>\n",
              "      <td>-0.086107</td>\n",
              "      <td>-0.027005</td>\n",
              "      <td>-0.136880</td>\n",
              "      <td>-0.078403</td>\n",
              "      <td>-0.169391</td>\n",
              "      <td>-0.072858</td>\n",
              "      <td>-0.135383</td>\n",
              "      <td>-0.055564</td>\n",
              "      <td>0.124891</td>\n",
              "      <td>0.166376</td>\n",
              "      <td>0.142582</td>\n",
              "      <td>0.228236</td>\n",
              "      <td>-0.200602</td>\n",
              "      <td>0.088066</td>\n",
              "      <td>0.107275</td>\n",
              "      <td>0.105453</td>\n",
              "      <td>0.163924</td>\n",
              "      <td>0.063521</td>\n",
              "      <td>0.654560</td>\n",
              "      <td>0.389744</td>\n",
              "      <td>0.613455</td>\n",
              "      <td>0.583561</td>\n",
              "      <td>0.602387</td>\n",
              "      <td>0.544225</td>\n",
              "      <td>0.671480</td>\n",
              "      <td>0.639433</td>\n",
              "      <td>0.301671</td>\n",
              "      <td>0.474424</td>\n",
              "      <td>-0.520132</td>\n",
              "      <td>-0.589437</td>\n",
              "      <td>-0.302145</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.430623</td>\n",
              "      <td>-0.616376</td>\n",
              "      <td>-0.606845</td>\n",
              "      <td>-0.586203</td>\n",
              "      <td>-0.610026</td>\n",
              "      <td>0.210102</td>\n",
              "      <td>-0.289315</td>\n",
              "      <td>0.016942</td>\n",
              "      <td>-0.423670</td>\n",
              "      <td>-0.446242</td>\n",
              "      <td>-0.253105</td>\n",
              "      <td>-0.283344</td>\n",
              "      <td>-0.262123</td>\n",
              "      <td>0.086048</td>\n",
              "      <td>-0.167842</td>\n",
              "      <td>0.058028</td>\n",
              "      <td>-0.219153</td>\n",
              "      <td>-0.526298</td>\n",
              "      <td>-0.442043</td>\n",
              "      <td>-0.466080</td>\n",
              "      <td>0.068466</td>\n",
              "      <td>0.028866</td>\n",
              "      <td>0.041280</td>\n",
              "      <td>0.020604</td>\n",
              "      <td>-0.040333</td>\n",
              "      <td>0.044272</td>\n",
              "      <td>0.092379</td>\n",
              "      <td>0.055533</td>\n",
              "      <td>0.030771</td>\n",
              "      <td>-0.030151</td>\n",
              "      <td>0.038029</td>\n",
              "      <td>0.115008</td>\n",
              "      <td>0.044851</td>\n",
              "      <td>0.108810</td>\n",
              "      <td>0.051346</td>\n",
              "      <td>0.108027</td>\n",
              "      <td>0.091915</td>\n",
              "      <td>0.111090</td>\n",
              "      <td>0.008813</td>\n",
              "      <td>-0.025474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>func_mean_fd</th>\n",
              "      <td>-0.103853</td>\n",
              "      <td>-0.093991</td>\n",
              "      <td>-0.103853</td>\n",
              "      <td>-0.188701</td>\n",
              "      <td>0.108027</td>\n",
              "      <td>-0.095414</td>\n",
              "      <td>-0.057336</td>\n",
              "      <td>-0.028221</td>\n",
              "      <td>-0.092272</td>\n",
              "      <td>-0.108552</td>\n",
              "      <td>-0.051714</td>\n",
              "      <td>-0.111444</td>\n",
              "      <td>-0.063304</td>\n",
              "      <td>0.025148</td>\n",
              "      <td>0.033580</td>\n",
              "      <td>-0.005952</td>\n",
              "      <td>-0.137287</td>\n",
              "      <td>0.080622</td>\n",
              "      <td>0.058304</td>\n",
              "      <td>0.100759</td>\n",
              "      <td>0.085815</td>\n",
              "      <td>-0.023684</td>\n",
              "      <td>0.132952</td>\n",
              "      <td>0.041767</td>\n",
              "      <td>0.085485</td>\n",
              "      <td>0.063666</td>\n",
              "      <td>-0.115764</td>\n",
              "      <td>0.203136</td>\n",
              "      <td>0.228713</td>\n",
              "      <td>0.005887</td>\n",
              "      <td>-0.037027</td>\n",
              "      <td>-0.018180</td>\n",
              "      <td>-0.019307</td>\n",
              "      <td>0.204968</td>\n",
              "      <td>-0.213587</td>\n",
              "      <td>0.218252</td>\n",
              "      <td>0.050100</td>\n",
              "      <td>-0.203463</td>\n",
              "      <td>-0.061662</td>\n",
              "      <td>0.021533</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.149587</td>\n",
              "      <td>-0.211023</td>\n",
              "      <td>-0.116255</td>\n",
              "      <td>-0.073703</td>\n",
              "      <td>-0.093648</td>\n",
              "      <td>-0.080934</td>\n",
              "      <td>-0.036307</td>\n",
              "      <td>0.050592</td>\n",
              "      <td>-0.255742</td>\n",
              "      <td>-0.240121</td>\n",
              "      <td>0.017814</td>\n",
              "      <td>-0.042249</td>\n",
              "      <td>-0.067943</td>\n",
              "      <td>0.113424</td>\n",
              "      <td>-0.043195</td>\n",
              "      <td>0.030825</td>\n",
              "      <td>-0.212282</td>\n",
              "      <td>-0.232271</td>\n",
              "      <td>-0.260778</td>\n",
              "      <td>-0.173871</td>\n",
              "      <td>-0.009533</td>\n",
              "      <td>-0.237395</td>\n",
              "      <td>0.262377</td>\n",
              "      <td>-0.063506</td>\n",
              "      <td>0.057898</td>\n",
              "      <td>-0.013141</td>\n",
              "      <td>-0.029087</td>\n",
              "      <td>0.023355</td>\n",
              "      <td>-0.047123</td>\n",
              "      <td>0.107233</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>0.074869</td>\n",
              "      <td>0.056566</td>\n",
              "      <td>0.545661</td>\n",
              "      <td>0.764164</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.827177</td>\n",
              "      <td>0.870692</td>\n",
              "      <td>0.170800</td>\n",
              "      <td>-0.268946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>func_num_fd</th>\n",
              "      <td>-0.187568</td>\n",
              "      <td>-0.174198</td>\n",
              "      <td>-0.187568</td>\n",
              "      <td>-0.178633</td>\n",
              "      <td>0.091915</td>\n",
              "      <td>0.012844</td>\n",
              "      <td>-0.041644</td>\n",
              "      <td>0.023696</td>\n",
              "      <td>-0.073006</td>\n",
              "      <td>-0.077327</td>\n",
              "      <td>-0.036196</td>\n",
              "      <td>-0.051566</td>\n",
              "      <td>-0.036979</td>\n",
              "      <td>0.039876</td>\n",
              "      <td>0.020205</td>\n",
              "      <td>0.016954</td>\n",
              "      <td>-0.065893</td>\n",
              "      <td>0.107401</td>\n",
              "      <td>0.101518</td>\n",
              "      <td>0.111257</td>\n",
              "      <td>0.093904</td>\n",
              "      <td>-0.006420</td>\n",
              "      <td>0.089888</td>\n",
              "      <td>0.053118</td>\n",
              "      <td>0.060782</td>\n",
              "      <td>0.049024</td>\n",
              "      <td>-0.135980</td>\n",
              "      <td>0.161109</td>\n",
              "      <td>0.131778</td>\n",
              "      <td>-0.036701</td>\n",
              "      <td>-0.111871</td>\n",
              "      <td>-0.113329</td>\n",
              "      <td>0.011951</td>\n",
              "      <td>0.221132</td>\n",
              "      <td>-0.211537</td>\n",
              "      <td>0.195791</td>\n",
              "      <td>-0.152187</td>\n",
              "      <td>-0.241257</td>\n",
              "      <td>-0.044094</td>\n",
              "      <td>0.011600</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.141141</td>\n",
              "      <td>-0.223819</td>\n",
              "      <td>-0.129844</td>\n",
              "      <td>-0.098052</td>\n",
              "      <td>-0.117477</td>\n",
              "      <td>-0.070559</td>\n",
              "      <td>-0.151661</td>\n",
              "      <td>-0.021271</td>\n",
              "      <td>-0.314575</td>\n",
              "      <td>-0.302780</td>\n",
              "      <td>-0.077998</td>\n",
              "      <td>-0.149898</td>\n",
              "      <td>-0.174062</td>\n",
              "      <td>0.048797</td>\n",
              "      <td>-0.076156</td>\n",
              "      <td>-0.029765</td>\n",
              "      <td>-0.252592</td>\n",
              "      <td>-0.289074</td>\n",
              "      <td>-0.327323</td>\n",
              "      <td>-0.220279</td>\n",
              "      <td>0.074494</td>\n",
              "      <td>-0.249114</td>\n",
              "      <td>0.314517</td>\n",
              "      <td>0.037561</td>\n",
              "      <td>0.089276</td>\n",
              "      <td>-0.004687</td>\n",
              "      <td>-0.005608</td>\n",
              "      <td>0.033922</td>\n",
              "      <td>-0.049038</td>\n",
              "      <td>0.218365</td>\n",
              "      <td>-0.135344</td>\n",
              "      <td>0.015232</td>\n",
              "      <td>0.046710</td>\n",
              "      <td>0.394561</td>\n",
              "      <td>0.709604</td>\n",
              "      <td>0.827177</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.941631</td>\n",
              "      <td>0.254488</td>\n",
              "      <td>-0.266672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>func_perc_fd</th>\n",
              "      <td>-0.114884</td>\n",
              "      <td>-0.101969</td>\n",
              "      <td>-0.114884</td>\n",
              "      <td>-0.189190</td>\n",
              "      <td>0.111090</td>\n",
              "      <td>-0.033542</td>\n",
              "      <td>-0.057742</td>\n",
              "      <td>0.011241</td>\n",
              "      <td>-0.090670</td>\n",
              "      <td>-0.107655</td>\n",
              "      <td>-0.040368</td>\n",
              "      <td>-0.059761</td>\n",
              "      <td>-0.034857</td>\n",
              "      <td>0.056252</td>\n",
              "      <td>0.051558</td>\n",
              "      <td>-0.013951</td>\n",
              "      <td>-0.115747</td>\n",
              "      <td>0.099245</td>\n",
              "      <td>0.078648</td>\n",
              "      <td>0.117838</td>\n",
              "      <td>0.094427</td>\n",
              "      <td>-0.041210</td>\n",
              "      <td>0.097617</td>\n",
              "      <td>0.003913</td>\n",
              "      <td>0.055367</td>\n",
              "      <td>0.057151</td>\n",
              "      <td>-0.176745</td>\n",
              "      <td>0.164686</td>\n",
              "      <td>0.131778</td>\n",
              "      <td>-0.036701</td>\n",
              "      <td>-0.111871</td>\n",
              "      <td>-0.113329</td>\n",
              "      <td>0.011951</td>\n",
              "      <td>0.221132</td>\n",
              "      <td>-0.204241</td>\n",
              "      <td>0.202616</td>\n",
              "      <td>-0.015490</td>\n",
              "      <td>-0.241257</td>\n",
              "      <td>-0.044094</td>\n",
              "      <td>0.011600</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.141141</td>\n",
              "      <td>-0.223819</td>\n",
              "      <td>-0.129844</td>\n",
              "      <td>-0.098052</td>\n",
              "      <td>-0.117477</td>\n",
              "      <td>-0.070559</td>\n",
              "      <td>-0.156887</td>\n",
              "      <td>-0.021706</td>\n",
              "      <td>-0.282358</td>\n",
              "      <td>-0.311647</td>\n",
              "      <td>-0.082032</td>\n",
              "      <td>-0.155779</td>\n",
              "      <td>-0.181244</td>\n",
              "      <td>0.058414</td>\n",
              "      <td>-0.091487</td>\n",
              "      <td>-0.032572</td>\n",
              "      <td>-0.232512</td>\n",
              "      <td>-0.253790</td>\n",
              "      <td>-0.332161</td>\n",
              "      <td>-0.238692</td>\n",
              "      <td>0.070430</td>\n",
              "      <td>-0.249114</td>\n",
              "      <td>0.314517</td>\n",
              "      <td>0.005702</td>\n",
              "      <td>0.070983</td>\n",
              "      <td>-0.043632</td>\n",
              "      <td>-0.015457</td>\n",
              "      <td>0.081668</td>\n",
              "      <td>-0.057536</td>\n",
              "      <td>0.176001</td>\n",
              "      <td>-0.069156</td>\n",
              "      <td>0.007274</td>\n",
              "      <td>0.062404</td>\n",
              "      <td>0.436459</td>\n",
              "      <td>0.646963</td>\n",
              "      <td>0.870692</td>\n",
              "      <td>0.941631</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.239755</td>\n",
              "      <td>-0.237150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>func_gsr</th>\n",
              "      <td>-0.170916</td>\n",
              "      <td>-0.126022</td>\n",
              "      <td>-0.170916</td>\n",
              "      <td>-0.046589</td>\n",
              "      <td>0.008813</td>\n",
              "      <td>0.225023</td>\n",
              "      <td>-0.069180</td>\n",
              "      <td>0.106522</td>\n",
              "      <td>-0.032377</td>\n",
              "      <td>-0.034631</td>\n",
              "      <td>-0.037313</td>\n",
              "      <td>0.067415</td>\n",
              "      <td>0.039329</td>\n",
              "      <td>0.037448</td>\n",
              "      <td>-0.061699</td>\n",
              "      <td>0.155383</td>\n",
              "      <td>0.071159</td>\n",
              "      <td>-0.035125</td>\n",
              "      <td>0.007596</td>\n",
              "      <td>-0.060715</td>\n",
              "      <td>-0.210845</td>\n",
              "      <td>0.038661</td>\n",
              "      <td>-0.053664</td>\n",
              "      <td>-0.184526</td>\n",
              "      <td>-0.102169</td>\n",
              "      <td>-0.113758</td>\n",
              "      <td>-0.042500</td>\n",
              "      <td>0.033808</td>\n",
              "      <td>0.118434</td>\n",
              "      <td>0.097233</td>\n",
              "      <td>0.107958</td>\n",
              "      <td>0.307731</td>\n",
              "      <td>0.163524</td>\n",
              "      <td>-0.065946</td>\n",
              "      <td>0.014000</td>\n",
              "      <td>0.098999</td>\n",
              "      <td>-0.703269</td>\n",
              "      <td>-0.135545</td>\n",
              "      <td>-0.008584</td>\n",
              "      <td>-0.080690</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002109</td>\n",
              "      <td>0.004516</td>\n",
              "      <td>-0.003587</td>\n",
              "      <td>-0.047025</td>\n",
              "      <td>-0.035963</td>\n",
              "      <td>0.041297</td>\n",
              "      <td>-0.019268</td>\n",
              "      <td>0.013207</td>\n",
              "      <td>0.070208</td>\n",
              "      <td>0.144652</td>\n",
              "      <td>0.084967</td>\n",
              "      <td>-0.079997</td>\n",
              "      <td>0.008011</td>\n",
              "      <td>-0.051966</td>\n",
              "      <td>0.079401</td>\n",
              "      <td>-0.021304</td>\n",
              "      <td>0.097766</td>\n",
              "      <td>0.028900</td>\n",
              "      <td>0.176283</td>\n",
              "      <td>0.067605</td>\n",
              "      <td>0.252370</td>\n",
              "      <td>0.412165</td>\n",
              "      <td>0.135375</td>\n",
              "      <td>0.047280</td>\n",
              "      <td>0.054031</td>\n",
              "      <td>-0.104098</td>\n",
              "      <td>0.186061</td>\n",
              "      <td>0.159772</td>\n",
              "      <td>0.051077</td>\n",
              "      <td>0.541692</td>\n",
              "      <td>-0.577534</td>\n",
              "      <td>0.087184</td>\n",
              "      <td>0.196361</td>\n",
              "      <td>-0.029459</td>\n",
              "      <td>0.029267</td>\n",
              "      <td>0.170800</td>\n",
              "      <td>0.254488</td>\n",
              "      <td>0.239755</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.088651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SUB_IN_SMP</th>\n",
              "      <td>-0.092060</td>\n",
              "      <td>-0.089751</td>\n",
              "      <td>-0.092060</td>\n",
              "      <td>0.009193</td>\n",
              "      <td>-0.025474</td>\n",
              "      <td>-0.042340</td>\n",
              "      <td>-0.690323</td>\n",
              "      <td>-0.054052</td>\n",
              "      <td>-0.050115</td>\n",
              "      <td>-0.041810</td>\n",
              "      <td>-0.048885</td>\n",
              "      <td>0.072339</td>\n",
              "      <td>0.064559</td>\n",
              "      <td>0.075421</td>\n",
              "      <td>0.108870</td>\n",
              "      <td>0.047641</td>\n",
              "      <td>-0.028797</td>\n",
              "      <td>-0.073499</td>\n",
              "      <td>-0.052454</td>\n",
              "      <td>-0.081475</td>\n",
              "      <td>-0.064191</td>\n",
              "      <td>-0.006563</td>\n",
              "      <td>-0.097578</td>\n",
              "      <td>-0.004250</td>\n",
              "      <td>-0.074977</td>\n",
              "      <td>-0.071887</td>\n",
              "      <td>0.149399</td>\n",
              "      <td>0.039931</td>\n",
              "      <td>-0.172478</td>\n",
              "      <td>-0.215294</td>\n",
              "      <td>-0.149407</td>\n",
              "      <td>-0.123275</td>\n",
              "      <td>-0.195596</td>\n",
              "      <td>0.173935</td>\n",
              "      <td>-0.355302</td>\n",
              "      <td>-0.032009</td>\n",
              "      <td>0.011154</td>\n",
              "      <td>-0.214528</td>\n",
              "      <td>-0.102563</td>\n",
              "      <td>-0.250769</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.125532</td>\n",
              "      <td>-0.061899</td>\n",
              "      <td>-0.101219</td>\n",
              "      <td>-0.147231</td>\n",
              "      <td>-0.181298</td>\n",
              "      <td>-0.138240</td>\n",
              "      <td>-0.173332</td>\n",
              "      <td>0.103134</td>\n",
              "      <td>-0.242428</td>\n",
              "      <td>-0.205309</td>\n",
              "      <td>-0.143497</td>\n",
              "      <td>-0.129249</td>\n",
              "      <td>-0.157788</td>\n",
              "      <td>0.327646</td>\n",
              "      <td>-0.020479</td>\n",
              "      <td>-0.060742</td>\n",
              "      <td>-0.301127</td>\n",
              "      <td>-0.072838</td>\n",
              "      <td>-0.272210</td>\n",
              "      <td>0.010746</td>\n",
              "      <td>-0.135828</td>\n",
              "      <td>0.131547</td>\n",
              "      <td>0.272610</td>\n",
              "      <td>-0.063058</td>\n",
              "      <td>0.153164</td>\n",
              "      <td>-0.184131</td>\n",
              "      <td>0.045506</td>\n",
              "      <td>-0.044819</td>\n",
              "      <td>-0.170022</td>\n",
              "      <td>0.007878</td>\n",
              "      <td>0.005435</td>\n",
              "      <td>-0.080481</td>\n",
              "      <td>-0.009683</td>\n",
              "      <td>-0.160377</td>\n",
              "      <td>-0.144470</td>\n",
              "      <td>-0.268946</td>\n",
              "      <td>-0.266672</td>\n",
              "      <td>-0.237150</td>\n",
              "      <td>-0.088651</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>86 rows Ã— 86 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1649136b-7aa6-4434-9a1f-11bef80ca317')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1649136b-7aa6-4434-9a1f-11bef80ca317 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1649136b-7aa6-4434-9a1f-11bef80ca317');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                SUB_ID         X   subject  ...  func_perc_fd  func_gsr  SUB_IN_SMP\n",
              "SUB_ID        1.000000  0.996938  1.000000  ...     -0.114884 -0.170916   -0.092060\n",
              "X             0.996938  1.000000  0.996938  ...     -0.101969 -0.126022   -0.089751\n",
              "subject       1.000000  0.996938  1.000000  ...     -0.114884 -0.170916   -0.092060\n",
              "DX_GROUP      0.061534  0.071853  0.061534  ...     -0.189190 -0.046589    0.009193\n",
              "DSM_IV_TR     0.005388 -0.004486  0.005388  ...      0.111090  0.008813   -0.025474\n",
              "...                ...       ...       ...  ...           ...       ...         ...\n",
              "func_mean_fd -0.103853 -0.093991 -0.103853  ...      0.870692  0.170800   -0.268946\n",
              "func_num_fd  -0.187568 -0.174198 -0.187568  ...      0.941631  0.254488   -0.266672\n",
              "func_perc_fd -0.114884 -0.101969 -0.114884  ...      1.000000  0.239755   -0.237150\n",
              "func_gsr     -0.170916 -0.126022 -0.170916  ...      0.239755  1.000000   -0.088651\n",
              "SUB_IN_SMP   -0.092060 -0.089751 -0.092060  ...     -0.237150 -0.088651    1.000000\n",
              "\n",
              "[86 rows x 86 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "#drop the unnamed coulmns\n",
        "selection.drop(selection.columns[selection.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
        "\n",
        "#corelation between the selected data\n",
        "correlation_s = selection.corr()\n",
        "correlation_s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITCVQmGRNcZX"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "wF2TT23dNcZX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "2f8a1c70-b9f2-4d53-f7c2-ca3084885bef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-fc534065-9feb-47e9-a2fd-ed90d4e15210\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SUB_ID</th>\n",
              "      <th>SEX</th>\n",
              "      <th>HANDEDNESS_CATEGORY</th>\n",
              "      <th>AGE_AT_SCAN</th>\n",
              "      <th>FIQ</th>\n",
              "      <th>VIQ</th>\n",
              "      <th>PIQ</th>\n",
              "      <th>ADOS_TOTAL</th>\n",
              "      <th>ADI_R_SOCIAL_TOTAL_A</th>\n",
              "      <th>ADI_R_VERBAL_TOTAL_BV</th>\n",
              "      <th>ADI_RRB_TOTAL_C</th>\n",
              "      <th>ADI_R_ONSET_TOTAL_D</th>\n",
              "      <th>SUB_IN_SMP</th>\n",
              "      <th>CURRENT_MED_STATUS</th>\n",
              "      <th>DSM_IV_TR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50002</td>\n",
              "      <td>1</td>\n",
              "      <td>Ambi</td>\n",
              "      <td>16.77</td>\n",
              "      <td>103.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50003</td>\n",
              "      <td>1</td>\n",
              "      <td>R</td>\n",
              "      <td>24.45</td>\n",
              "      <td>124.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50004</td>\n",
              "      <td>1</td>\n",
              "      <td>R</td>\n",
              "      <td>19.09</td>\n",
              "      <td>113.0</td>\n",
              "      <td>108.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>50005</td>\n",
              "      <td>2</td>\n",
              "      <td>R</td>\n",
              "      <td>13.73</td>\n",
              "      <td>119.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50006</td>\n",
              "      <td>1</td>\n",
              "      <td>L</td>\n",
              "      <td>13.37</td>\n",
              "      <td>109.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>50007</td>\n",
              "      <td>1</td>\n",
              "      <td>R</td>\n",
              "      <td>17.78</td>\n",
              "      <td>110.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>50008</td>\n",
              "      <td>1</td>\n",
              "      <td>R</td>\n",
              "      <td>32.45</td>\n",
              "      <td>123.0</td>\n",
              "      <td>123.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>50009</td>\n",
              "      <td>1</td>\n",
              "      <td>R</td>\n",
              "      <td>33.86</td>\n",
              "      <td>126.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>50010</td>\n",
              "      <td>1</td>\n",
              "      <td>L</td>\n",
              "      <td>35.20</td>\n",
              "      <td>81.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>10.876344</td>\n",
              "      <td>19.705882</td>\n",
              "      <td>15.739003</td>\n",
              "      <td>6.094118</td>\n",
              "      <td>3.242424</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>50011</td>\n",
              "      <td>1</td>\n",
              "      <td>L</td>\n",
              "      <td>16.93</td>\n",
              "      <td>111.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc534065-9feb-47e9-a2fd-ed90d4e15210')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fc534065-9feb-47e9-a2fd-ed90d4e15210 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fc534065-9feb-47e9-a2fd-ed90d4e15210');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   SUB_ID  SEX HANDEDNESS_CATEGORY  ...  SUB_IN_SMP  CURRENT_MED_STATUS  DSM_IV_TR\n",
              "0   50002    1                Ambi  ...           1                 0.0        1.0\n",
              "1   50003    1                   R  ...           1                 1.0        1.0\n",
              "2   50004    1                   R  ...           1                 0.0        1.0\n",
              "3   50005    2                   R  ...           0                 1.0        1.0\n",
              "4   50006    1                   L  ...           1                 0.0        1.0\n",
              "5   50007    1                   R  ...           1                 0.0        1.0\n",
              "6   50008    1                   R  ...           1                 1.0        1.0\n",
              "7   50009    1                   R  ...           1                 1.0        1.0\n",
              "8   50010    1                   L  ...           1                 0.0        1.0\n",
              "9   50011    1                   L  ...           0                 0.0        1.0\n",
              "\n",
              "[10 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "f_dataset = selection.loc[:,['SUB_ID','SEX','HANDEDNESS_CATEGORY','AGE_AT_SCAN','FIQ','VIQ','PIQ','ADOS_TOTAL',\n",
        "                            'ADI_R_SOCIAL_TOTAL_A','ADI_R_VERBAL_TOTAL_BV','ADI_RRB_TOTAL_C','ADI_R_ONSET_TOTAL_D',\n",
        "                            'SUB_IN_SMP','CURRENT_MED_STATUS','DSM_IV_TR']]\n",
        "\n",
        "# The handedness field contains categorical data, since it has considerably large number of missing values, NaN's are replaced by string 'n/a' (not available) as a new category \n",
        "f_dataset.HANDEDNESS_CATEGORY = f_dataset.HANDEDNESS_CATEGORY.fillna('n/a')\n",
        "\n",
        "# Replacing other missing values with mean of respective columns\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "f_dataset.iloc[:, 3:12] = imputer.fit_transform(f_dataset.iloc[:, 3:12])\n",
        "        \n",
        "# Dropping all rows which still contain any NaN values\n",
        "f_dataset = f_dataset.dropna()\n",
        "f_dataset.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f_dataset['DSM_IV_TR'].value_counts())\n",
        "\n",
        "# Features in X and resultant classes in y\n",
        "X = np.array(f_dataset.iloc[:,1:14].values)\n",
        "y = np.array(f_dataset.iloc[:,14].values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbJeCVwPXUQ0",
        "outputId": "e6eba46f-1ecf-4bdd-ddb4-fd753d24ef99"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0    403\n",
            "1.0    231\n",
            "2.0     71\n",
            "3.0     14\n",
            "Name: DSM_IV_TR, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# One-hot-encoding categorical data of HANDEDNESS_CATEGORY\n",
        "columnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n",
        "X = columnTransformer.fit_transform(X)\n",
        "\n",
        "# Feature Scaling\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set in ratio 80:20\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ],
      "metadata": {
        "id": "IVsbb8HTXhLG"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2fkAT5BQgZh",
        "outputId": "233d8f5b-039b-40af-c3d2-ec29d314e5cf"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "9xBwk_5hNcZY"
      },
      "outputs": [],
      "source": [
        "#  Dropping all rows which still contain any NaN values\n",
        "f_dataset = f_dataset.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5uiHU_3NcZY"
      },
      "source": [
        "## Model Evaluation and predicting classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "ssZT_eqVNcZY"
      },
      "outputs": [],
      "source": [
        "model_list = ['Random Forest', 'Logistic Regression', 'kNN', 'SVM_rbf', 'ANN', 'XGBoost']\n",
        "accuracies = [0]*6\n",
        "auc_roc = [0]*6\n",
        "\n",
        "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
        "    lb = LabelBinarizer()\n",
        "    lb.fit(y_test)\n",
        "    y_test = lb.transform(y_test)\n",
        "    y_pred = lb.transform(y_pred)\n",
        "    return roc_auc_score(y_test, y_pred, average=average)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "UvL4hBvgNcZY"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Using the best parameters combination after tuning hyper-parameters\n",
        "rf_classifier = RandomForestClassifier(n_estimators = 45, min_samples_split = 3, max_depth = 15, \n",
        "                                       bootstrap = False, random_state = 42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "accuracies[0] = accuracy_score(y_test, y_pred)\n",
        "auc_roc[0] = multiclass_roc_auc_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "vz8ut9QnNcZZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(solver = 'saga', multi_class = 'ovr', max_iter = 500)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracies[1] = accuracy_score(y_test, y_pred)\n",
        "auc_roc[1] = multiclass_roc_auc_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "1-7EVNsvNcZZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "knn = KNeighborsClassifier(n_neighbors = 5, weights = 'distance')\n",
        "knn.fit(X_train, y_train) \n",
        "y_pred = knn.predict(X_test)  \n",
        "\n",
        "accuracies[2] = accuracy_score(y_test, y_pred)\n",
        "auc_roc[2] = multiclass_roc_auc_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "4mNLlgkiNcZZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC \n",
        "svm_model = SVC(kernel = 'rbf', gamma = 'auto', C=0.8)\n",
        "svm_model.fit(X_train, y_train) \n",
        "y_pred = svm_model.predict(X_test) \n",
        "\n",
        "accuracies[3] = accuracy_score(y_test, y_pred)\n",
        "auc_roc[3] = multiclass_roc_auc_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "RlnVCQHrNcZZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d24dc57-8eed-4ad2-9fca-114f05f12924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "26/26 [==============================] - 1s 3ms/step - loss: 1.3638 - accuracy: 0.3570\n",
            "Epoch 2/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.3165 - accuracy: 0.4513\n",
            "Epoch 3/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.2847 - accuracy: 0.4946\n",
            "Epoch 4/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.2305 - accuracy: 0.5178\n",
            "Epoch 5/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.1746 - accuracy: 0.5641\n",
            "Epoch 6/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.1103 - accuracy: 0.6167\n",
            "Epoch 7/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.0331 - accuracy: 0.6770\n",
            "Epoch 8/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.9468 - accuracy: 0.6847\n",
            "Epoch 9/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8808 - accuracy: 0.7110\n",
            "Epoch 10/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.8843 - accuracy: 0.6878\n",
            "Epoch 11/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8353 - accuracy: 0.7141\n",
            "Epoch 12/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7999 - accuracy: 0.7125\n",
            "Epoch 13/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.7663 - accuracy: 0.7280\n",
            "Epoch 14/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.7834 - accuracy: 0.7110\n",
            "Epoch 15/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7241 - accuracy: 0.7403\n",
            "Epoch 16/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7055 - accuracy: 0.7342\n",
            "Epoch 17/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.7134 - accuracy: 0.7372\n",
            "Epoch 18/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.7034 - accuracy: 0.7512\n",
            "Epoch 19/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6953 - accuracy: 0.7388\n",
            "Epoch 20/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6616 - accuracy: 0.7635\n",
            "Epoch 21/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6830 - accuracy: 0.7759\n",
            "Epoch 22/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6623 - accuracy: 0.7682\n",
            "Epoch 23/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6374 - accuracy: 0.7836\n",
            "Epoch 24/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6436 - accuracy: 0.7852\n",
            "Epoch 25/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6441 - accuracy: 0.7867\n",
            "Epoch 26/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6149 - accuracy: 0.7960\n",
            "Epoch 27/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6001 - accuracy: 0.7944\n",
            "Epoch 28/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5920 - accuracy: 0.7991\n",
            "Epoch 29/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6065 - accuracy: 0.7883\n",
            "Epoch 30/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6117 - accuracy: 0.8022\n",
            "Epoch 31/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5993 - accuracy: 0.7975\n",
            "Epoch 32/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5587 - accuracy: 0.8130\n",
            "Epoch 33/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5481 - accuracy: 0.8114\n",
            "Epoch 34/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5692 - accuracy: 0.8068\n",
            "Epoch 35/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5393 - accuracy: 0.8068\n",
            "Epoch 36/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5463 - accuracy: 0.8192\n",
            "Epoch 37/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5536 - accuracy: 0.8176\n",
            "Epoch 38/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5525 - accuracy: 0.8145\n",
            "Epoch 39/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5334 - accuracy: 0.8176\n",
            "Epoch 40/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5180 - accuracy: 0.8223\n",
            "Epoch 41/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5267 - accuracy: 0.8114\n",
            "Epoch 42/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5284 - accuracy: 0.8207\n",
            "Epoch 43/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5238 - accuracy: 0.8223\n",
            "Epoch 44/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5228 - accuracy: 0.8223\n",
            "Epoch 45/200\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.5261 - accuracy: 0.8207\n",
            "Epoch 46/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5049 - accuracy: 0.8253\n",
            "Epoch 47/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5080 - accuracy: 0.8362\n",
            "Epoch 48/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4770 - accuracy: 0.8362\n",
            "Epoch 49/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4977 - accuracy: 0.8408\n",
            "Epoch 50/200\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.5070 - accuracy: 0.8331\n",
            "Epoch 51/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5131 - accuracy: 0.8315\n",
            "Epoch 52/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4977 - accuracy: 0.8346\n",
            "Epoch 53/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4671 - accuracy: 0.8640\n",
            "Epoch 54/200\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.4931 - accuracy: 0.8454\n",
            "Epoch 55/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4841 - accuracy: 0.8377\n",
            "Epoch 56/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4825 - accuracy: 0.8454\n",
            "Epoch 57/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4696 - accuracy: 0.8315\n",
            "Epoch 58/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4670 - accuracy: 0.8408\n",
            "Epoch 59/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4839 - accuracy: 0.8362\n",
            "Epoch 60/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4681 - accuracy: 0.8501\n",
            "Epoch 61/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4734 - accuracy: 0.8362\n",
            "Epoch 62/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4708 - accuracy: 0.8346\n",
            "Epoch 63/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4758 - accuracy: 0.8331\n",
            "Epoch 64/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4627 - accuracy: 0.8331\n",
            "Epoch 65/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4528 - accuracy: 0.8516\n",
            "Epoch 66/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4538 - accuracy: 0.8516\n",
            "Epoch 67/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4535 - accuracy: 0.8516\n",
            "Epoch 68/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4523 - accuracy: 0.8377\n",
            "Epoch 69/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4600 - accuracy: 0.8408\n",
            "Epoch 70/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4447 - accuracy: 0.8501\n",
            "Epoch 71/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4342 - accuracy: 0.8346\n",
            "Epoch 72/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4474 - accuracy: 0.8547\n",
            "Epoch 73/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4444 - accuracy: 0.8454\n",
            "Epoch 74/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4266 - accuracy: 0.8423\n",
            "Epoch 75/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4397 - accuracy: 0.8393\n",
            "Epoch 76/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4556 - accuracy: 0.8470\n",
            "Epoch 77/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4295 - accuracy: 0.8501\n",
            "Epoch 78/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4173 - accuracy: 0.8393\n",
            "Epoch 79/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4564 - accuracy: 0.8454\n",
            "Epoch 80/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4197 - accuracy: 0.8377\n",
            "Epoch 81/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4221 - accuracy: 0.8454\n",
            "Epoch 82/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4010 - accuracy: 0.8439\n",
            "Epoch 83/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4404 - accuracy: 0.8393\n",
            "Epoch 84/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4464 - accuracy: 0.8423\n",
            "Epoch 85/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4133 - accuracy: 0.8501\n",
            "Epoch 86/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4420 - accuracy: 0.8393\n",
            "Epoch 87/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4215 - accuracy: 0.8362\n",
            "Epoch 88/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4394 - accuracy: 0.8547\n",
            "Epoch 89/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4355 - accuracy: 0.8454\n",
            "Epoch 90/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4262 - accuracy: 0.8516\n",
            "Epoch 91/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4124 - accuracy: 0.8439\n",
            "Epoch 92/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4170 - accuracy: 0.8377\n",
            "Epoch 93/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4023 - accuracy: 0.8547\n",
            "Epoch 94/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4169 - accuracy: 0.8516\n",
            "Epoch 95/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4147 - accuracy: 0.8501\n",
            "Epoch 96/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4200 - accuracy: 0.8516\n",
            "Epoch 97/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4380 - accuracy: 0.8423\n",
            "Epoch 98/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4264 - accuracy: 0.8485\n",
            "Epoch 99/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4130 - accuracy: 0.8516\n",
            "Epoch 100/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4123 - accuracy: 0.8501\n",
            "Epoch 101/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3964 - accuracy: 0.8485\n",
            "Epoch 102/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3849 - accuracy: 0.8640\n",
            "Epoch 103/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4157 - accuracy: 0.8439\n",
            "Epoch 104/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4209 - accuracy: 0.8532\n",
            "Epoch 105/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4272 - accuracy: 0.8485\n",
            "Epoch 106/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4015 - accuracy: 0.8454\n",
            "Epoch 107/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4155 - accuracy: 0.8284\n",
            "Epoch 108/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4023 - accuracy: 0.8532\n",
            "Epoch 109/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3943 - accuracy: 0.8439\n",
            "Epoch 110/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3943 - accuracy: 0.8454\n",
            "Epoch 111/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4050 - accuracy: 0.8501\n",
            "Epoch 112/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3868 - accuracy: 0.8609\n",
            "Epoch 113/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4086 - accuracy: 0.8423\n",
            "Epoch 114/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3936 - accuracy: 0.8470\n",
            "Epoch 115/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4085 - accuracy: 0.8485\n",
            "Epoch 116/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3860 - accuracy: 0.8609\n",
            "Epoch 117/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3964 - accuracy: 0.8532\n",
            "Epoch 118/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3864 - accuracy: 0.8640\n",
            "Epoch 119/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3820 - accuracy: 0.8578\n",
            "Epoch 120/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4075 - accuracy: 0.8408\n",
            "Epoch 121/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3862 - accuracy: 0.8485\n",
            "Epoch 122/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3894 - accuracy: 0.8609\n",
            "Epoch 123/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4096 - accuracy: 0.8501\n",
            "Epoch 124/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4015 - accuracy: 0.8408\n",
            "Epoch 125/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3905 - accuracy: 0.8563\n",
            "Epoch 126/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3776 - accuracy: 0.8594\n",
            "Epoch 127/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3886 - accuracy: 0.8594\n",
            "Epoch 128/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4065 - accuracy: 0.8516\n",
            "Epoch 129/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3932 - accuracy: 0.8423\n",
            "Epoch 130/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3832 - accuracy: 0.8563\n",
            "Epoch 131/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3917 - accuracy: 0.8501\n",
            "Epoch 132/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3942 - accuracy: 0.8532\n",
            "Epoch 133/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3744 - accuracy: 0.8547\n",
            "Epoch 134/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3811 - accuracy: 0.8470\n",
            "Epoch 135/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3965 - accuracy: 0.8516\n",
            "Epoch 136/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3842 - accuracy: 0.8454\n",
            "Epoch 137/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3808 - accuracy: 0.8655\n",
            "Epoch 138/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3830 - accuracy: 0.8501\n",
            "Epoch 139/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3837 - accuracy: 0.8532\n",
            "Epoch 140/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3775 - accuracy: 0.8640\n",
            "Epoch 141/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3944 - accuracy: 0.8470\n",
            "Epoch 142/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3719 - accuracy: 0.8501\n",
            "Epoch 143/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4017 - accuracy: 0.8423\n",
            "Epoch 144/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4093 - accuracy: 0.8532\n",
            "Epoch 145/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3712 - accuracy: 0.8485\n",
            "Epoch 146/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3806 - accuracy: 0.8578\n",
            "Epoch 147/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3972 - accuracy: 0.8346\n",
            "Epoch 148/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3730 - accuracy: 0.8702\n",
            "Epoch 149/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3847 - accuracy: 0.8516\n",
            "Epoch 150/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3824 - accuracy: 0.8501\n",
            "Epoch 151/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3814 - accuracy: 0.8532\n",
            "Epoch 152/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3603 - accuracy: 0.8779\n",
            "Epoch 153/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3557 - accuracy: 0.8702\n",
            "Epoch 154/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3755 - accuracy: 0.8594\n",
            "Epoch 155/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3754 - accuracy: 0.8671\n",
            "Epoch 156/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3547 - accuracy: 0.8624\n",
            "Epoch 157/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3812 - accuracy: 0.8547\n",
            "Epoch 158/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3865 - accuracy: 0.8516\n",
            "Epoch 159/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3737 - accuracy: 0.8609\n",
            "Epoch 160/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3631 - accuracy: 0.8655\n",
            "Epoch 161/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3804 - accuracy: 0.8516\n",
            "Epoch 162/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3694 - accuracy: 0.8671\n",
            "Epoch 163/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3834 - accuracy: 0.8563\n",
            "Epoch 164/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3783 - accuracy: 0.8532\n",
            "Epoch 165/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3925 - accuracy: 0.8516\n",
            "Epoch 166/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3630 - accuracy: 0.8470\n",
            "Epoch 167/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3885 - accuracy: 0.8516\n",
            "Epoch 168/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3654 - accuracy: 0.8609\n",
            "Epoch 169/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3831 - accuracy: 0.8624\n",
            "Epoch 170/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3847 - accuracy: 0.8594\n",
            "Epoch 171/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3736 - accuracy: 0.8609\n",
            "Epoch 172/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3741 - accuracy: 0.8516\n",
            "Epoch 173/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3593 - accuracy: 0.8594\n",
            "Epoch 174/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3625 - accuracy: 0.8563\n",
            "Epoch 175/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3644 - accuracy: 0.8686\n",
            "Epoch 176/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3504 - accuracy: 0.8748\n",
            "Epoch 177/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3543 - accuracy: 0.8640\n",
            "Epoch 178/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3744 - accuracy: 0.8563\n",
            "Epoch 179/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3711 - accuracy: 0.8547\n",
            "Epoch 180/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3631 - accuracy: 0.8717\n",
            "Epoch 181/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3494 - accuracy: 0.8624\n",
            "Epoch 182/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3694 - accuracy: 0.8686\n",
            "Epoch 183/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3659 - accuracy: 0.8733\n",
            "Epoch 184/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3477 - accuracy: 0.8841\n",
            "Epoch 185/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3712 - accuracy: 0.8640\n",
            "Epoch 186/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3550 - accuracy: 0.8640\n",
            "Epoch 187/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3583 - accuracy: 0.8764\n",
            "Epoch 188/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3643 - accuracy: 0.8794\n",
            "Epoch 189/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3379 - accuracy: 0.8671\n",
            "Epoch 190/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3597 - accuracy: 0.8671\n",
            "Epoch 191/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3265 - accuracy: 0.8887\n",
            "Epoch 192/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3373 - accuracy: 0.8841\n",
            "Epoch 193/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3473 - accuracy: 0.8748\n",
            "Epoch 194/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3603 - accuracy: 0.8655\n",
            "Epoch 195/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3454 - accuracy: 0.8779\n",
            "Epoch 196/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3598 - accuracy: 0.8624\n",
            "Epoch 197/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3475 - accuracy: 0.8764\n",
            "Epoch 198/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3383 - accuracy: 0.8794\n",
            "Epoch 199/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3520 - accuracy: 0.8671\n",
            "Epoch 200/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3526 - accuracy: 0.8748\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.5400 - accuracy: 0.8472\n",
            "Epoch 1/200\n",
            "26/26 [==============================] - 1s 2ms/step - loss: 1.3221 - accuracy: 0.3292\n",
            "Epoch 2/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.2801 - accuracy: 0.3709\n",
            "Epoch 3/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.2250 - accuracy: 0.3447\n",
            "Epoch 4/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.1802 - accuracy: 0.3663\n",
            "Epoch 5/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.1201 - accuracy: 0.4019\n",
            "Epoch 6/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.0730 - accuracy: 0.4730\n",
            "Epoch 7/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.0335 - accuracy: 0.5162\n",
            "Epoch 8/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.9988 - accuracy: 0.5703\n",
            "Epoch 9/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.9869 - accuracy: 0.5889\n",
            "Epoch 10/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9599 - accuracy: 0.6182\n",
            "Epoch 11/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.9694 - accuracy: 0.6074\n",
            "Epoch 12/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.9370 - accuracy: 0.6105\n",
            "Epoch 13/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9139 - accuracy: 0.6229\n",
            "Epoch 14/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.8851 - accuracy: 0.6291\n",
            "Epoch 15/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.8805 - accuracy: 0.6476\n",
            "Epoch 16/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.8484 - accuracy: 0.6754\n",
            "Epoch 17/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.8453 - accuracy: 0.6739\n",
            "Epoch 18/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.8373 - accuracy: 0.6754\n",
            "Epoch 19/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7805 - accuracy: 0.7110\n",
            "Epoch 20/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7798 - accuracy: 0.7002\n",
            "Epoch 21/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.7687 - accuracy: 0.7218\n",
            "Epoch 22/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.7197 - accuracy: 0.7218\n",
            "Epoch 23/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.7237 - accuracy: 0.7249\n",
            "Epoch 24/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6927 - accuracy: 0.7403\n",
            "Epoch 25/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6476 - accuracy: 0.7697\n",
            "Epoch 26/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6534 - accuracy: 0.7604\n",
            "Epoch 27/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6778 - accuracy: 0.7481\n",
            "Epoch 28/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6728 - accuracy: 0.7527\n",
            "Epoch 29/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6361 - accuracy: 0.7620\n",
            "Epoch 30/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6137 - accuracy: 0.7759\n",
            "Epoch 31/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6014 - accuracy: 0.7821\n",
            "Epoch 32/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6050 - accuracy: 0.7913\n",
            "Epoch 33/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5867 - accuracy: 0.7867\n",
            "Epoch 34/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6032 - accuracy: 0.7867\n",
            "Epoch 35/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5647 - accuracy: 0.7975\n",
            "Epoch 36/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5609 - accuracy: 0.8053\n",
            "Epoch 37/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5688 - accuracy: 0.7960\n",
            "Epoch 38/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5406 - accuracy: 0.7960\n",
            "Epoch 39/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5255 - accuracy: 0.8269\n",
            "Epoch 40/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5305 - accuracy: 0.8006\n",
            "Epoch 41/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4980 - accuracy: 0.8362\n",
            "Epoch 42/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5206 - accuracy: 0.8269\n",
            "Epoch 43/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5184 - accuracy: 0.8176\n",
            "Epoch 44/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5119 - accuracy: 0.8253\n",
            "Epoch 45/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5140 - accuracy: 0.8238\n",
            "Epoch 46/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5062 - accuracy: 0.8300\n",
            "Epoch 47/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4746 - accuracy: 0.8362\n",
            "Epoch 48/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4942 - accuracy: 0.8300\n",
            "Epoch 49/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4781 - accuracy: 0.8393\n",
            "Epoch 50/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4632 - accuracy: 0.8315\n",
            "Epoch 51/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4802 - accuracy: 0.8253\n",
            "Epoch 52/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4487 - accuracy: 0.8470\n",
            "Epoch 53/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4725 - accuracy: 0.8362\n",
            "Epoch 54/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4817 - accuracy: 0.8485\n",
            "Epoch 55/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4639 - accuracy: 0.8315\n",
            "Epoch 56/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4393 - accuracy: 0.8485\n",
            "Epoch 57/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4413 - accuracy: 0.8346\n",
            "Epoch 58/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4368 - accuracy: 0.8532\n",
            "Epoch 59/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4066 - accuracy: 0.8594\n",
            "Epoch 60/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4482 - accuracy: 0.8485\n",
            "Epoch 61/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4447 - accuracy: 0.8346\n",
            "Epoch 62/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4252 - accuracy: 0.8485\n",
            "Epoch 63/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4173 - accuracy: 0.8563\n",
            "Epoch 64/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4374 - accuracy: 0.8671\n",
            "Epoch 65/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4162 - accuracy: 0.8609\n",
            "Epoch 66/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4097 - accuracy: 0.8547\n",
            "Epoch 67/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4085 - accuracy: 0.8547\n",
            "Epoch 68/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4153 - accuracy: 0.8501\n",
            "Epoch 69/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4055 - accuracy: 0.8671\n",
            "Epoch 70/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4273 - accuracy: 0.8439\n",
            "Epoch 71/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3973 - accuracy: 0.8655\n",
            "Epoch 72/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3983 - accuracy: 0.8624\n",
            "Epoch 73/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4062 - accuracy: 0.8578\n",
            "Epoch 74/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4381 - accuracy: 0.8393\n",
            "Epoch 75/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3728 - accuracy: 0.8686\n",
            "Epoch 76/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4032 - accuracy: 0.8594\n",
            "Epoch 77/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3841 - accuracy: 0.8624\n",
            "Epoch 78/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3942 - accuracy: 0.8563\n",
            "Epoch 79/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3898 - accuracy: 0.8624\n",
            "Epoch 80/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4127 - accuracy: 0.8423\n",
            "Epoch 81/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3811 - accuracy: 0.8624\n",
            "Epoch 82/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3886 - accuracy: 0.8624\n",
            "Epoch 83/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3936 - accuracy: 0.8640\n",
            "Epoch 84/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4110 - accuracy: 0.8578\n",
            "Epoch 85/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4171 - accuracy: 0.8624\n",
            "Epoch 86/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3775 - accuracy: 0.8686\n",
            "Epoch 87/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4181 - accuracy: 0.8609\n",
            "Epoch 88/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3801 - accuracy: 0.8717\n",
            "Epoch 89/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3982 - accuracy: 0.8547\n",
            "Epoch 90/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3851 - accuracy: 0.8609\n",
            "Epoch 91/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3835 - accuracy: 0.8609\n",
            "Epoch 92/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3870 - accuracy: 0.8671\n",
            "Epoch 93/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3870 - accuracy: 0.8655\n",
            "Epoch 94/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3706 - accuracy: 0.8609\n",
            "Epoch 95/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3544 - accuracy: 0.8702\n",
            "Epoch 96/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3989 - accuracy: 0.8655\n",
            "Epoch 97/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3694 - accuracy: 0.8686\n",
            "Epoch 98/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3605 - accuracy: 0.8764\n",
            "Epoch 99/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3865 - accuracy: 0.8686\n",
            "Epoch 100/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3714 - accuracy: 0.8686\n",
            "Epoch 101/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3653 - accuracy: 0.8841\n",
            "Epoch 102/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3655 - accuracy: 0.8686\n",
            "Epoch 103/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4040 - accuracy: 0.8640\n",
            "Epoch 104/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3688 - accuracy: 0.8671\n",
            "Epoch 105/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3425 - accuracy: 0.8764\n",
            "Epoch 106/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3559 - accuracy: 0.8733\n",
            "Epoch 107/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3670 - accuracy: 0.8764\n",
            "Epoch 108/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3624 - accuracy: 0.8609\n",
            "Epoch 109/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3698 - accuracy: 0.8764\n",
            "Epoch 110/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3615 - accuracy: 0.8794\n",
            "Epoch 111/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3663 - accuracy: 0.8733\n",
            "Epoch 112/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3574 - accuracy: 0.8764\n",
            "Epoch 113/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3336 - accuracy: 0.8764\n",
            "Epoch 114/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3483 - accuracy: 0.8810\n",
            "Epoch 115/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3498 - accuracy: 0.8794\n",
            "Epoch 116/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3335 - accuracy: 0.8949\n",
            "Epoch 117/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3533 - accuracy: 0.8733\n",
            "Epoch 118/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3668 - accuracy: 0.8671\n",
            "Epoch 119/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3501 - accuracy: 0.8841\n",
            "Epoch 120/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3539 - accuracy: 0.8779\n",
            "Epoch 121/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3497 - accuracy: 0.8640\n",
            "Epoch 122/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3454 - accuracy: 0.8794\n",
            "Epoch 123/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3794 - accuracy: 0.8686\n",
            "Epoch 124/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3557 - accuracy: 0.8733\n",
            "Epoch 125/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3515 - accuracy: 0.8717\n",
            "Epoch 126/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3522 - accuracy: 0.8640\n",
            "Epoch 127/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3542 - accuracy: 0.8779\n",
            "Epoch 128/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3462 - accuracy: 0.8794\n",
            "Epoch 129/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3347 - accuracy: 0.8934\n",
            "Epoch 130/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3220 - accuracy: 0.8903\n",
            "Epoch 131/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3258 - accuracy: 0.8794\n",
            "Epoch 132/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3475 - accuracy: 0.8764\n",
            "Epoch 133/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3043 - accuracy: 0.8779\n",
            "Epoch 134/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3313 - accuracy: 0.8949\n",
            "Epoch 135/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3374 - accuracy: 0.8934\n",
            "Epoch 136/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3523 - accuracy: 0.8671\n",
            "Epoch 137/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3213 - accuracy: 0.8856\n",
            "Epoch 138/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3260 - accuracy: 0.8794\n",
            "Epoch 139/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3394 - accuracy: 0.8794\n",
            "Epoch 140/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3530 - accuracy: 0.8779\n",
            "Epoch 141/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3299 - accuracy: 0.8794\n",
            "Epoch 142/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3308 - accuracy: 0.8702\n",
            "Epoch 143/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3217 - accuracy: 0.8810\n",
            "Epoch 144/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3504 - accuracy: 0.8764\n",
            "Epoch 145/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3255 - accuracy: 0.8872\n",
            "Epoch 146/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3436 - accuracy: 0.8872\n",
            "Epoch 147/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3621 - accuracy: 0.8717\n",
            "Epoch 148/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3124 - accuracy: 0.8887\n",
            "Epoch 149/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3080 - accuracy: 0.8872\n",
            "Epoch 150/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3365 - accuracy: 0.8764\n",
            "Epoch 151/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3091 - accuracy: 0.8841\n",
            "Epoch 152/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3269 - accuracy: 0.8887\n",
            "Epoch 153/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3375 - accuracy: 0.8825\n",
            "Epoch 154/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3412 - accuracy: 0.8872\n",
            "Epoch 155/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3117 - accuracy: 0.8825\n",
            "Epoch 156/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3179 - accuracy: 0.8887\n",
            "Epoch 157/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3195 - accuracy: 0.8872\n",
            "Epoch 158/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3029 - accuracy: 0.8856\n",
            "Epoch 159/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3208 - accuracy: 0.8856\n",
            "Epoch 160/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3108 - accuracy: 0.8903\n",
            "Epoch 161/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3340 - accuracy: 0.8794\n",
            "Epoch 162/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3385 - accuracy: 0.8841\n",
            "Epoch 163/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3309 - accuracy: 0.8918\n",
            "Epoch 164/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3235 - accuracy: 0.8794\n",
            "Epoch 165/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3330 - accuracy: 0.8872\n",
            "Epoch 166/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3372 - accuracy: 0.8903\n",
            "Epoch 167/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3174 - accuracy: 0.8779\n",
            "Epoch 168/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3547 - accuracy: 0.8764\n",
            "Epoch 169/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3065 - accuracy: 0.8964\n",
            "Epoch 170/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3228 - accuracy: 0.8964\n",
            "Epoch 171/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3239 - accuracy: 0.8856\n",
            "Epoch 172/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3299 - accuracy: 0.8810\n",
            "Epoch 173/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3694 - accuracy: 0.8733\n",
            "Epoch 174/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2998 - accuracy: 0.8810\n",
            "Epoch 175/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3295 - accuracy: 0.8794\n",
            "Epoch 176/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3227 - accuracy: 0.8794\n",
            "Epoch 177/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3440 - accuracy: 0.8825\n",
            "Epoch 178/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3215 - accuracy: 0.8949\n",
            "Epoch 179/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3252 - accuracy: 0.8733\n",
            "Epoch 180/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3093 - accuracy: 0.8872\n",
            "Epoch 181/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3288 - accuracy: 0.8918\n",
            "Epoch 182/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3246 - accuracy: 0.8779\n",
            "Epoch 183/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3143 - accuracy: 0.8903\n",
            "Epoch 184/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3040 - accuracy: 0.8949\n",
            "Epoch 185/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3244 - accuracy: 0.8964\n",
            "Epoch 186/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3118 - accuracy: 0.8794\n",
            "Epoch 187/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2899 - accuracy: 0.8964\n",
            "Epoch 188/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3085 - accuracy: 0.9011\n",
            "Epoch 189/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3042 - accuracy: 0.8934\n",
            "Epoch 190/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3115 - accuracy: 0.8779\n",
            "Epoch 191/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3251 - accuracy: 0.8856\n",
            "Epoch 192/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3132 - accuracy: 0.8841\n",
            "Epoch 193/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3059 - accuracy: 0.8934\n",
            "Epoch 194/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2997 - accuracy: 0.8949\n",
            "Epoch 195/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2974 - accuracy: 0.8949\n",
            "Epoch 196/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3011 - accuracy: 0.8903\n",
            "Epoch 197/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3175 - accuracy: 0.8841\n",
            "Epoch 198/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2931 - accuracy: 0.8995\n",
            "Epoch 199/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2875 - accuracy: 0.8964\n",
            "Epoch 200/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2975 - accuracy: 0.8949\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.7881 - accuracy: 0.8611\n",
            "Epoch 1/200\n",
            "26/26 [==============================] - 1s 3ms/step - loss: 1.3159 - accuracy: 0.3771\n",
            "Epoch 2/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.2623 - accuracy: 0.4312\n",
            "Epoch 3/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.2410 - accuracy: 0.4621\n",
            "Epoch 4/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.1827 - accuracy: 0.4977\n",
            "Epoch 5/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.1580 - accuracy: 0.5580\n",
            "Epoch 6/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.1041 - accuracy: 0.5796\n",
            "Epoch 7/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.0870 - accuracy: 0.6213\n",
            "Epoch 8/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.0652 - accuracy: 0.6399\n",
            "Epoch 9/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.0220 - accuracy: 0.6847\n",
            "Epoch 10/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9969 - accuracy: 0.7002\n",
            "Epoch 11/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9936 - accuracy: 0.6924\n",
            "Epoch 12/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9660 - accuracy: 0.7048\n",
            "Epoch 13/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9634 - accuracy: 0.7048\n",
            "Epoch 14/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9205 - accuracy: 0.7295\n",
            "Epoch 15/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.8875 - accuracy: 0.7218\n",
            "Epoch 16/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8715 - accuracy: 0.7187\n",
            "Epoch 17/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8572 - accuracy: 0.7280\n",
            "Epoch 18/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.8336 - accuracy: 0.7434\n",
            "Epoch 19/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8162 - accuracy: 0.7434\n",
            "Epoch 20/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7924 - accuracy: 0.7342\n",
            "Epoch 21/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7828 - accuracy: 0.7264\n",
            "Epoch 22/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7373 - accuracy: 0.7512\n",
            "Epoch 23/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6937 - accuracy: 0.7697\n",
            "Epoch 24/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6629 - accuracy: 0.7821\n",
            "Epoch 25/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6574 - accuracy: 0.7666\n",
            "Epoch 26/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6356 - accuracy: 0.7898\n",
            "Epoch 27/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6389 - accuracy: 0.7759\n",
            "Epoch 28/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6405 - accuracy: 0.7790\n",
            "Epoch 29/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5983 - accuracy: 0.7852\n",
            "Epoch 30/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5889 - accuracy: 0.7913\n",
            "Epoch 31/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5755 - accuracy: 0.7944\n",
            "Epoch 32/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5623 - accuracy: 0.7759\n",
            "Epoch 33/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5782 - accuracy: 0.7852\n",
            "Epoch 34/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5802 - accuracy: 0.7759\n",
            "Epoch 35/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5512 - accuracy: 0.8006\n",
            "Epoch 36/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5549 - accuracy: 0.8130\n",
            "Epoch 37/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5198 - accuracy: 0.8099\n",
            "Epoch 38/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5390 - accuracy: 0.8130\n",
            "Epoch 39/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5229 - accuracy: 0.7960\n",
            "Epoch 40/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5052 - accuracy: 0.8269\n",
            "Epoch 41/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4915 - accuracy: 0.8253\n",
            "Epoch 42/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5297 - accuracy: 0.8145\n",
            "Epoch 43/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5239 - accuracy: 0.8161\n",
            "Epoch 44/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4902 - accuracy: 0.8269\n",
            "Epoch 45/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4730 - accuracy: 0.8269\n",
            "Epoch 46/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4880 - accuracy: 0.7929\n",
            "Epoch 47/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4741 - accuracy: 0.8300\n",
            "Epoch 48/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4565 - accuracy: 0.8346\n",
            "Epoch 49/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4820 - accuracy: 0.8238\n",
            "Epoch 50/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4699 - accuracy: 0.8485\n",
            "Epoch 51/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4818 - accuracy: 0.8315\n",
            "Epoch 52/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4762 - accuracy: 0.8269\n",
            "Epoch 53/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4617 - accuracy: 0.8331\n",
            "Epoch 54/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4605 - accuracy: 0.8331\n",
            "Epoch 55/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4549 - accuracy: 0.8300\n",
            "Epoch 56/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4842 - accuracy: 0.8284\n",
            "Epoch 57/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4491 - accuracy: 0.8346\n",
            "Epoch 58/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4262 - accuracy: 0.8454\n",
            "Epoch 59/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4428 - accuracy: 0.8439\n",
            "Epoch 60/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4478 - accuracy: 0.8439\n",
            "Epoch 61/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4423 - accuracy: 0.8501\n",
            "Epoch 62/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4482 - accuracy: 0.8470\n",
            "Epoch 63/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4445 - accuracy: 0.8346\n",
            "Epoch 64/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4387 - accuracy: 0.8346\n",
            "Epoch 65/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4295 - accuracy: 0.8408\n",
            "Epoch 66/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4241 - accuracy: 0.8485\n",
            "Epoch 67/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4063 - accuracy: 0.8315\n",
            "Epoch 68/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4242 - accuracy: 0.8408\n",
            "Epoch 69/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4307 - accuracy: 0.8393\n",
            "Epoch 70/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4016 - accuracy: 0.8516\n",
            "Epoch 71/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4016 - accuracy: 0.8563\n",
            "Epoch 72/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4074 - accuracy: 0.8547\n",
            "Epoch 73/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4059 - accuracy: 0.8485\n",
            "Epoch 74/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4211 - accuracy: 0.8346\n",
            "Epoch 75/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4281 - accuracy: 0.8331\n",
            "Epoch 76/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4134 - accuracy: 0.8485\n",
            "Epoch 77/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4140 - accuracy: 0.8501\n",
            "Epoch 78/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3954 - accuracy: 0.8393\n",
            "Epoch 79/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4028 - accuracy: 0.8578\n",
            "Epoch 80/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3848 - accuracy: 0.8655\n",
            "Epoch 81/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3981 - accuracy: 0.8578\n",
            "Epoch 82/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3961 - accuracy: 0.8624\n",
            "Epoch 83/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3939 - accuracy: 0.8532\n",
            "Epoch 84/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4040 - accuracy: 0.8594\n",
            "Epoch 85/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4153 - accuracy: 0.8655\n",
            "Epoch 86/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3729 - accuracy: 0.8779\n",
            "Epoch 87/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3816 - accuracy: 0.8609\n",
            "Epoch 88/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3870 - accuracy: 0.8609\n",
            "Epoch 89/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3525 - accuracy: 0.8702\n",
            "Epoch 90/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3650 - accuracy: 0.8779\n",
            "Epoch 91/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4018 - accuracy: 0.8501\n",
            "Epoch 92/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3922 - accuracy: 0.8624\n",
            "Epoch 93/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3525 - accuracy: 0.8686\n",
            "Epoch 94/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3681 - accuracy: 0.8686\n",
            "Epoch 95/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3861 - accuracy: 0.8578\n",
            "Epoch 96/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3701 - accuracy: 0.8594\n",
            "Epoch 97/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3817 - accuracy: 0.8516\n",
            "Epoch 98/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3789 - accuracy: 0.8655\n",
            "Epoch 99/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3610 - accuracy: 0.8640\n",
            "Epoch 100/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3945 - accuracy: 0.8686\n",
            "Epoch 101/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3689 - accuracy: 0.8655\n",
            "Epoch 102/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3665 - accuracy: 0.8671\n",
            "Epoch 103/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3437 - accuracy: 0.8794\n",
            "Epoch 104/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3723 - accuracy: 0.8594\n",
            "Epoch 105/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3716 - accuracy: 0.8563\n",
            "Epoch 106/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3851 - accuracy: 0.8640\n",
            "Epoch 107/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3600 - accuracy: 0.8779\n",
            "Epoch 108/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3586 - accuracy: 0.8717\n",
            "Epoch 109/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3667 - accuracy: 0.8671\n",
            "Epoch 110/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3719 - accuracy: 0.8702\n",
            "Epoch 111/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3460 - accuracy: 0.8825\n",
            "Epoch 112/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3346 - accuracy: 0.8779\n",
            "Epoch 113/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3711 - accuracy: 0.8578\n",
            "Epoch 114/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3472 - accuracy: 0.8872\n",
            "Epoch 115/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3420 - accuracy: 0.8779\n",
            "Epoch 116/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3365 - accuracy: 0.8717\n",
            "Epoch 117/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3380 - accuracy: 0.8779\n",
            "Epoch 118/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3457 - accuracy: 0.8686\n",
            "Epoch 119/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3698 - accuracy: 0.8671\n",
            "Epoch 120/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3410 - accuracy: 0.8717\n",
            "Epoch 121/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3427 - accuracy: 0.8841\n",
            "Epoch 122/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3678 - accuracy: 0.8702\n",
            "Epoch 123/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3505 - accuracy: 0.8671\n",
            "Epoch 124/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3392 - accuracy: 0.8764\n",
            "Epoch 125/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3550 - accuracy: 0.8794\n",
            "Epoch 126/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3376 - accuracy: 0.8856\n",
            "Epoch 127/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3642 - accuracy: 0.8733\n",
            "Epoch 128/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3538 - accuracy: 0.8764\n",
            "Epoch 129/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3787 - accuracy: 0.8655\n",
            "Epoch 130/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3830 - accuracy: 0.8624\n",
            "Epoch 131/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3566 - accuracy: 0.8810\n",
            "Epoch 132/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3452 - accuracy: 0.8702\n",
            "Epoch 133/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3526 - accuracy: 0.8717\n",
            "Epoch 134/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3199 - accuracy: 0.8903\n",
            "Epoch 135/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3350 - accuracy: 0.8841\n",
            "Epoch 136/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3367 - accuracy: 0.8949\n",
            "Epoch 137/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3361 - accuracy: 0.8764\n",
            "Epoch 138/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3399 - accuracy: 0.8671\n",
            "Epoch 139/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3220 - accuracy: 0.8825\n",
            "Epoch 140/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3559 - accuracy: 0.8779\n",
            "Epoch 141/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3468 - accuracy: 0.8733\n",
            "Epoch 142/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3186 - accuracy: 0.8825\n",
            "Epoch 143/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3531 - accuracy: 0.8779\n",
            "Epoch 144/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3151 - accuracy: 0.8841\n",
            "Epoch 145/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3178 - accuracy: 0.8949\n",
            "Epoch 146/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3357 - accuracy: 0.8779\n",
            "Epoch 147/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3223 - accuracy: 0.8887\n",
            "Epoch 148/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3360 - accuracy: 0.8810\n",
            "Epoch 149/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2901 - accuracy: 0.8934\n",
            "Epoch 150/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3224 - accuracy: 0.8810\n",
            "Epoch 151/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3413 - accuracy: 0.8918\n",
            "Epoch 152/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3296 - accuracy: 0.8794\n",
            "Epoch 153/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3182 - accuracy: 0.8887\n",
            "Epoch 154/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3223 - accuracy: 0.8872\n",
            "Epoch 155/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3379 - accuracy: 0.8810\n",
            "Epoch 156/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3162 - accuracy: 0.8964\n",
            "Epoch 157/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3204 - accuracy: 0.8903\n",
            "Epoch 158/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3089 - accuracy: 0.8934\n",
            "Epoch 159/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3261 - accuracy: 0.8949\n",
            "Epoch 160/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3242 - accuracy: 0.8856\n",
            "Epoch 161/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3464 - accuracy: 0.8779\n",
            "Epoch 162/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3152 - accuracy: 0.8872\n",
            "Epoch 163/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3127 - accuracy: 0.8918\n",
            "Epoch 164/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3376 - accuracy: 0.8841\n",
            "Epoch 165/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3035 - accuracy: 0.9011\n",
            "Epoch 166/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3039 - accuracy: 0.8995\n",
            "Epoch 167/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3382 - accuracy: 0.8779\n",
            "Epoch 168/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2986 - accuracy: 0.8934\n",
            "Epoch 169/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3048 - accuracy: 0.8918\n",
            "Epoch 170/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3363 - accuracy: 0.8779\n",
            "Epoch 171/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3329 - accuracy: 0.8841\n",
            "Epoch 172/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3192 - accuracy: 0.8872\n",
            "Epoch 173/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3221 - accuracy: 0.8825\n",
            "Epoch 174/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3283 - accuracy: 0.8779\n",
            "Epoch 175/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2990 - accuracy: 0.8856\n",
            "Epoch 176/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3082 - accuracy: 0.8934\n",
            "Epoch 177/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3091 - accuracy: 0.8856\n",
            "Epoch 178/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3038 - accuracy: 0.8887\n",
            "Epoch 179/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3114 - accuracy: 0.8903\n",
            "Epoch 180/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3161 - accuracy: 0.8934\n",
            "Epoch 181/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3014 - accuracy: 0.8934\n",
            "Epoch 182/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3104 - accuracy: 0.8872\n",
            "Epoch 183/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3194 - accuracy: 0.8841\n",
            "Epoch 184/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3138 - accuracy: 0.8887\n",
            "Epoch 185/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3091 - accuracy: 0.8949\n",
            "Epoch 186/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3013 - accuracy: 0.8918\n",
            "Epoch 187/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3169 - accuracy: 0.8887\n",
            "Epoch 188/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3251 - accuracy: 0.8934\n",
            "Epoch 189/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2927 - accuracy: 0.9057\n",
            "Epoch 190/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2944 - accuracy: 0.8918\n",
            "Epoch 191/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3278 - accuracy: 0.8825\n",
            "Epoch 192/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3071 - accuracy: 0.8934\n",
            "Epoch 193/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3082 - accuracy: 0.8794\n",
            "Epoch 194/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3156 - accuracy: 0.8779\n",
            "Epoch 195/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3406 - accuracy: 0.8872\n",
            "Epoch 196/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3060 - accuracy: 0.8995\n",
            "Epoch 197/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3193 - accuracy: 0.8903\n",
            "Epoch 198/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3140 - accuracy: 0.8872\n",
            "Epoch 199/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3146 - accuracy: 0.8903\n",
            "Epoch 200/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2873 - accuracy: 0.9026\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 0.5616 - accuracy: 0.8472\n",
            "Epoch 1/200\n",
            "26/26 [==============================] - 1s 3ms/step - loss: 1.3947 - accuracy: 0.3292\n",
            "Epoch 2/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.3026 - accuracy: 0.5131\n",
            "Epoch 3/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.2181 - accuracy: 0.5533\n",
            "Epoch 4/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.1378 - accuracy: 0.5672\n",
            "Epoch 5/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.0715 - accuracy: 0.5719\n",
            "Epoch 6/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.0323 - accuracy: 0.5703\n",
            "Epoch 7/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.0058 - accuracy: 0.5734\n",
            "Epoch 8/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9701 - accuracy: 0.5734\n",
            "Epoch 9/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9741 - accuracy: 0.5719\n",
            "Epoch 10/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.9408 - accuracy: 0.5734\n",
            "Epoch 11/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9241 - accuracy: 0.5734\n",
            "Epoch 12/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9271 - accuracy: 0.5719\n",
            "Epoch 13/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8876 - accuracy: 0.5750\n",
            "Epoch 14/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8816 - accuracy: 0.5734\n",
            "Epoch 15/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8655 - accuracy: 0.5842\n",
            "Epoch 16/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8608 - accuracy: 0.5842\n",
            "Epoch 17/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8265 - accuracy: 0.5889\n",
            "Epoch 18/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8144 - accuracy: 0.6198\n",
            "Epoch 19/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7849 - accuracy: 0.6461\n",
            "Epoch 20/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7673 - accuracy: 0.6924\n",
            "Epoch 21/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7544 - accuracy: 0.7295\n",
            "Epoch 22/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7350 - accuracy: 0.7372\n",
            "Epoch 23/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.7156 - accuracy: 0.7651\n",
            "Epoch 24/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7222 - accuracy: 0.7759\n",
            "Epoch 25/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6890 - accuracy: 0.7867\n",
            "Epoch 26/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6844 - accuracy: 0.7975\n",
            "Epoch 27/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6715 - accuracy: 0.7944\n",
            "Epoch 28/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6561 - accuracy: 0.8053\n",
            "Epoch 29/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6405 - accuracy: 0.8130\n",
            "Epoch 30/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6529 - accuracy: 0.8083\n",
            "Epoch 31/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6252 - accuracy: 0.8176\n",
            "Epoch 32/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6005 - accuracy: 0.8192\n",
            "Epoch 33/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6221 - accuracy: 0.8176\n",
            "Epoch 34/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6183 - accuracy: 0.8068\n",
            "Epoch 35/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6070 - accuracy: 0.8223\n",
            "Epoch 36/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5854 - accuracy: 0.8300\n",
            "Epoch 37/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5727 - accuracy: 0.8238\n",
            "Epoch 38/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5756 - accuracy: 0.8315\n",
            "Epoch 39/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5727 - accuracy: 0.8269\n",
            "Epoch 40/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5427 - accuracy: 0.8393\n",
            "Epoch 41/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5618 - accuracy: 0.8300\n",
            "Epoch 42/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5471 - accuracy: 0.8346\n",
            "Epoch 43/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5565 - accuracy: 0.8284\n",
            "Epoch 44/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5289 - accuracy: 0.8315\n",
            "Epoch 45/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5393 - accuracy: 0.8362\n",
            "Epoch 46/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5110 - accuracy: 0.8393\n",
            "Epoch 47/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5227 - accuracy: 0.8346\n",
            "Epoch 48/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5296 - accuracy: 0.8331\n",
            "Epoch 49/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5089 - accuracy: 0.8470\n",
            "Epoch 50/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4931 - accuracy: 0.8439\n",
            "Epoch 51/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4975 - accuracy: 0.8377\n",
            "Epoch 52/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4964 - accuracy: 0.8393\n",
            "Epoch 53/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.8393\n",
            "Epoch 54/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4949 - accuracy: 0.8501\n",
            "Epoch 55/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4795 - accuracy: 0.8470\n",
            "Epoch 56/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4691 - accuracy: 0.8501\n",
            "Epoch 57/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4757 - accuracy: 0.8532\n",
            "Epoch 58/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4758 - accuracy: 0.8516\n",
            "Epoch 59/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4760 - accuracy: 0.8501\n",
            "Epoch 60/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4522 - accuracy: 0.8516\n",
            "Epoch 61/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4729 - accuracy: 0.8377\n",
            "Epoch 62/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4383 - accuracy: 0.8547\n",
            "Epoch 63/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4399 - accuracy: 0.8516\n",
            "Epoch 64/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4400 - accuracy: 0.8439\n",
            "Epoch 65/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4401 - accuracy: 0.8516\n",
            "Epoch 66/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4391 - accuracy: 0.8547\n",
            "Epoch 67/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4388 - accuracy: 0.8470\n",
            "Epoch 68/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4463 - accuracy: 0.8470\n",
            "Epoch 69/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4289 - accuracy: 0.8532\n",
            "Epoch 70/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4293 - accuracy: 0.8624\n",
            "Epoch 71/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4249 - accuracy: 0.8454\n",
            "Epoch 72/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4216 - accuracy: 0.8563\n",
            "Epoch 73/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4162 - accuracy: 0.8594\n",
            "Epoch 74/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4424 - accuracy: 0.8454\n",
            "Epoch 75/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4140 - accuracy: 0.8501\n",
            "Epoch 76/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4059 - accuracy: 0.8655\n",
            "Epoch 77/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4164 - accuracy: 0.8485\n",
            "Epoch 78/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4040 - accuracy: 0.8532\n",
            "Epoch 79/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4198 - accuracy: 0.8655\n",
            "Epoch 80/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3988 - accuracy: 0.8655\n",
            "Epoch 81/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4178 - accuracy: 0.8470\n",
            "Epoch 82/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3903 - accuracy: 0.8702\n",
            "Epoch 83/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4163 - accuracy: 0.8578\n",
            "Epoch 84/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3901 - accuracy: 0.8624\n",
            "Epoch 85/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3974 - accuracy: 0.8671\n",
            "Epoch 86/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3870 - accuracy: 0.8609\n",
            "Epoch 87/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3904 - accuracy: 0.8547\n",
            "Epoch 88/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4028 - accuracy: 0.8609\n",
            "Epoch 89/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3964 - accuracy: 0.8624\n",
            "Epoch 90/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3778 - accuracy: 0.8671\n",
            "Epoch 91/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3767 - accuracy: 0.8624\n",
            "Epoch 92/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3806 - accuracy: 0.8671\n",
            "Epoch 93/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3838 - accuracy: 0.8624\n",
            "Epoch 94/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3729 - accuracy: 0.8733\n",
            "Epoch 95/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3869 - accuracy: 0.8640\n",
            "Epoch 96/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3700 - accuracy: 0.8655\n",
            "Epoch 97/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3777 - accuracy: 0.8624\n",
            "Epoch 98/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3725 - accuracy: 0.8702\n",
            "Epoch 99/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3700 - accuracy: 0.8748\n",
            "Epoch 100/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3630 - accuracy: 0.8733\n",
            "Epoch 101/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3688 - accuracy: 0.8764\n",
            "Epoch 102/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3735 - accuracy: 0.8655\n",
            "Epoch 103/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3782 - accuracy: 0.8516\n",
            "Epoch 104/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3444 - accuracy: 0.8717\n",
            "Epoch 105/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3794 - accuracy: 0.8624\n",
            "Epoch 106/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3602 - accuracy: 0.8671\n",
            "Epoch 107/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3960 - accuracy: 0.8624\n",
            "Epoch 108/200\n",
            "26/26 [==============================] - 0s 13ms/step - loss: 0.3563 - accuracy: 0.8686\n",
            "Epoch 109/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3502 - accuracy: 0.8702\n",
            "Epoch 110/200\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3545 - accuracy: 0.8779\n",
            "Epoch 111/200\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3595 - accuracy: 0.8655\n",
            "Epoch 112/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3612 - accuracy: 0.8578\n",
            "Epoch 113/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3600 - accuracy: 0.8655\n",
            "Epoch 114/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3449 - accuracy: 0.8841\n",
            "Epoch 115/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3497 - accuracy: 0.8671\n",
            "Epoch 116/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3590 - accuracy: 0.8609\n",
            "Epoch 117/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3261 - accuracy: 0.8825\n",
            "Epoch 118/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3464 - accuracy: 0.8686\n",
            "Epoch 119/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3488 - accuracy: 0.8825\n",
            "Epoch 120/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3431 - accuracy: 0.8779\n",
            "Epoch 121/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3602 - accuracy: 0.8764\n",
            "Epoch 122/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3463 - accuracy: 0.8702\n",
            "Epoch 123/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3421 - accuracy: 0.8810\n",
            "Epoch 124/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3619 - accuracy: 0.8702\n",
            "Epoch 125/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3714 - accuracy: 0.8748\n",
            "Epoch 126/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3414 - accuracy: 0.8748\n",
            "Epoch 127/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3331 - accuracy: 0.8748\n",
            "Epoch 128/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3460 - accuracy: 0.8609\n",
            "Epoch 129/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3636 - accuracy: 0.8563\n",
            "Epoch 130/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3228 - accuracy: 0.8764\n",
            "Epoch 131/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3313 - accuracy: 0.8794\n",
            "Epoch 132/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.8702\n",
            "Epoch 133/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3294 - accuracy: 0.8764\n",
            "Epoch 134/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3467 - accuracy: 0.8794\n",
            "Epoch 135/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3404 - accuracy: 0.8810\n",
            "Epoch 136/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3457 - accuracy: 0.8810\n",
            "Epoch 137/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3530 - accuracy: 0.8748\n",
            "Epoch 138/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3345 - accuracy: 0.8779\n",
            "Epoch 139/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3155 - accuracy: 0.8825\n",
            "Epoch 140/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3232 - accuracy: 0.8810\n",
            "Epoch 141/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3375 - accuracy: 0.8794\n",
            "Epoch 142/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3243 - accuracy: 0.8856\n",
            "Epoch 143/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3447 - accuracy: 0.8748\n",
            "Epoch 144/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3303 - accuracy: 0.8825\n",
            "Epoch 145/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3497 - accuracy: 0.8779\n",
            "Epoch 146/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3279 - accuracy: 0.8655\n",
            "Epoch 147/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3050 - accuracy: 0.8748\n",
            "Epoch 148/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3302 - accuracy: 0.8794\n",
            "Epoch 149/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3324 - accuracy: 0.8794\n",
            "Epoch 150/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3245 - accuracy: 0.8872\n",
            "Epoch 151/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3337 - accuracy: 0.8810\n",
            "Epoch 152/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3298 - accuracy: 0.8764\n",
            "Epoch 153/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3060 - accuracy: 0.8841\n",
            "Epoch 154/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3162 - accuracy: 0.8903\n",
            "Epoch 155/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.8856\n",
            "Epoch 156/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3164 - accuracy: 0.8794\n",
            "Epoch 157/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3383 - accuracy: 0.8779\n",
            "Epoch 158/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3322 - accuracy: 0.8779\n",
            "Epoch 159/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3210 - accuracy: 0.8856\n",
            "Epoch 160/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3318 - accuracy: 0.8794\n",
            "Epoch 161/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2921 - accuracy: 0.8918\n",
            "Epoch 162/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3317 - accuracy: 0.8825\n",
            "Epoch 163/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3409 - accuracy: 0.8733\n",
            "Epoch 164/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3375 - accuracy: 0.8733\n",
            "Epoch 165/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3118 - accuracy: 0.8794\n",
            "Epoch 166/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3161 - accuracy: 0.8949\n",
            "Epoch 167/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3211 - accuracy: 0.8856\n",
            "Epoch 168/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3123 - accuracy: 0.8856\n",
            "Epoch 169/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3206 - accuracy: 0.8841\n",
            "Epoch 170/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3127 - accuracy: 0.8918\n",
            "Epoch 171/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3003 - accuracy: 0.8887\n",
            "Epoch 172/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3289 - accuracy: 0.8856\n",
            "Epoch 173/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3359 - accuracy: 0.8810\n",
            "Epoch 174/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3101 - accuracy: 0.8934\n",
            "Epoch 175/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3213 - accuracy: 0.8794\n",
            "Epoch 176/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3193 - accuracy: 0.8733\n",
            "Epoch 177/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3170 - accuracy: 0.8810\n",
            "Epoch 178/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3127 - accuracy: 0.8872\n",
            "Epoch 179/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3044 - accuracy: 0.8856\n",
            "Epoch 180/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3219 - accuracy: 0.8810\n",
            "Epoch 181/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2926 - accuracy: 0.8872\n",
            "Epoch 182/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3165 - accuracy: 0.8810\n",
            "Epoch 183/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3173 - accuracy: 0.8856\n",
            "Epoch 184/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3358 - accuracy: 0.8733\n",
            "Epoch 185/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3078 - accuracy: 0.8872\n",
            "Epoch 186/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3093 - accuracy: 0.8825\n",
            "Epoch 187/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3163 - accuracy: 0.8872\n",
            "Epoch 188/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.2965 - accuracy: 0.8918\n",
            "Epoch 189/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3032 - accuracy: 0.8810\n",
            "Epoch 190/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3082 - accuracy: 0.8825\n",
            "Epoch 191/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3063 - accuracy: 0.8872\n",
            "Epoch 192/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3185 - accuracy: 0.8810\n",
            "Epoch 193/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3057 - accuracy: 0.8872\n",
            "Epoch 194/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2970 - accuracy: 0.8934\n",
            "Epoch 195/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2987 - accuracy: 0.8903\n",
            "Epoch 196/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3225 - accuracy: 0.8856\n",
            "Epoch 197/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2974 - accuracy: 0.8856\n",
            "Epoch 198/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3204 - accuracy: 0.8810\n",
            "Epoch 199/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3072 - accuracy: 0.8825\n",
            "Epoch 200/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2808 - accuracy: 0.8872\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.7641 - accuracy: 0.7361\n",
            "Epoch 1/200\n",
            "26/26 [==============================] - 1s 3ms/step - loss: 1.3274 - accuracy: 0.3107\n",
            "Epoch 2/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.2883 - accuracy: 0.4389\n",
            "Epoch 3/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.1995 - accuracy: 0.5657\n",
            "Epoch 4/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.1178 - accuracy: 0.6105\n",
            "Epoch 5/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.0269 - accuracy: 0.6631\n",
            "Epoch 6/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9870 - accuracy: 0.6662\n",
            "Epoch 7/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9401 - accuracy: 0.6909\n",
            "Epoch 8/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8940 - accuracy: 0.7063\n",
            "Epoch 9/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8472 - accuracy: 0.7125\n",
            "Epoch 10/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8136 - accuracy: 0.7280\n",
            "Epoch 11/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7621 - accuracy: 0.7280\n",
            "Epoch 12/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7390 - accuracy: 0.7543\n",
            "Epoch 13/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7560 - accuracy: 0.7527\n",
            "Epoch 14/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6948 - accuracy: 0.7651\n",
            "Epoch 15/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7137 - accuracy: 0.7743\n",
            "Epoch 16/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6551 - accuracy: 0.7728\n",
            "Epoch 17/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6736 - accuracy: 0.7836\n",
            "Epoch 18/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6583 - accuracy: 0.7836\n",
            "Epoch 19/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6263 - accuracy: 0.7821\n",
            "Epoch 20/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6197 - accuracy: 0.8006\n",
            "Epoch 21/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6097 - accuracy: 0.7991\n",
            "Epoch 22/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6078 - accuracy: 0.8053\n",
            "Epoch 23/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5912 - accuracy: 0.8022\n",
            "Epoch 24/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5901 - accuracy: 0.7898\n",
            "Epoch 25/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5424 - accuracy: 0.8238\n",
            "Epoch 26/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5547 - accuracy: 0.8114\n",
            "Epoch 27/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5596 - accuracy: 0.8207\n",
            "Epoch 28/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5777 - accuracy: 0.8083\n",
            "Epoch 29/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5469 - accuracy: 0.8223\n",
            "Epoch 30/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5480 - accuracy: 0.8145\n",
            "Epoch 31/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5422 - accuracy: 0.8176\n",
            "Epoch 32/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5199 - accuracy: 0.8362\n",
            "Epoch 33/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5483 - accuracy: 0.8223\n",
            "Epoch 34/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5517 - accuracy: 0.8130\n",
            "Epoch 35/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5132 - accuracy: 0.8284\n",
            "Epoch 36/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5415 - accuracy: 0.8192\n",
            "Epoch 37/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.8362\n",
            "Epoch 38/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4991 - accuracy: 0.8223\n",
            "Epoch 39/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5144 - accuracy: 0.8300\n",
            "Epoch 40/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4979 - accuracy: 0.8315\n",
            "Epoch 41/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4878 - accuracy: 0.8362\n",
            "Epoch 42/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4914 - accuracy: 0.8331\n",
            "Epoch 43/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4691 - accuracy: 0.8377\n",
            "Epoch 44/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4974 - accuracy: 0.8362\n",
            "Epoch 45/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4974 - accuracy: 0.8315\n",
            "Epoch 46/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4771 - accuracy: 0.8470\n",
            "Epoch 47/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4748 - accuracy: 0.8485\n",
            "Epoch 48/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5032 - accuracy: 0.8315\n",
            "Epoch 49/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4921 - accuracy: 0.8454\n",
            "Epoch 50/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4900 - accuracy: 0.8439\n",
            "Epoch 51/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4752 - accuracy: 0.8485\n",
            "Epoch 52/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4480 - accuracy: 0.8501\n",
            "Epoch 53/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4667 - accuracy: 0.8408\n",
            "Epoch 54/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4896 - accuracy: 0.8315\n",
            "Epoch 55/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4590 - accuracy: 0.8485\n",
            "Epoch 56/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4561 - accuracy: 0.8563\n",
            "Epoch 57/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4573 - accuracy: 0.8547\n",
            "Epoch 58/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4293 - accuracy: 0.8547\n",
            "Epoch 59/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4454 - accuracy: 0.8470\n",
            "Epoch 60/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4550 - accuracy: 0.8454\n",
            "Epoch 61/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4299 - accuracy: 0.8408\n",
            "Epoch 62/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4481 - accuracy: 0.8485\n",
            "Epoch 63/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4266 - accuracy: 0.8485\n",
            "Epoch 64/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4289 - accuracy: 0.8454\n",
            "Epoch 65/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4155 - accuracy: 0.8454\n",
            "Epoch 66/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4198 - accuracy: 0.8516\n",
            "Epoch 67/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4159 - accuracy: 0.8578\n",
            "Epoch 68/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4268 - accuracy: 0.8501\n",
            "Epoch 69/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4264 - accuracy: 0.8516\n",
            "Epoch 70/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4335 - accuracy: 0.8362\n",
            "Epoch 71/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4124 - accuracy: 0.8532\n",
            "Epoch 72/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3966 - accuracy: 0.8624\n",
            "Epoch 73/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4252 - accuracy: 0.8640\n",
            "Epoch 74/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4220 - accuracy: 0.8501\n",
            "Epoch 75/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3959 - accuracy: 0.8686\n",
            "Epoch 76/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4294 - accuracy: 0.8393\n",
            "Epoch 77/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3793 - accuracy: 0.8764\n",
            "Epoch 78/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4050 - accuracy: 0.8578\n",
            "Epoch 79/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3913 - accuracy: 0.8702\n",
            "Epoch 80/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4262 - accuracy: 0.8454\n",
            "Epoch 81/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3960 - accuracy: 0.8655\n",
            "Epoch 82/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3944 - accuracy: 0.8671\n",
            "Epoch 83/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3912 - accuracy: 0.8733\n",
            "Epoch 84/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3926 - accuracy: 0.8594\n",
            "Epoch 85/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3736 - accuracy: 0.8717\n",
            "Epoch 86/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3681 - accuracy: 0.8655\n",
            "Epoch 87/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3784 - accuracy: 0.8733\n",
            "Epoch 88/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3693 - accuracy: 0.8640\n",
            "Epoch 89/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3763 - accuracy: 0.8655\n",
            "Epoch 90/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3578 - accuracy: 0.8671\n",
            "Epoch 91/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3799 - accuracy: 0.8671\n",
            "Epoch 92/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3753 - accuracy: 0.8702\n",
            "Epoch 93/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3632 - accuracy: 0.8655\n",
            "Epoch 94/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3535 - accuracy: 0.8748\n",
            "Epoch 95/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3631 - accuracy: 0.8733\n",
            "Epoch 96/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3578 - accuracy: 0.8825\n",
            "Epoch 97/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3743 - accuracy: 0.8717\n",
            "Epoch 98/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3557 - accuracy: 0.8702\n",
            "Epoch 99/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3377 - accuracy: 0.8779\n",
            "Epoch 100/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3639 - accuracy: 0.8655\n",
            "Epoch 101/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3569 - accuracy: 0.8717\n",
            "Epoch 102/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3502 - accuracy: 0.8702\n",
            "Epoch 103/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3579 - accuracy: 0.8841\n",
            "Epoch 104/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3241 - accuracy: 0.8825\n",
            "Epoch 105/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3312 - accuracy: 0.8841\n",
            "Epoch 106/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3656 - accuracy: 0.8764\n",
            "Epoch 107/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3297 - accuracy: 0.8779\n",
            "Epoch 108/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3446 - accuracy: 0.8825\n",
            "Epoch 109/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3545 - accuracy: 0.8686\n",
            "Epoch 110/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3538 - accuracy: 0.8779\n",
            "Epoch 111/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3115 - accuracy: 0.8841\n",
            "Epoch 112/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3617 - accuracy: 0.8655\n",
            "Epoch 113/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3243 - accuracy: 0.8887\n",
            "Epoch 114/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3229 - accuracy: 0.8810\n",
            "Epoch 115/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3400 - accuracy: 0.8733\n",
            "Epoch 116/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3330 - accuracy: 0.8779\n",
            "Epoch 117/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3639 - accuracy: 0.8748\n",
            "Epoch 118/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3760 - accuracy: 0.8640\n",
            "Epoch 119/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3329 - accuracy: 0.8918\n",
            "Epoch 120/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3172 - accuracy: 0.8856\n",
            "Epoch 121/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3499 - accuracy: 0.8733\n",
            "Epoch 122/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3344 - accuracy: 0.8733\n",
            "Epoch 123/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3432 - accuracy: 0.8764\n",
            "Epoch 124/200\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3393 - accuracy: 0.8702\n",
            "Epoch 125/200\n",
            "26/26 [==============================] - 0s 10ms/step - loss: 0.3287 - accuracy: 0.8686\n",
            "Epoch 126/200\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.3183 - accuracy: 0.8887\n",
            "Epoch 127/200\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.3136 - accuracy: 0.8903\n",
            "Epoch 128/200\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3427 - accuracy: 0.8748\n",
            "Epoch 129/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3418 - accuracy: 0.8717\n",
            "Epoch 130/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3408 - accuracy: 0.8825\n",
            "Epoch 131/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3288 - accuracy: 0.8748\n",
            "Epoch 132/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3350 - accuracy: 0.8748\n",
            "Epoch 133/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3309 - accuracy: 0.8717\n",
            "Epoch 134/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3315 - accuracy: 0.8810\n",
            "Epoch 135/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3291 - accuracy: 0.8748\n",
            "Epoch 136/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3262 - accuracy: 0.8825\n",
            "Epoch 137/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3139 - accuracy: 0.8764\n",
            "Epoch 138/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3200 - accuracy: 0.8733\n",
            "Epoch 139/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3074 - accuracy: 0.8872\n",
            "Epoch 140/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3031 - accuracy: 0.8903\n",
            "Epoch 141/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3115 - accuracy: 0.8748\n",
            "Epoch 142/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3191 - accuracy: 0.8825\n",
            "Epoch 143/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3113 - accuracy: 0.8733\n",
            "Epoch 144/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3141 - accuracy: 0.8872\n",
            "Epoch 145/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3122 - accuracy: 0.8779\n",
            "Epoch 146/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3219 - accuracy: 0.8686\n",
            "Epoch 147/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3073 - accuracy: 0.8748\n",
            "Epoch 148/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3289 - accuracy: 0.8686\n",
            "Epoch 149/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2911 - accuracy: 0.8995\n",
            "Epoch 150/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3154 - accuracy: 0.8825\n",
            "Epoch 151/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3069 - accuracy: 0.8872\n",
            "Epoch 152/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3194 - accuracy: 0.8825\n",
            "Epoch 153/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2987 - accuracy: 0.8872\n",
            "Epoch 154/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2960 - accuracy: 0.8810\n",
            "Epoch 155/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3138 - accuracy: 0.8872\n",
            "Epoch 156/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2912 - accuracy: 0.8872\n",
            "Epoch 157/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3332 - accuracy: 0.8825\n",
            "Epoch 158/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3119 - accuracy: 0.8671\n",
            "Epoch 159/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3030 - accuracy: 0.8702\n",
            "Epoch 160/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3010 - accuracy: 0.8918\n",
            "Epoch 161/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3005 - accuracy: 0.8779\n",
            "Epoch 162/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.2725 - accuracy: 0.8934\n",
            "Epoch 163/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2883 - accuracy: 0.8856\n",
            "Epoch 164/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3150 - accuracy: 0.8794\n",
            "Epoch 165/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2961 - accuracy: 0.8872\n",
            "Epoch 166/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3070 - accuracy: 0.8794\n",
            "Epoch 167/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2926 - accuracy: 0.8918\n",
            "Epoch 168/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3056 - accuracy: 0.8856\n",
            "Epoch 169/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2999 - accuracy: 0.8872\n",
            "Epoch 170/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2956 - accuracy: 0.8717\n",
            "Epoch 171/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2931 - accuracy: 0.8918\n",
            "Epoch 172/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2800 - accuracy: 0.8918\n",
            "Epoch 173/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3021 - accuracy: 0.8733\n",
            "Epoch 174/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3088 - accuracy: 0.8779\n",
            "Epoch 175/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2895 - accuracy: 0.8856\n",
            "Epoch 176/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3094 - accuracy: 0.8934\n",
            "Epoch 177/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2982 - accuracy: 0.8794\n",
            "Epoch 178/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2839 - accuracy: 0.8856\n",
            "Epoch 179/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2915 - accuracy: 0.8733\n",
            "Epoch 180/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2964 - accuracy: 0.8841\n",
            "Epoch 181/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2778 - accuracy: 0.8980\n",
            "Epoch 182/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2949 - accuracy: 0.8810\n",
            "Epoch 183/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2944 - accuracy: 0.8856\n",
            "Epoch 184/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2791 - accuracy: 0.8887\n",
            "Epoch 185/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2743 - accuracy: 0.8964\n",
            "Epoch 186/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2963 - accuracy: 0.8856\n",
            "Epoch 187/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2943 - accuracy: 0.8903\n",
            "Epoch 188/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2790 - accuracy: 0.8903\n",
            "Epoch 189/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2656 - accuracy: 0.8841\n",
            "Epoch 190/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2949 - accuracy: 0.8856\n",
            "Epoch 191/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3003 - accuracy: 0.8825\n",
            "Epoch 192/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2823 - accuracy: 0.8841\n",
            "Epoch 193/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2783 - accuracy: 0.8980\n",
            "Epoch 194/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2707 - accuracy: 0.8872\n",
            "Epoch 195/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2639 - accuracy: 0.8964\n",
            "Epoch 196/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3051 - accuracy: 0.8903\n",
            "Epoch 197/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2704 - accuracy: 0.9011\n",
            "Epoch 198/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2492 - accuracy: 0.8810\n",
            "Epoch 199/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2565 - accuracy: 0.9119\n",
            "Epoch 200/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2830 - accuracy: 0.8856\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.6237 - accuracy: 0.7917\n",
            "Epoch 1/200\n",
            "26/26 [==============================] - 1s 3ms/step - loss: 1.4075 - accuracy: 0.2056\n",
            "Epoch 2/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 1.3078 - accuracy: 0.4173\n",
            "Epoch 3/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.2074 - accuracy: 0.5240\n",
            "Epoch 4/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.1383 - accuracy: 0.5873\n",
            "Epoch 5/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.0921 - accuracy: 0.6151\n",
            "Epoch 6/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.0306 - accuracy: 0.6337\n",
            "Epoch 7/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9813 - accuracy: 0.6801\n",
            "Epoch 8/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9644 - accuracy: 0.6739\n",
            "Epoch 9/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9402 - accuracy: 0.6723\n",
            "Epoch 10/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9082 - accuracy: 0.6816\n",
            "Epoch 11/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8736 - accuracy: 0.7094\n",
            "Epoch 12/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8323 - accuracy: 0.7079\n",
            "Epoch 13/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8102 - accuracy: 0.7295\n",
            "Epoch 14/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7636 - accuracy: 0.7372\n",
            "Epoch 15/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7615 - accuracy: 0.7558\n",
            "Epoch 16/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7351 - accuracy: 0.7589\n",
            "Epoch 17/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7000 - accuracy: 0.7759\n",
            "Epoch 18/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6939 - accuracy: 0.7620\n",
            "Epoch 19/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6520 - accuracy: 0.7790\n",
            "Epoch 20/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6754 - accuracy: 0.7790\n",
            "Epoch 21/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6384 - accuracy: 0.7836\n",
            "Epoch 22/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6212 - accuracy: 0.7929\n",
            "Epoch 23/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6300 - accuracy: 0.7713\n",
            "Epoch 24/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6181 - accuracy: 0.7944\n",
            "Epoch 25/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5942 - accuracy: 0.7898\n",
            "Epoch 26/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5890 - accuracy: 0.7944\n",
            "Epoch 27/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5838 - accuracy: 0.8068\n",
            "Epoch 28/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5707 - accuracy: 0.7991\n",
            "Epoch 29/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5522 - accuracy: 0.8130\n",
            "Epoch 30/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5443 - accuracy: 0.8145\n",
            "Epoch 31/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5615 - accuracy: 0.8099\n",
            "Epoch 32/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5865 - accuracy: 0.7991\n",
            "Epoch 33/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5375 - accuracy: 0.8207\n",
            "Epoch 34/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5225 - accuracy: 0.8207\n",
            "Epoch 35/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.8083\n",
            "Epoch 36/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5277 - accuracy: 0.8238\n",
            "Epoch 37/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.8207\n",
            "Epoch 38/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.8114\n",
            "Epoch 39/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5312 - accuracy: 0.7913\n",
            "Epoch 40/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5299 - accuracy: 0.8192\n",
            "Epoch 41/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5156 - accuracy: 0.8207\n",
            "Epoch 42/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5098 - accuracy: 0.8300\n",
            "Epoch 43/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5001 - accuracy: 0.8315\n",
            "Epoch 44/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4729 - accuracy: 0.8362\n",
            "Epoch 45/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5028 - accuracy: 0.8269\n",
            "Epoch 46/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4901 - accuracy: 0.8331\n",
            "Epoch 47/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4940 - accuracy: 0.8238\n",
            "Epoch 48/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4853 - accuracy: 0.8362\n",
            "Epoch 49/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4705 - accuracy: 0.8423\n",
            "Epoch 50/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4700 - accuracy: 0.8377\n",
            "Epoch 51/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4904 - accuracy: 0.8346\n",
            "Epoch 52/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4471 - accuracy: 0.8501\n",
            "Epoch 53/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4688 - accuracy: 0.8423\n",
            "Epoch 54/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4849 - accuracy: 0.8408\n",
            "Epoch 55/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4510 - accuracy: 0.8501\n",
            "Epoch 56/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4581 - accuracy: 0.8408\n",
            "Epoch 57/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4581 - accuracy: 0.8423\n",
            "Epoch 58/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4477 - accuracy: 0.8393\n",
            "Epoch 59/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4393 - accuracy: 0.8454\n",
            "Epoch 60/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4373 - accuracy: 0.8547\n",
            "Epoch 61/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4398 - accuracy: 0.8423\n",
            "Epoch 62/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4364 - accuracy: 0.8609\n",
            "Epoch 63/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4401 - accuracy: 0.8516\n",
            "Epoch 64/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4390 - accuracy: 0.8578\n",
            "Epoch 65/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4337 - accuracy: 0.8547\n",
            "Epoch 66/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4217 - accuracy: 0.8547\n",
            "Epoch 67/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4256 - accuracy: 0.8516\n",
            "Epoch 68/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4272 - accuracy: 0.8516\n",
            "Epoch 69/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4403 - accuracy: 0.8532\n",
            "Epoch 70/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4197 - accuracy: 0.8624\n",
            "Epoch 71/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4206 - accuracy: 0.8532\n",
            "Epoch 72/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4161 - accuracy: 0.8532\n",
            "Epoch 73/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3988 - accuracy: 0.8640\n",
            "Epoch 74/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4254 - accuracy: 0.8686\n",
            "Epoch 75/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4122 - accuracy: 0.8563\n",
            "Epoch 76/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4176 - accuracy: 0.8609\n",
            "Epoch 77/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4163 - accuracy: 0.8547\n",
            "Epoch 78/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4590 - accuracy: 0.8362\n",
            "Epoch 79/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4091 - accuracy: 0.8655\n",
            "Epoch 80/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4156 - accuracy: 0.8470\n",
            "Epoch 81/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4123 - accuracy: 0.8671\n",
            "Epoch 82/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4044 - accuracy: 0.8454\n",
            "Epoch 83/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4015 - accuracy: 0.8686\n",
            "Epoch 84/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4032 - accuracy: 0.8501\n",
            "Epoch 85/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4009 - accuracy: 0.8640\n",
            "Epoch 86/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4221 - accuracy: 0.8532\n",
            "Epoch 87/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3927 - accuracy: 0.8624\n",
            "Epoch 88/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3909 - accuracy: 0.8686\n",
            "Epoch 89/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4125 - accuracy: 0.8547\n",
            "Epoch 90/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3890 - accuracy: 0.8717\n",
            "Epoch 91/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4287 - accuracy: 0.8547\n",
            "Epoch 92/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4021 - accuracy: 0.8733\n",
            "Epoch 93/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3821 - accuracy: 0.8609\n",
            "Epoch 94/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3864 - accuracy: 0.8594\n",
            "Epoch 95/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3978 - accuracy: 0.8470\n",
            "Epoch 96/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3888 - accuracy: 0.8702\n",
            "Epoch 97/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3979 - accuracy: 0.8578\n",
            "Epoch 98/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3708 - accuracy: 0.8702\n",
            "Epoch 99/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3928 - accuracy: 0.8686\n",
            "Epoch 100/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3800 - accuracy: 0.8717\n",
            "Epoch 101/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3767 - accuracy: 0.8748\n",
            "Epoch 102/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3987 - accuracy: 0.8779\n",
            "Epoch 103/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3754 - accuracy: 0.8717\n",
            "Epoch 104/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3881 - accuracy: 0.8733\n",
            "Epoch 105/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4015 - accuracy: 0.8640\n",
            "Epoch 106/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3593 - accuracy: 0.8825\n",
            "Epoch 107/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3940 - accuracy: 0.8578\n",
            "Epoch 108/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3792 - accuracy: 0.8640\n",
            "Epoch 109/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3536 - accuracy: 0.8764\n",
            "Epoch 110/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3639 - accuracy: 0.8825\n",
            "Epoch 111/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3859 - accuracy: 0.8779\n",
            "Epoch 112/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3638 - accuracy: 0.8748\n",
            "Epoch 113/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3829 - accuracy: 0.8671\n",
            "Epoch 114/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3757 - accuracy: 0.8686\n",
            "Epoch 115/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3527 - accuracy: 0.8748\n",
            "Epoch 116/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3590 - accuracy: 0.8794\n",
            "Epoch 117/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3454 - accuracy: 0.8872\n",
            "Epoch 118/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3851 - accuracy: 0.8594\n",
            "Epoch 119/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3669 - accuracy: 0.8702\n",
            "Epoch 120/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3679 - accuracy: 0.8655\n",
            "Epoch 121/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3450 - accuracy: 0.8702\n",
            "Epoch 122/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3606 - accuracy: 0.8810\n",
            "Epoch 123/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3825 - accuracy: 0.8624\n",
            "Epoch 124/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3696 - accuracy: 0.8609\n",
            "Epoch 125/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3585 - accuracy: 0.8856\n",
            "Epoch 126/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3720 - accuracy: 0.8794\n",
            "Epoch 127/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3493 - accuracy: 0.8872\n",
            "Epoch 128/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3495 - accuracy: 0.8717\n",
            "Epoch 129/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3483 - accuracy: 0.8825\n",
            "Epoch 130/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3553 - accuracy: 0.8825\n",
            "Epoch 131/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3708 - accuracy: 0.8609\n",
            "Epoch 132/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3623 - accuracy: 0.8779\n",
            "Epoch 133/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3450 - accuracy: 0.8856\n",
            "Epoch 134/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3510 - accuracy: 0.8841\n",
            "Epoch 135/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3564 - accuracy: 0.8779\n",
            "Epoch 136/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3293 - accuracy: 0.8887\n",
            "Epoch 137/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3479 - accuracy: 0.8794\n",
            "Epoch 138/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3674 - accuracy: 0.8717\n",
            "Epoch 139/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3505 - accuracy: 0.8856\n",
            "Epoch 140/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3565 - accuracy: 0.8717\n",
            "Epoch 141/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3503 - accuracy: 0.8733\n",
            "Epoch 142/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3357 - accuracy: 0.8856\n",
            "Epoch 143/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3617 - accuracy: 0.8779\n",
            "Epoch 144/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3392 - accuracy: 0.8903\n",
            "Epoch 145/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3457 - accuracy: 0.8717\n",
            "Epoch 146/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3624 - accuracy: 0.8764\n",
            "Epoch 147/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3391 - accuracy: 0.8841\n",
            "Epoch 148/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3260 - accuracy: 0.8856\n",
            "Epoch 149/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3602 - accuracy: 0.8764\n",
            "Epoch 150/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3405 - accuracy: 0.8918\n",
            "Epoch 151/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3585 - accuracy: 0.8733\n",
            "Epoch 152/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3392 - accuracy: 0.8856\n",
            "Epoch 153/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3334 - accuracy: 0.8887\n",
            "Epoch 154/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3394 - accuracy: 0.8841\n",
            "Epoch 155/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3308 - accuracy: 0.8841\n",
            "Epoch 156/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3545 - accuracy: 0.8856\n",
            "Epoch 157/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3405 - accuracy: 0.8903\n",
            "Epoch 158/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3125 - accuracy: 0.9011\n",
            "Epoch 159/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3238 - accuracy: 0.8872\n",
            "Epoch 160/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3298 - accuracy: 0.8887\n",
            "Epoch 161/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3504 - accuracy: 0.8794\n",
            "Epoch 162/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3258 - accuracy: 0.8748\n",
            "Epoch 163/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3104 - accuracy: 0.8995\n",
            "Epoch 164/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3568 - accuracy: 0.8856\n",
            "Epoch 165/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3260 - accuracy: 0.8733\n",
            "Epoch 166/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.8764\n",
            "Epoch 167/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3296 - accuracy: 0.8887\n",
            "Epoch 168/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3422 - accuracy: 0.8841\n",
            "Epoch 169/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3293 - accuracy: 0.8825\n",
            "Epoch 170/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3219 - accuracy: 0.8841\n",
            "Epoch 171/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3027 - accuracy: 0.8980\n",
            "Epoch 172/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3218 - accuracy: 0.8841\n",
            "Epoch 173/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3065 - accuracy: 0.8918\n",
            "Epoch 174/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3031 - accuracy: 0.8825\n",
            "Epoch 175/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3166 - accuracy: 0.8887\n",
            "Epoch 176/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3163 - accuracy: 0.8872\n",
            "Epoch 177/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2999 - accuracy: 0.8856\n",
            "Epoch 178/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3255 - accuracy: 0.8856\n",
            "Epoch 179/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3089 - accuracy: 0.8872\n",
            "Epoch 180/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3136 - accuracy: 0.8856\n",
            "Epoch 181/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3193 - accuracy: 0.8856\n",
            "Epoch 182/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3020 - accuracy: 0.8934\n",
            "Epoch 183/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3103 - accuracy: 0.8794\n",
            "Epoch 184/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3162 - accuracy: 0.8856\n",
            "Epoch 185/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3006 - accuracy: 0.8918\n",
            "Epoch 186/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3031 - accuracy: 0.8872\n",
            "Epoch 187/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2950 - accuracy: 0.8964\n",
            "Epoch 188/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2865 - accuracy: 0.9011\n",
            "Epoch 189/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2997 - accuracy: 0.8918\n",
            "Epoch 190/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2960 - accuracy: 0.9026\n",
            "Epoch 191/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2956 - accuracy: 0.9011\n",
            "Epoch 192/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3059 - accuracy: 0.8980\n",
            "Epoch 193/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.2885 - accuracy: 0.9073\n",
            "Epoch 194/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2996 - accuracy: 0.8980\n",
            "Epoch 195/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2913 - accuracy: 0.8934\n",
            "Epoch 196/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3262 - accuracy: 0.8856\n",
            "Epoch 197/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3044 - accuracy: 0.8903\n",
            "Epoch 198/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3022 - accuracy: 0.8872\n",
            "Epoch 199/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3142 - accuracy: 0.8995\n",
            "Epoch 200/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2968 - accuracy: 0.8949\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 1.0204 - accuracy: 0.8333\n",
            "Epoch 1/200\n",
            "26/26 [==============================] - 1s 3ms/step - loss: 1.3498 - accuracy: 0.5209\n",
            "Epoch 2/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.2728 - accuracy: 0.5611\n",
            "Epoch 3/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.2057 - accuracy: 0.5765\n",
            "Epoch 4/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.1510 - accuracy: 0.5827\n",
            "Epoch 5/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.0886 - accuracy: 0.5750\n",
            "Epoch 6/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 1.0622 - accuracy: 0.5935\n",
            "Epoch 7/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.0237 - accuracy: 0.6028\n",
            "Epoch 8/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9978 - accuracy: 0.6074\n",
            "Epoch 9/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9783 - accuracy: 0.6430\n",
            "Epoch 10/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.9912 - accuracy: 0.6059\n",
            "Epoch 11/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9655 - accuracy: 0.6167\n",
            "Epoch 12/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.9173 - accuracy: 0.6491\n",
            "Epoch 13/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9015 - accuracy: 0.6461\n",
            "Epoch 14/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8824 - accuracy: 0.6615\n",
            "Epoch 15/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8358 - accuracy: 0.6971\n",
            "Epoch 16/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8414 - accuracy: 0.6955\n",
            "Epoch 17/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7928 - accuracy: 0.7172\n",
            "Epoch 18/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.7819 - accuracy: 0.7311\n",
            "Epoch 19/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7595 - accuracy: 0.7388\n",
            "Epoch 20/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7730 - accuracy: 0.7094\n",
            "Epoch 21/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7481 - accuracy: 0.7187\n",
            "Epoch 22/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7558 - accuracy: 0.7172\n",
            "Epoch 23/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6982 - accuracy: 0.7388\n",
            "Epoch 24/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7145 - accuracy: 0.7388\n",
            "Epoch 25/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6989 - accuracy: 0.7233\n",
            "Epoch 26/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6733 - accuracy: 0.7496\n",
            "Epoch 27/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6884 - accuracy: 0.7172\n",
            "Epoch 28/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6437 - accuracy: 0.7697\n",
            "Epoch 29/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.6507 - accuracy: 0.7651\n",
            "Epoch 30/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6452 - accuracy: 0.7589\n",
            "Epoch 31/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6305 - accuracy: 0.7635\n",
            "Epoch 32/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.6022 - accuracy: 0.7620\n",
            "Epoch 33/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6039 - accuracy: 0.7743\n",
            "Epoch 34/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6191 - accuracy: 0.7635\n",
            "Epoch 35/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6089 - accuracy: 0.7651\n",
            "Epoch 36/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6135 - accuracy: 0.7666\n",
            "Epoch 37/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5921 - accuracy: 0.7697\n",
            "Epoch 38/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5702 - accuracy: 0.7774\n",
            "Epoch 39/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5895 - accuracy: 0.7852\n",
            "Epoch 40/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6046 - accuracy: 0.7635\n",
            "Epoch 41/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5730 - accuracy: 0.7682\n",
            "Epoch 42/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5460 - accuracy: 0.7913\n",
            "Epoch 43/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5669 - accuracy: 0.7944\n",
            "Epoch 44/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5675 - accuracy: 0.7913\n",
            "Epoch 45/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5505 - accuracy: 0.8022\n",
            "Epoch 46/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5590 - accuracy: 0.7944\n",
            "Epoch 47/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5353 - accuracy: 0.7991\n",
            "Epoch 48/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5151 - accuracy: 0.8130\n",
            "Epoch 49/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.8083\n",
            "Epoch 50/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5238 - accuracy: 0.8053\n",
            "Epoch 51/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5420 - accuracy: 0.8068\n",
            "Epoch 52/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5324 - accuracy: 0.8130\n",
            "Epoch 53/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5297 - accuracy: 0.8145\n",
            "Epoch 54/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.8161\n",
            "Epoch 55/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4866 - accuracy: 0.8176\n",
            "Epoch 56/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5230 - accuracy: 0.8068\n",
            "Epoch 57/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5315 - accuracy: 0.8037\n",
            "Epoch 58/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5248 - accuracy: 0.8099\n",
            "Epoch 59/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5204 - accuracy: 0.8083\n",
            "Epoch 60/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5006 - accuracy: 0.8253\n",
            "Epoch 61/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5232 - accuracy: 0.8238\n",
            "Epoch 62/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4980 - accuracy: 0.8315\n",
            "Epoch 63/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4628 - accuracy: 0.8362\n",
            "Epoch 64/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4890 - accuracy: 0.8346\n",
            "Epoch 65/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4825 - accuracy: 0.8300\n",
            "Epoch 66/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4904 - accuracy: 0.8315\n",
            "Epoch 67/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4852 - accuracy: 0.8454\n",
            "Epoch 68/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5046 - accuracy: 0.8176\n",
            "Epoch 69/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4736 - accuracy: 0.8408\n",
            "Epoch 70/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4894 - accuracy: 0.8192\n",
            "Epoch 71/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4790 - accuracy: 0.8300\n",
            "Epoch 72/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4721 - accuracy: 0.8269\n",
            "Epoch 73/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4731 - accuracy: 0.8439\n",
            "Epoch 74/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4756 - accuracy: 0.8238\n",
            "Epoch 75/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4484 - accuracy: 0.8423\n",
            "Epoch 76/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4678 - accuracy: 0.8423\n",
            "Epoch 77/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4646 - accuracy: 0.8346\n",
            "Epoch 78/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4369 - accuracy: 0.8485\n",
            "Epoch 79/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4588 - accuracy: 0.8501\n",
            "Epoch 80/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4227 - accuracy: 0.8563\n",
            "Epoch 81/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4513 - accuracy: 0.8315\n",
            "Epoch 82/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4434 - accuracy: 0.8393\n",
            "Epoch 83/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4285 - accuracy: 0.8470\n",
            "Epoch 84/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4671 - accuracy: 0.8393\n",
            "Epoch 85/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4553 - accuracy: 0.8423\n",
            "Epoch 86/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4480 - accuracy: 0.8454\n",
            "Epoch 87/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4646 - accuracy: 0.8377\n",
            "Epoch 88/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4600 - accuracy: 0.8377\n",
            "Epoch 89/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4468 - accuracy: 0.8532\n",
            "Epoch 90/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4396 - accuracy: 0.8454\n",
            "Epoch 91/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4381 - accuracy: 0.8454\n",
            "Epoch 92/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4449 - accuracy: 0.8516\n",
            "Epoch 93/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4326 - accuracy: 0.8454\n",
            "Epoch 94/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4331 - accuracy: 0.8454\n",
            "Epoch 95/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4317 - accuracy: 0.8547\n",
            "Epoch 96/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4194 - accuracy: 0.8547\n",
            "Epoch 97/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4460 - accuracy: 0.8439\n",
            "Epoch 98/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4465 - accuracy: 0.8532\n",
            "Epoch 99/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4429 - accuracy: 0.8423\n",
            "Epoch 100/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4565 - accuracy: 0.8269\n",
            "Epoch 101/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4210 - accuracy: 0.8640\n",
            "Epoch 102/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4528 - accuracy: 0.8408\n",
            "Epoch 103/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4324 - accuracy: 0.8485\n",
            "Epoch 104/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4320 - accuracy: 0.8563\n",
            "Epoch 105/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4378 - accuracy: 0.8408\n",
            "Epoch 106/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4312 - accuracy: 0.8501\n",
            "Epoch 107/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3986 - accuracy: 0.8640\n",
            "Epoch 108/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4212 - accuracy: 0.8516\n",
            "Epoch 109/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4103 - accuracy: 0.8563\n",
            "Epoch 110/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4114 - accuracy: 0.8655\n",
            "Epoch 111/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4161 - accuracy: 0.8516\n",
            "Epoch 112/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4090 - accuracy: 0.8501\n",
            "Epoch 113/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4289 - accuracy: 0.8485\n",
            "Epoch 114/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4032 - accuracy: 0.8609\n",
            "Epoch 115/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4119 - accuracy: 0.8624\n",
            "Epoch 116/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4103 - accuracy: 0.8532\n",
            "Epoch 117/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4139 - accuracy: 0.8516\n",
            "Epoch 118/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4008 - accuracy: 0.8578\n",
            "Epoch 119/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4302 - accuracy: 0.8315\n",
            "Epoch 120/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3987 - accuracy: 0.8702\n",
            "Epoch 121/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4104 - accuracy: 0.8547\n",
            "Epoch 122/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4025 - accuracy: 0.8624\n",
            "Epoch 123/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4006 - accuracy: 0.8671\n",
            "Epoch 124/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4256 - accuracy: 0.8454\n",
            "Epoch 125/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3978 - accuracy: 0.8655\n",
            "Epoch 126/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4220 - accuracy: 0.8563\n",
            "Epoch 127/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4086 - accuracy: 0.8563\n",
            "Epoch 128/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4103 - accuracy: 0.8516\n",
            "Epoch 129/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4185 - accuracy: 0.8516\n",
            "Epoch 130/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4089 - accuracy: 0.8609\n",
            "Epoch 131/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3731 - accuracy: 0.8671\n",
            "Epoch 132/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4232 - accuracy: 0.8423\n",
            "Epoch 133/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4300 - accuracy: 0.8547\n",
            "Epoch 134/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4182 - accuracy: 0.8532\n",
            "Epoch 135/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3942 - accuracy: 0.8563\n",
            "Epoch 136/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4099 - accuracy: 0.8594\n",
            "Epoch 137/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4087 - accuracy: 0.8640\n",
            "Epoch 138/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4009 - accuracy: 0.8655\n",
            "Epoch 139/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3917 - accuracy: 0.8655\n",
            "Epoch 140/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4014 - accuracy: 0.8655\n",
            "Epoch 141/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3962 - accuracy: 0.8686\n",
            "Epoch 142/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3791 - accuracy: 0.8609\n",
            "Epoch 143/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4010 - accuracy: 0.8563\n",
            "Epoch 144/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3866 - accuracy: 0.8578\n",
            "Epoch 145/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3910 - accuracy: 0.8686\n",
            "Epoch 146/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3817 - accuracy: 0.8686\n",
            "Epoch 147/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4045 - accuracy: 0.8563\n",
            "Epoch 148/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3868 - accuracy: 0.8640\n",
            "Epoch 149/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3810 - accuracy: 0.8609\n",
            "Epoch 150/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3822 - accuracy: 0.8624\n",
            "Epoch 151/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3960 - accuracy: 0.8563\n",
            "Epoch 152/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4012 - accuracy: 0.8655\n",
            "Epoch 153/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3991 - accuracy: 0.8624\n",
            "Epoch 154/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3589 - accuracy: 0.8764\n",
            "Epoch 155/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3824 - accuracy: 0.8733\n",
            "Epoch 156/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4130 - accuracy: 0.8578\n",
            "Epoch 157/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3916 - accuracy: 0.8563\n",
            "Epoch 158/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3886 - accuracy: 0.8779\n",
            "Epoch 159/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3933 - accuracy: 0.8640\n",
            "Epoch 160/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3610 - accuracy: 0.8748\n",
            "Epoch 161/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3848 - accuracy: 0.8624\n",
            "Epoch 162/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3740 - accuracy: 0.8563\n",
            "Epoch 163/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3757 - accuracy: 0.8671\n",
            "Epoch 164/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3725 - accuracy: 0.8764\n",
            "Epoch 165/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3845 - accuracy: 0.8702\n",
            "Epoch 166/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3751 - accuracy: 0.8624\n",
            "Epoch 167/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3658 - accuracy: 0.8702\n",
            "Epoch 168/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3788 - accuracy: 0.8640\n",
            "Epoch 169/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3583 - accuracy: 0.8748\n",
            "Epoch 170/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3822 - accuracy: 0.8578\n",
            "Epoch 171/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3710 - accuracy: 0.8686\n",
            "Epoch 172/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3701 - accuracy: 0.8702\n",
            "Epoch 173/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3808 - accuracy: 0.8733\n",
            "Epoch 174/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3591 - accuracy: 0.8702\n",
            "Epoch 175/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3608 - accuracy: 0.8779\n",
            "Epoch 176/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3666 - accuracy: 0.8779\n",
            "Epoch 177/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3755 - accuracy: 0.8748\n",
            "Epoch 178/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3622 - accuracy: 0.8748\n",
            "Epoch 179/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3847 - accuracy: 0.8671\n",
            "Epoch 180/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3837 - accuracy: 0.8702\n",
            "Epoch 181/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3811 - accuracy: 0.8779\n",
            "Epoch 182/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3891 - accuracy: 0.8702\n",
            "Epoch 183/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3644 - accuracy: 0.8717\n",
            "Epoch 184/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3746 - accuracy: 0.8624\n",
            "Epoch 185/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3529 - accuracy: 0.8810\n",
            "Epoch 186/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3661 - accuracy: 0.8841\n",
            "Epoch 187/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3956 - accuracy: 0.8655\n",
            "Epoch 188/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3900 - accuracy: 0.8655\n",
            "Epoch 189/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3841 - accuracy: 0.8532\n",
            "Epoch 190/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3581 - accuracy: 0.8748\n",
            "Epoch 191/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3588 - accuracy: 0.8748\n",
            "Epoch 192/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3721 - accuracy: 0.8702\n",
            "Epoch 193/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3660 - accuracy: 0.8686\n",
            "Epoch 194/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3805 - accuracy: 0.8717\n",
            "Epoch 195/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3676 - accuracy: 0.8717\n",
            "Epoch 196/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3523 - accuracy: 0.8655\n",
            "Epoch 197/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3352 - accuracy: 0.8841\n",
            "Epoch 198/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3695 - accuracy: 0.8671\n",
            "Epoch 199/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3739 - accuracy: 0.8640\n",
            "Epoch 200/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3540 - accuracy: 0.8764\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 0.6301 - accuracy: 0.8194\n",
            "Epoch 1/200\n",
            "26/26 [==============================] - 1s 3ms/step - loss: 1.3342 - accuracy: 0.5487\n",
            "Epoch 2/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.2638 - accuracy: 0.5549\n",
            "Epoch 3/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 1.2157 - accuracy: 0.5471\n",
            "Epoch 4/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 1.1418 - accuracy: 0.5533\n",
            "Epoch 5/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.0810 - accuracy: 0.5533\n",
            "Epoch 6/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.0545 - accuracy: 0.5471\n",
            "Epoch 7/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.0220 - accuracy: 0.5595\n",
            "Epoch 8/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.0039 - accuracy: 0.5518\n",
            "Epoch 9/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9849 - accuracy: 0.5611\n",
            "Epoch 10/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.9762 - accuracy: 0.5626\n",
            "Epoch 11/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9486 - accuracy: 0.5672\n",
            "Epoch 12/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9401 - accuracy: 0.5672\n",
            "Epoch 13/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.9296 - accuracy: 0.5781\n",
            "Epoch 14/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9120 - accuracy: 0.6012\n",
            "Epoch 15/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9015 - accuracy: 0.5920\n",
            "Epoch 16/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8966 - accuracy: 0.6059\n",
            "Epoch 17/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8813 - accuracy: 0.6182\n",
            "Epoch 18/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8538 - accuracy: 0.6522\n",
            "Epoch 19/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8545 - accuracy: 0.6476\n",
            "Epoch 20/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8359 - accuracy: 0.6708\n",
            "Epoch 21/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8453 - accuracy: 0.6615\n",
            "Epoch 22/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.8159 - accuracy: 0.7079\n",
            "Epoch 23/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.8338 - accuracy: 0.6862\n",
            "Epoch 24/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.8039 - accuracy: 0.6893\n",
            "Epoch 25/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7798 - accuracy: 0.6971\n",
            "Epoch 26/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7792 - accuracy: 0.7141\n",
            "Epoch 27/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7716 - accuracy: 0.7233\n",
            "Epoch 28/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7491 - accuracy: 0.7326\n",
            "Epoch 29/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7529 - accuracy: 0.7249\n",
            "Epoch 30/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7455 - accuracy: 0.7326\n",
            "Epoch 31/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7357 - accuracy: 0.7187\n",
            "Epoch 32/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7382 - accuracy: 0.7218\n",
            "Epoch 33/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7440 - accuracy: 0.7202\n",
            "Epoch 34/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7111 - accuracy: 0.7403\n",
            "Epoch 35/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7153 - accuracy: 0.7249\n",
            "Epoch 36/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.7204 - accuracy: 0.7357\n",
            "Epoch 37/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7002 - accuracy: 0.7450\n",
            "Epoch 38/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6745 - accuracy: 0.7759\n",
            "Epoch 39/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6928 - accuracy: 0.7388\n",
            "Epoch 40/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6924 - accuracy: 0.7419\n",
            "Epoch 41/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6818 - accuracy: 0.7403\n",
            "Epoch 42/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.6869 - accuracy: 0.7481\n",
            "Epoch 43/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6591 - accuracy: 0.7697\n",
            "Epoch 44/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6591 - accuracy: 0.7728\n",
            "Epoch 45/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6357 - accuracy: 0.7759\n",
            "Epoch 46/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.6426 - accuracy: 0.7527\n",
            "Epoch 47/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6618 - accuracy: 0.7434\n",
            "Epoch 48/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6612 - accuracy: 0.7651\n",
            "Epoch 49/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6269 - accuracy: 0.7713\n",
            "Epoch 50/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6469 - accuracy: 0.7635\n",
            "Epoch 51/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6364 - accuracy: 0.7651\n",
            "Epoch 52/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6105 - accuracy: 0.7728\n",
            "Epoch 53/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6499 - accuracy: 0.7635\n",
            "Epoch 54/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6566 - accuracy: 0.7342\n",
            "Epoch 55/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6118 - accuracy: 0.7759\n",
            "Epoch 56/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6360 - accuracy: 0.7620\n",
            "Epoch 57/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6099 - accuracy: 0.7682\n",
            "Epoch 58/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.6201 - accuracy: 0.7697\n",
            "Epoch 59/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.6356 - accuracy: 0.7713\n",
            "Epoch 60/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5926 - accuracy: 0.7867\n",
            "Epoch 61/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6114 - accuracy: 0.7713\n",
            "Epoch 62/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6041 - accuracy: 0.7682\n",
            "Epoch 63/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.6307 - accuracy: 0.7666\n",
            "Epoch 64/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5766 - accuracy: 0.7867\n",
            "Epoch 65/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5971 - accuracy: 0.7805\n",
            "Epoch 66/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5884 - accuracy: 0.7821\n",
            "Epoch 67/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6209 - accuracy: 0.7635\n",
            "Epoch 68/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6057 - accuracy: 0.7790\n",
            "Epoch 69/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5779 - accuracy: 0.7759\n",
            "Epoch 70/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5904 - accuracy: 0.7805\n",
            "Epoch 71/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5829 - accuracy: 0.7852\n",
            "Epoch 72/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5879 - accuracy: 0.7805\n",
            "Epoch 73/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6048 - accuracy: 0.7682\n",
            "Epoch 74/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5853 - accuracy: 0.7697\n",
            "Epoch 75/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5944 - accuracy: 0.7666\n",
            "Epoch 76/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5811 - accuracy: 0.7991\n",
            "Epoch 77/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5708 - accuracy: 0.7805\n",
            "Epoch 78/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5858 - accuracy: 0.7805\n",
            "Epoch 79/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5384 - accuracy: 0.8006\n",
            "Epoch 80/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5779 - accuracy: 0.7697\n",
            "Epoch 81/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5575 - accuracy: 0.7821\n",
            "Epoch 82/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5888 - accuracy: 0.7836\n",
            "Epoch 83/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5581 - accuracy: 0.7913\n",
            "Epoch 84/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5800 - accuracy: 0.7805\n",
            "Epoch 85/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5384 - accuracy: 0.8037\n",
            "Epoch 86/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5700 - accuracy: 0.7898\n",
            "Epoch 87/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5384 - accuracy: 0.8068\n",
            "Epoch 88/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5743 - accuracy: 0.7759\n",
            "Epoch 89/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5683 - accuracy: 0.7944\n",
            "Epoch 90/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5631 - accuracy: 0.7944\n",
            "Epoch 91/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5337 - accuracy: 0.7960\n",
            "Epoch 92/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5567 - accuracy: 0.7883\n",
            "Epoch 93/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5566 - accuracy: 0.8145\n",
            "Epoch 94/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5321 - accuracy: 0.8099\n",
            "Epoch 95/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5614 - accuracy: 0.7883\n",
            "Epoch 96/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.8037\n",
            "Epoch 97/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5356 - accuracy: 0.7975\n",
            "Epoch 98/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5397 - accuracy: 0.8037\n",
            "Epoch 99/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5445 - accuracy: 0.7883\n",
            "Epoch 100/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5245 - accuracy: 0.8022\n",
            "Epoch 101/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5514 - accuracy: 0.8022\n",
            "Epoch 102/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5128 - accuracy: 0.8006\n",
            "Epoch 103/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5215 - accuracy: 0.8114\n",
            "Epoch 104/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5200 - accuracy: 0.8130\n",
            "Epoch 105/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5195 - accuracy: 0.8068\n",
            "Epoch 106/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7929\n",
            "Epoch 107/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5035 - accuracy: 0.8315\n",
            "Epoch 108/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4991 - accuracy: 0.8145\n",
            "Epoch 109/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5223 - accuracy: 0.8068\n",
            "Epoch 110/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5387 - accuracy: 0.7991\n",
            "Epoch 111/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5099 - accuracy: 0.8099\n",
            "Epoch 112/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5183 - accuracy: 0.8037\n",
            "Epoch 113/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5284 - accuracy: 0.8130\n",
            "Epoch 114/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5228 - accuracy: 0.8145\n",
            "Epoch 115/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5461 - accuracy: 0.8037\n",
            "Epoch 116/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5101 - accuracy: 0.8207\n",
            "Epoch 117/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5235 - accuracy: 0.8053\n",
            "Epoch 118/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4876 - accuracy: 0.8161\n",
            "Epoch 119/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5155 - accuracy: 0.8083\n",
            "Epoch 120/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5211 - accuracy: 0.8068\n",
            "Epoch 121/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4955 - accuracy: 0.8238\n",
            "Epoch 122/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5074 - accuracy: 0.8238\n",
            "Epoch 123/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5129 - accuracy: 0.8037\n",
            "Epoch 124/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5070 - accuracy: 0.8053\n",
            "Epoch 125/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4831 - accuracy: 0.8145\n",
            "Epoch 126/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5101 - accuracy: 0.8114\n",
            "Epoch 127/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5020 - accuracy: 0.8176\n",
            "Epoch 128/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4987 - accuracy: 0.8130\n",
            "Epoch 129/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4708 - accuracy: 0.8284\n",
            "Epoch 130/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5386 - accuracy: 0.8099\n",
            "Epoch 131/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5034 - accuracy: 0.8269\n",
            "Epoch 132/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4978 - accuracy: 0.8284\n",
            "Epoch 133/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4658 - accuracy: 0.8223\n",
            "Epoch 134/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4820 - accuracy: 0.8269\n",
            "Epoch 135/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.8238\n",
            "Epoch 136/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5110 - accuracy: 0.8068\n",
            "Epoch 137/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5204 - accuracy: 0.8176\n",
            "Epoch 138/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4929 - accuracy: 0.8300\n",
            "Epoch 139/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4900 - accuracy: 0.8207\n",
            "Epoch 140/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.5094 - accuracy: 0.8253\n",
            "Epoch 141/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4813 - accuracy: 0.8223\n",
            "Epoch 142/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5093 - accuracy: 0.8223\n",
            "Epoch 143/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4781 - accuracy: 0.8223\n",
            "Epoch 144/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4691 - accuracy: 0.8346\n",
            "Epoch 145/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4685 - accuracy: 0.8284\n",
            "Epoch 146/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4845 - accuracy: 0.8253\n",
            "Epoch 147/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4589 - accuracy: 0.8346\n",
            "Epoch 148/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4864 - accuracy: 0.8331\n",
            "Epoch 149/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4863 - accuracy: 0.8223\n",
            "Epoch 150/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4892 - accuracy: 0.8223\n",
            "Epoch 151/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4481 - accuracy: 0.8331\n",
            "Epoch 152/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4451 - accuracy: 0.8423\n",
            "Epoch 153/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4749 - accuracy: 0.8300\n",
            "Epoch 154/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4671 - accuracy: 0.8284\n",
            "Epoch 155/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4832 - accuracy: 0.8238\n",
            "Epoch 156/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4798 - accuracy: 0.8253\n",
            "Epoch 157/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4667 - accuracy: 0.8176\n",
            "Epoch 158/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4768 - accuracy: 0.8145\n",
            "Epoch 159/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4783 - accuracy: 0.8346\n",
            "Epoch 160/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4606 - accuracy: 0.8393\n",
            "Epoch 161/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4590 - accuracy: 0.8362\n",
            "Epoch 162/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4474 - accuracy: 0.8377\n",
            "Epoch 163/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4953 - accuracy: 0.8176\n",
            "Epoch 164/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4448 - accuracy: 0.8470\n",
            "Epoch 165/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4698 - accuracy: 0.8269\n",
            "Epoch 166/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4679 - accuracy: 0.8377\n",
            "Epoch 167/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4551 - accuracy: 0.8408\n",
            "Epoch 168/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4764 - accuracy: 0.8362\n",
            "Epoch 169/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4473 - accuracy: 0.8439\n",
            "Epoch 170/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4523 - accuracy: 0.8423\n",
            "Epoch 171/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4527 - accuracy: 0.8408\n",
            "Epoch 172/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4533 - accuracy: 0.8377\n",
            "Epoch 173/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4467 - accuracy: 0.8439\n",
            "Epoch 174/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4657 - accuracy: 0.8393\n",
            "Epoch 175/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4486 - accuracy: 0.8331\n",
            "Epoch 176/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4585 - accuracy: 0.8362\n",
            "Epoch 177/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4641 - accuracy: 0.8408\n",
            "Epoch 178/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4182 - accuracy: 0.8470\n",
            "Epoch 179/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4425 - accuracy: 0.8362\n",
            "Epoch 180/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4469 - accuracy: 0.8346\n",
            "Epoch 181/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4346 - accuracy: 0.8377\n",
            "Epoch 182/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4460 - accuracy: 0.8346\n",
            "Epoch 183/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4506 - accuracy: 0.8439\n",
            "Epoch 184/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4357 - accuracy: 0.8516\n",
            "Epoch 185/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4465 - accuracy: 0.8377\n",
            "Epoch 186/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4681 - accuracy: 0.8192\n",
            "Epoch 187/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4594 - accuracy: 0.8269\n",
            "Epoch 188/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4511 - accuracy: 0.8331\n",
            "Epoch 189/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4434 - accuracy: 0.8501\n",
            "Epoch 190/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4419 - accuracy: 0.8423\n",
            "Epoch 191/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4397 - accuracy: 0.8346\n",
            "Epoch 192/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4520 - accuracy: 0.8315\n",
            "Epoch 193/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4198 - accuracy: 0.8470\n",
            "Epoch 194/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4637 - accuracy: 0.8315\n",
            "Epoch 195/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4362 - accuracy: 0.8454\n",
            "Epoch 196/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4357 - accuracy: 0.8563\n",
            "Epoch 197/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4286 - accuracy: 0.8439\n",
            "Epoch 198/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4367 - accuracy: 0.8470\n",
            "Epoch 199/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4306 - accuracy: 0.8408\n",
            "Epoch 200/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.4263 - accuracy: 0.8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 105, in __call__\n",
            "    score = scorer(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/keras/wrappers/scikit_learn.py\", line 313, in score\n",
            "    outputs = self.model.evaluate(x, y, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\", line 1129, in autograph_handler\n",
            "    raise e.ag_error_metadata.to_exception(e)\n",
            "ValueError: in user code:\n",
            "\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1366, in test_function  *\n",
            "        return step_function(self, iterator)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1356, in step_function  **\n",
            "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1349, in run_step  **\n",
            "        outputs = model.test_step(data)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1306, in test_step\n",
            "        y, y_pred, sample_weight, regularization_losses=self.losses)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n",
            "        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 141, in __call__\n",
            "        losses = call_fn(y_true, y_pred)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 245, in call  **\n",
            "        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 1665, in categorical_crossentropy\n",
            "        y_true, y_pred, from_logits=from_logits, axis=axis)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 4994, in categorical_crossentropy\n",
            "        target.shape.assert_is_compatible_with(output.shape)\n",
            "\n",
            "    ValueError: Shapes (None, 3) and (None, 4) are incompatible\n",
            "\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "26/26 [==============================] - 1s 3ms/step - loss: 1.3637 - accuracy: 0.4730\n",
            "Epoch 2/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.2941 - accuracy: 0.5765\n",
            "Epoch 3/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.2370 - accuracy: 0.5611\n",
            "Epoch 4/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.1863 - accuracy: 0.5889\n",
            "Epoch 5/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.1305 - accuracy: 0.5966\n",
            "Epoch 6/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.0688 - accuracy: 0.6244\n",
            "Epoch 7/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.0605 - accuracy: 0.6043\n",
            "Epoch 8/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.9814 - accuracy: 0.6754\n",
            "Epoch 9/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9518 - accuracy: 0.6955\n",
            "Epoch 10/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.9297 - accuracy: 0.6692\n",
            "Epoch 11/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.8942 - accuracy: 0.6662\n",
            "Epoch 12/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8559 - accuracy: 0.7017\n",
            "Epoch 13/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8702 - accuracy: 0.6878\n",
            "Epoch 14/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.8166 - accuracy: 0.7141\n",
            "Epoch 15/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.7723 - accuracy: 0.7342\n",
            "Epoch 16/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.7730 - accuracy: 0.7094\n",
            "Epoch 17/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7356 - accuracy: 0.7465\n",
            "Epoch 18/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.7518 - accuracy: 0.7434\n",
            "Epoch 19/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.7104 - accuracy: 0.7573\n",
            "Epoch 20/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.7031 - accuracy: 0.7419\n",
            "Epoch 21/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6987 - accuracy: 0.7450\n",
            "Epoch 22/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6856 - accuracy: 0.7666\n",
            "Epoch 23/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6573 - accuracy: 0.7713\n",
            "Epoch 24/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6792 - accuracy: 0.7666\n",
            "Epoch 25/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6649 - accuracy: 0.7759\n",
            "Epoch 26/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6764 - accuracy: 0.7620\n",
            "Epoch 27/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6691 - accuracy: 0.7713\n",
            "Epoch 28/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6287 - accuracy: 0.7898\n",
            "Epoch 29/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6498 - accuracy: 0.7774\n",
            "Epoch 30/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6227 - accuracy: 0.7805\n",
            "Epoch 31/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6071 - accuracy: 0.7991\n",
            "Epoch 32/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6057 - accuracy: 0.7852\n",
            "Epoch 33/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6145 - accuracy: 0.7913\n",
            "Epoch 34/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6168 - accuracy: 0.7836\n",
            "Epoch 35/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5982 - accuracy: 0.7852\n",
            "Epoch 36/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5705 - accuracy: 0.8083\n",
            "Epoch 37/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5909 - accuracy: 0.7883\n",
            "Epoch 38/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5828 - accuracy: 0.8053\n",
            "Epoch 39/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5688 - accuracy: 0.8207\n",
            "Epoch 40/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5919 - accuracy: 0.7867\n",
            "Epoch 41/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5627 - accuracy: 0.8022\n",
            "Epoch 42/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5645 - accuracy: 0.8006\n",
            "Epoch 43/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5600 - accuracy: 0.8114\n",
            "Epoch 44/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5692 - accuracy: 0.8068\n",
            "Epoch 45/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5564 - accuracy: 0.8022\n",
            "Epoch 46/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5479 - accuracy: 0.8006\n",
            "Epoch 47/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5467 - accuracy: 0.8145\n",
            "Epoch 48/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.8068\n",
            "Epoch 49/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5283 - accuracy: 0.8099\n",
            "Epoch 50/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5345 - accuracy: 0.8114\n",
            "Epoch 51/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5411 - accuracy: 0.8068\n",
            "Epoch 52/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5394 - accuracy: 0.8223\n",
            "Epoch 53/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5532 - accuracy: 0.8223\n",
            "Epoch 54/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5404 - accuracy: 0.8053\n",
            "Epoch 55/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5309 - accuracy: 0.8223\n",
            "Epoch 56/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5200 - accuracy: 0.8161\n",
            "Epoch 57/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5352 - accuracy: 0.8145\n",
            "Epoch 58/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5235 - accuracy: 0.8099\n",
            "Epoch 59/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5227 - accuracy: 0.8192\n",
            "Epoch 60/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5115 - accuracy: 0.8284\n",
            "Epoch 61/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.8207\n",
            "Epoch 62/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5116 - accuracy: 0.8284\n",
            "Epoch 63/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.8315\n",
            "Epoch 64/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5078 - accuracy: 0.8331\n",
            "Epoch 65/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5105 - accuracy: 0.8284\n",
            "Epoch 66/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5164 - accuracy: 0.8346\n",
            "Epoch 67/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5210 - accuracy: 0.8238\n",
            "Epoch 68/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5047 - accuracy: 0.8130\n",
            "Epoch 69/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4940 - accuracy: 0.8284\n",
            "Epoch 70/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5267 - accuracy: 0.8315\n",
            "Epoch 71/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.8408\n",
            "Epoch 72/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4980 - accuracy: 0.8238\n",
            "Epoch 73/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5243 - accuracy: 0.8284\n",
            "Epoch 74/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4890 - accuracy: 0.8362\n",
            "Epoch 75/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4947 - accuracy: 0.8238\n",
            "Epoch 76/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4889 - accuracy: 0.8362\n",
            "Epoch 77/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.8207\n",
            "Epoch 78/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4872 - accuracy: 0.8346\n",
            "Epoch 79/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4928 - accuracy: 0.8315\n",
            "Epoch 80/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4810 - accuracy: 0.8346\n",
            "Epoch 81/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4722 - accuracy: 0.8284\n",
            "Epoch 82/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4777 - accuracy: 0.8423\n",
            "Epoch 83/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4646 - accuracy: 0.8423\n",
            "Epoch 84/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4483 - accuracy: 0.8547\n",
            "Epoch 85/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4727 - accuracy: 0.8393\n",
            "Epoch 86/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4568 - accuracy: 0.8454\n",
            "Epoch 87/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4714 - accuracy: 0.8315\n",
            "Epoch 88/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4509 - accuracy: 0.8346\n",
            "Epoch 89/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4737 - accuracy: 0.8408\n",
            "Epoch 90/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4718 - accuracy: 0.8284\n",
            "Epoch 91/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4776 - accuracy: 0.8238\n",
            "Epoch 92/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4711 - accuracy: 0.8315\n",
            "Epoch 93/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4639 - accuracy: 0.8362\n",
            "Epoch 94/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4574 - accuracy: 0.8439\n",
            "Epoch 95/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4725 - accuracy: 0.8284\n",
            "Epoch 96/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4637 - accuracy: 0.8377\n",
            "Epoch 97/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4648 - accuracy: 0.8362\n",
            "Epoch 98/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4639 - accuracy: 0.8470\n",
            "Epoch 99/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4744 - accuracy: 0.8346\n",
            "Epoch 100/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4533 - accuracy: 0.8454\n",
            "Epoch 101/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4280 - accuracy: 0.8439\n",
            "Epoch 102/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4592 - accuracy: 0.8423\n",
            "Epoch 103/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4398 - accuracy: 0.8454\n",
            "Epoch 104/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4343 - accuracy: 0.8454\n",
            "Epoch 105/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4641 - accuracy: 0.8253\n",
            "Epoch 106/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4396 - accuracy: 0.8408\n",
            "Epoch 107/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4486 - accuracy: 0.8501\n",
            "Epoch 108/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4287 - accuracy: 0.8563\n",
            "Epoch 109/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4548 - accuracy: 0.8408\n",
            "Epoch 110/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4271 - accuracy: 0.8501\n",
            "Epoch 111/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4495 - accuracy: 0.8439\n",
            "Epoch 112/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4371 - accuracy: 0.8563\n",
            "Epoch 113/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4226 - accuracy: 0.8423\n",
            "Epoch 114/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4137 - accuracy: 0.8563\n",
            "Epoch 115/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4300 - accuracy: 0.8516\n",
            "Epoch 116/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4245 - accuracy: 0.8393\n",
            "Epoch 117/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4220 - accuracy: 0.8501\n",
            "Epoch 118/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4380 - accuracy: 0.8377\n",
            "Epoch 119/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4119 - accuracy: 0.8516\n",
            "Epoch 120/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4186 - accuracy: 0.8516\n",
            "Epoch 121/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4357 - accuracy: 0.8393\n",
            "Epoch 122/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4440 - accuracy: 0.8377\n",
            "Epoch 123/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4401 - accuracy: 0.8439\n",
            "Epoch 124/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4074 - accuracy: 0.8547\n",
            "Epoch 125/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4193 - accuracy: 0.8408\n",
            "Epoch 126/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4048 - accuracy: 0.8516\n",
            "Epoch 127/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4415 - accuracy: 0.8609\n",
            "Epoch 128/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4335 - accuracy: 0.8501\n",
            "Epoch 129/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4306 - accuracy: 0.8470\n",
            "Epoch 130/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4096 - accuracy: 0.8516\n",
            "Epoch 131/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4115 - accuracy: 0.8578\n",
            "Epoch 132/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4100 - accuracy: 0.8532\n",
            "Epoch 133/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4219 - accuracy: 0.8393\n",
            "Epoch 134/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4211 - accuracy: 0.8501\n",
            "Epoch 135/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3996 - accuracy: 0.8594\n",
            "Epoch 136/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4220 - accuracy: 0.8532\n",
            "Epoch 137/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4018 - accuracy: 0.8547\n",
            "Epoch 138/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4252 - accuracy: 0.8578\n",
            "Epoch 139/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4122 - accuracy: 0.8423\n",
            "Epoch 140/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4239 - accuracy: 0.8393\n",
            "Epoch 141/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4315 - accuracy: 0.8485\n",
            "Epoch 142/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4092 - accuracy: 0.8563\n",
            "Epoch 143/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4105 - accuracy: 0.8547\n",
            "Epoch 144/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4185 - accuracy: 0.8516\n",
            "Epoch 145/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4008 - accuracy: 0.8624\n",
            "Epoch 146/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4167 - accuracy: 0.8470\n",
            "Epoch 147/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4037 - accuracy: 0.8640\n",
            "Epoch 148/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4000 - accuracy: 0.8532\n",
            "Epoch 149/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4046 - accuracy: 0.8594\n",
            "Epoch 150/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4009 - accuracy: 0.8640\n",
            "Epoch 151/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4255 - accuracy: 0.8393\n",
            "Epoch 152/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4101 - accuracy: 0.8547\n",
            "Epoch 153/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4034 - accuracy: 0.8640\n",
            "Epoch 154/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4032 - accuracy: 0.8655\n",
            "Epoch 155/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4002 - accuracy: 0.8563\n",
            "Epoch 156/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4024 - accuracy: 0.8532\n",
            "Epoch 157/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3951 - accuracy: 0.8609\n",
            "Epoch 158/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3935 - accuracy: 0.8609\n",
            "Epoch 159/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3961 - accuracy: 0.8609\n",
            "Epoch 160/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3877 - accuracy: 0.8686\n",
            "Epoch 161/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4104 - accuracy: 0.8578\n",
            "Epoch 162/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4150 - accuracy: 0.8609\n",
            "Epoch 163/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3917 - accuracy: 0.8563\n",
            "Epoch 164/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4020 - accuracy: 0.8594\n",
            "Epoch 165/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3870 - accuracy: 0.8717\n",
            "Epoch 166/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3940 - accuracy: 0.8655\n",
            "Epoch 167/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3803 - accuracy: 0.8655\n",
            "Epoch 168/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3816 - accuracy: 0.8594\n",
            "Epoch 169/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3940 - accuracy: 0.8686\n",
            "Epoch 170/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3733 - accuracy: 0.8764\n",
            "Epoch 171/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4005 - accuracy: 0.8624\n",
            "Epoch 172/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4042 - accuracy: 0.8609\n",
            "Epoch 173/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3721 - accuracy: 0.8702\n",
            "Epoch 174/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4036 - accuracy: 0.8609\n",
            "Epoch 175/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3988 - accuracy: 0.8470\n",
            "Epoch 176/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3778 - accuracy: 0.8547\n",
            "Epoch 177/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4036 - accuracy: 0.8594\n",
            "Epoch 178/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3772 - accuracy: 0.8748\n",
            "Epoch 179/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3755 - accuracy: 0.8624\n",
            "Epoch 180/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3901 - accuracy: 0.8609\n",
            "Epoch 181/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3963 - accuracy: 0.8671\n",
            "Epoch 182/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3959 - accuracy: 0.8578\n",
            "Epoch 183/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3966 - accuracy: 0.8655\n",
            "Epoch 184/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3754 - accuracy: 0.8655\n",
            "Epoch 185/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3929 - accuracy: 0.8671\n",
            "Epoch 186/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4023 - accuracy: 0.8624\n",
            "Epoch 187/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3754 - accuracy: 0.8702\n",
            "Epoch 188/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4022 - accuracy: 0.8624\n",
            "Epoch 189/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3918 - accuracy: 0.8594\n",
            "Epoch 190/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3708 - accuracy: 0.8578\n",
            "Epoch 191/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3940 - accuracy: 0.8624\n",
            "Epoch 192/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3734 - accuracy: 0.8671\n",
            "Epoch 193/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3867 - accuracy: 0.8764\n",
            "Epoch 194/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3640 - accuracy: 0.8655\n",
            "Epoch 195/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3888 - accuracy: 0.8578\n",
            "Epoch 196/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3768 - accuracy: 0.8594\n",
            "Epoch 197/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3731 - accuracy: 0.8748\n",
            "Epoch 198/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3872 - accuracy: 0.8733\n",
            "Epoch 199/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3796 - accuracy: 0.8578\n",
            "Epoch 200/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3536 - accuracy: 0.8717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 105, in __call__\n",
            "    score = scorer(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/keras/wrappers/scikit_learn.py\", line 313, in score\n",
            "    outputs = self.model.evaluate(x, y, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\", line 1129, in autograph_handler\n",
            "    raise e.ag_error_metadata.to_exception(e)\n",
            "ValueError: in user code:\n",
            "\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1366, in test_function  *\n",
            "        return step_function(self, iterator)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1356, in step_function  **\n",
            "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1349, in run_step  **\n",
            "        outputs = model.test_step(data)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1306, in test_step\n",
            "        y, y_pred, sample_weight, regularization_losses=self.losses)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n",
            "        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 141, in __call__\n",
            "        losses = call_fn(y_true, y_pred)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 245, in call  **\n",
            "        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 1665, in categorical_crossentropy\n",
            "        y_true, y_pred, from_logits=from_logits, axis=axis)\n",
            "    File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 4994, in categorical_crossentropy\n",
            "        target.shape.assert_is_compatible_with(output.shape)\n",
            "\n",
            "    ValueError: Shapes (None, 3) and (None, 4) are incompatible\n",
            "\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "26/26 [==============================] - 1s 2ms/step - loss: 1.3678 - accuracy: 0.3688\n",
            "Epoch 2/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.3256 - accuracy: 0.4985\n",
            "Epoch 3/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.2808 - accuracy: 0.5309\n",
            "Epoch 4/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.2257 - accuracy: 0.5525\n",
            "Epoch 5/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.1723 - accuracy: 0.5448\n",
            "Epoch 6/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.1152 - accuracy: 0.5849\n",
            "Epoch 7/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.0830 - accuracy: 0.5910\n",
            "Epoch 8/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.0545 - accuracy: 0.5895\n",
            "Epoch 9/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.0383 - accuracy: 0.5957\n",
            "Epoch 10/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.0311 - accuracy: 0.5926\n",
            "Epoch 11/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.9954 - accuracy: 0.5833\n",
            "Epoch 12/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.9832 - accuracy: 0.6019\n",
            "Epoch 13/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9730 - accuracy: 0.6281\n",
            "Epoch 14/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.9340 - accuracy: 0.6543\n",
            "Epoch 15/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.9217 - accuracy: 0.6559\n",
            "Epoch 16/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.9125 - accuracy: 0.6543\n",
            "Epoch 17/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.8795 - accuracy: 0.7037\n",
            "Epoch 18/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.8779 - accuracy: 0.6914\n",
            "Epoch 19/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.8320 - accuracy: 0.7052\n",
            "Epoch 20/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8137 - accuracy: 0.7176\n",
            "Epoch 21/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.7944 - accuracy: 0.7130\n",
            "Epoch 22/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.7659 - accuracy: 0.7269\n",
            "Epoch 23/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7583 - accuracy: 0.7222\n",
            "Epoch 24/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7449 - accuracy: 0.7423\n",
            "Epoch 25/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.7086 - accuracy: 0.7577\n",
            "Epoch 26/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.7064 - accuracy: 0.7546\n",
            "Epoch 27/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6983 - accuracy: 0.7562\n",
            "Epoch 28/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6804 - accuracy: 0.7670\n",
            "Epoch 29/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6285 - accuracy: 0.7824\n",
            "Epoch 30/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6443 - accuracy: 0.7762\n",
            "Epoch 31/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6113 - accuracy: 0.7917\n",
            "Epoch 32/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6399 - accuracy: 0.7577\n",
            "Epoch 33/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6127 - accuracy: 0.7778\n",
            "Epoch 34/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5588 - accuracy: 0.8025\n",
            "Epoch 35/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5638 - accuracy: 0.7840\n",
            "Epoch 36/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5600 - accuracy: 0.8102\n",
            "Epoch 37/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5673 - accuracy: 0.8071\n",
            "Epoch 38/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5958 - accuracy: 0.7824\n",
            "Epoch 39/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5540 - accuracy: 0.8040\n",
            "Epoch 40/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5385 - accuracy: 0.8164\n",
            "Epoch 41/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5203 - accuracy: 0.8164\n",
            "Epoch 42/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5223 - accuracy: 0.8333\n",
            "Epoch 43/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5434 - accuracy: 0.8179\n",
            "Epoch 44/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5195 - accuracy: 0.8102\n",
            "Epoch 45/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.8071\n",
            "Epoch 46/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5403 - accuracy: 0.8086\n",
            "Epoch 47/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5307 - accuracy: 0.8179\n",
            "Epoch 48/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5304 - accuracy: 0.8117\n",
            "Epoch 49/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4979 - accuracy: 0.8164\n",
            "Epoch 50/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5058 - accuracy: 0.8194\n",
            "Epoch 51/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4966 - accuracy: 0.8194\n",
            "Epoch 52/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4865 - accuracy: 0.8164\n",
            "Epoch 53/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4759 - accuracy: 0.8241\n",
            "Epoch 54/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4974 - accuracy: 0.8179\n",
            "Epoch 55/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4991 - accuracy: 0.8194\n",
            "Epoch 56/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4856 - accuracy: 0.8272\n",
            "Epoch 57/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5047 - accuracy: 0.8133\n",
            "Epoch 58/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4800 - accuracy: 0.8318\n",
            "Epoch 59/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4468 - accuracy: 0.8395\n",
            "Epoch 60/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4547 - accuracy: 0.8488\n",
            "Epoch 61/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4756 - accuracy: 0.8380\n",
            "Epoch 62/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4908 - accuracy: 0.8410\n",
            "Epoch 63/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4657 - accuracy: 0.8380\n",
            "Epoch 64/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4511 - accuracy: 0.8457\n",
            "Epoch 65/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4504 - accuracy: 0.8380\n",
            "Epoch 66/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4552 - accuracy: 0.8642\n",
            "Epoch 67/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4443 - accuracy: 0.8519\n",
            "Epoch 68/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4433 - accuracy: 0.8519\n",
            "Epoch 69/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4282 - accuracy: 0.8549\n",
            "Epoch 70/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4457 - accuracy: 0.8596\n",
            "Epoch 71/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4154 - accuracy: 0.8627\n",
            "Epoch 72/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4390 - accuracy: 0.8565\n",
            "Epoch 73/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4176 - accuracy: 0.8688\n",
            "Epoch 74/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3999 - accuracy: 0.8704\n",
            "Epoch 75/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4235 - accuracy: 0.8657\n",
            "Epoch 76/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4197 - accuracy: 0.8611\n",
            "Epoch 77/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4122 - accuracy: 0.8673\n",
            "Epoch 78/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4073 - accuracy: 0.8688\n",
            "Epoch 79/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4150 - accuracy: 0.8704\n",
            "Epoch 80/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3975 - accuracy: 0.8565\n",
            "Epoch 81/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4014 - accuracy: 0.8673\n",
            "Epoch 82/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4022 - accuracy: 0.8642\n",
            "Epoch 83/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3925 - accuracy: 0.8704\n",
            "Epoch 84/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3912 - accuracy: 0.8673\n",
            "Epoch 85/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3957 - accuracy: 0.8750\n",
            "Epoch 86/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3850 - accuracy: 0.8719\n",
            "Epoch 87/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3846 - accuracy: 0.8858\n",
            "Epoch 88/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3913 - accuracy: 0.8580\n",
            "Epoch 89/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4012 - accuracy: 0.8688\n",
            "Epoch 90/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3896 - accuracy: 0.8688\n",
            "Epoch 91/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3908 - accuracy: 0.8735\n",
            "Epoch 92/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3784 - accuracy: 0.8673\n",
            "Epoch 93/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3973 - accuracy: 0.8735\n",
            "Epoch 94/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3876 - accuracy: 0.8719\n",
            "Epoch 95/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3632 - accuracy: 0.8827\n",
            "Epoch 96/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3870 - accuracy: 0.8642\n",
            "Epoch 97/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3717 - accuracy: 0.8688\n",
            "Epoch 98/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3823 - accuracy: 0.8765\n",
            "Epoch 99/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4012 - accuracy: 0.8735\n",
            "Epoch 100/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3708 - accuracy: 0.8812\n",
            "Epoch 101/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3726 - accuracy: 0.8688\n",
            "Epoch 102/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3849 - accuracy: 0.8688\n",
            "Epoch 103/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3446 - accuracy: 0.8904\n",
            "Epoch 104/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3795 - accuracy: 0.8719\n",
            "Epoch 105/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3765 - accuracy: 0.8765\n",
            "Epoch 106/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3420 - accuracy: 0.8843\n",
            "Epoch 107/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3579 - accuracy: 0.8858\n",
            "Epoch 108/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3750 - accuracy: 0.8812\n",
            "Epoch 109/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3533 - accuracy: 0.8719\n",
            "Epoch 110/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3675 - accuracy: 0.8904\n",
            "Epoch 111/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3763 - accuracy: 0.8765\n",
            "Epoch 112/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3673 - accuracy: 0.8765\n",
            "Epoch 113/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3484 - accuracy: 0.8827\n",
            "Epoch 114/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3673 - accuracy: 0.8858\n",
            "Epoch 115/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3611 - accuracy: 0.8904\n",
            "Epoch 116/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3609 - accuracy: 0.8843\n",
            "Epoch 117/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3711 - accuracy: 0.8765\n",
            "Epoch 118/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3640 - accuracy: 0.8735\n",
            "Epoch 119/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3309 - accuracy: 0.8904\n",
            "Epoch 120/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3480 - accuracy: 0.8827\n",
            "Epoch 121/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3609 - accuracy: 0.8735\n",
            "Epoch 122/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3669 - accuracy: 0.8750\n",
            "Epoch 123/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3325 - accuracy: 0.8920\n",
            "Epoch 124/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3569 - accuracy: 0.8796\n",
            "Epoch 125/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3343 - accuracy: 0.8966\n",
            "Epoch 126/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3380 - accuracy: 0.8827\n",
            "Epoch 127/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3355 - accuracy: 0.8966\n",
            "Epoch 128/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3501 - accuracy: 0.8781\n",
            "Epoch 129/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3425 - accuracy: 0.8827\n",
            "Epoch 130/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3324 - accuracy: 0.8873\n",
            "Epoch 131/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3376 - accuracy: 0.8889\n",
            "Epoch 132/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3593 - accuracy: 0.8858\n",
            "Epoch 133/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3517 - accuracy: 0.8873\n",
            "Epoch 134/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3429 - accuracy: 0.8843\n",
            "Epoch 135/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3255 - accuracy: 0.8843\n",
            "Epoch 136/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3254 - accuracy: 0.8889\n",
            "Epoch 137/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3278 - accuracy: 0.8951\n",
            "Epoch 138/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3256 - accuracy: 0.8904\n",
            "Epoch 139/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3339 - accuracy: 0.8889\n",
            "Epoch 140/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3249 - accuracy: 0.8951\n",
            "Epoch 141/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3145 - accuracy: 0.8981\n",
            "Epoch 142/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3457 - accuracy: 0.8889\n",
            "Epoch 143/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3567 - accuracy: 0.8920\n",
            "Epoch 144/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3212 - accuracy: 0.9028\n",
            "Epoch 145/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3189 - accuracy: 0.8951\n",
            "Epoch 146/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3379 - accuracy: 0.8935\n",
            "Epoch 147/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3238 - accuracy: 0.8827\n",
            "Epoch 148/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3398 - accuracy: 0.8827\n",
            "Epoch 149/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3548 - accuracy: 0.8812\n",
            "Epoch 150/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3275 - accuracy: 0.8812\n",
            "Epoch 151/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3430 - accuracy: 0.8812\n",
            "Epoch 152/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3410 - accuracy: 0.8796\n",
            "Epoch 153/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3381 - accuracy: 0.8889\n",
            "Epoch 154/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3292 - accuracy: 0.8920\n",
            "Epoch 155/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3283 - accuracy: 0.8981\n",
            "Epoch 156/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3306 - accuracy: 0.8920\n",
            "Epoch 157/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3419 - accuracy: 0.8812\n",
            "Epoch 158/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3302 - accuracy: 0.8966\n",
            "Epoch 159/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3105 - accuracy: 0.8873\n",
            "Epoch 160/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3467 - accuracy: 0.8750\n",
            "Epoch 161/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3328 - accuracy: 0.8920\n",
            "Epoch 162/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3163 - accuracy: 0.9028\n",
            "Epoch 163/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3278 - accuracy: 0.8873\n",
            "Epoch 164/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3589 - accuracy: 0.8765\n",
            "Epoch 165/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3258 - accuracy: 0.8966\n",
            "Epoch 166/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3053 - accuracy: 0.8873\n",
            "Epoch 167/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3155 - accuracy: 0.8920\n",
            "Epoch 168/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3325 - accuracy: 0.8889\n",
            "Epoch 169/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3372 - accuracy: 0.8796\n",
            "Epoch 170/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3348 - accuracy: 0.8843\n",
            "Epoch 171/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3002 - accuracy: 0.8951\n",
            "Epoch 172/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3299 - accuracy: 0.8920\n",
            "Epoch 173/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3230 - accuracy: 0.8951\n",
            "Epoch 174/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3089 - accuracy: 0.8951\n",
            "Epoch 175/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3125 - accuracy: 0.9059\n",
            "Epoch 176/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3136 - accuracy: 0.8966\n",
            "Epoch 177/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3397 - accuracy: 0.8843\n",
            "Epoch 178/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3030 - accuracy: 0.9059\n",
            "Epoch 179/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3224 - accuracy: 0.8981\n",
            "Epoch 180/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3267 - accuracy: 0.8997\n",
            "Epoch 181/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3133 - accuracy: 0.8873\n",
            "Epoch 182/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3304 - accuracy: 0.8920\n",
            "Epoch 183/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3278 - accuracy: 0.8981\n",
            "Epoch 184/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3160 - accuracy: 0.8843\n",
            "Epoch 185/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3152 - accuracy: 0.9028\n",
            "Epoch 186/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3499 - accuracy: 0.8935\n",
            "Epoch 187/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3201 - accuracy: 0.8981\n",
            "Epoch 188/200\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3326 - accuracy: 0.8873\n",
            "Epoch 189/200\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3158 - accuracy: 0.8904\n",
            "Epoch 190/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3339 - accuracy: 0.8843\n",
            "Epoch 191/200\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3348 - accuracy: 0.8812\n",
            "Epoch 192/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3270 - accuracy: 0.8889\n",
            "Epoch 193/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3322 - accuracy: 0.9012\n",
            "Epoch 194/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3027 - accuracy: 0.8981\n",
            "Epoch 195/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2934 - accuracy: 0.8997\n",
            "Epoch 196/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3275 - accuracy: 0.8889\n",
            "Epoch 197/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3138 - accuracy: 0.8873\n",
            "Epoch 198/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2958 - accuracy: 0.8935\n",
            "Epoch 199/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3109 - accuracy: 0.8873\n",
            "Epoch 200/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3213 - accuracy: 0.8981\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.4612 - accuracy: 0.8732\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "def build_classifier():\n",
        "    classifier = Sequential()\n",
        "    # Adding the input layer and the first hidden layer \n",
        "    classifier.add(Dense(units = 15, activation = 'relu', input_dim = 17))\n",
        "    # Adding a hidden layer\n",
        "    classifier.add(Dense(units = 7, activation = 'relu'))\n",
        "    classifier.add(Dropout(rate = 0.1))\n",
        "    # Adding a hidden layer\n",
        "    classifier.add(Dense(units = 9, activation = 'relu'))\n",
        "    classifier.add(Dropout(rate = 0.1))\n",
        "    # Adding a hidden layer\n",
        "    classifier.add(Dense(units = 7, activation = 'relu'))\n",
        "    classifier.add(Dropout(rate = 0.1))\n",
        "    # Adding the output layer\n",
        "    classifier.add(Dense(units = 4, activation = 'softmax'))\n",
        "    # Compiling the ANN\n",
        "    classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return classifier\n",
        "\n",
        "# Encoding the classes in y\n",
        "y_binary = y\n",
        "\n",
        "cv = KFold(n_splits=10, random_state=42, shuffle=True)\n",
        "classifier = KerasClassifier(build_fn = build_classifier, batch_size=25, epochs=200)\n",
        "acc_cv = cross_val_score(estimator = classifier, X = X, y = y_binary, cv = cv)\n",
        "\n",
        "accuracies[4] = acc_cv.mean()\n",
        "auc_roc[4] = ' '"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "zENAgTkKNcZZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4f86341-2e4d-4dc9-a84e-0fcb25e69c14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLASSIFICATION REPORT for XGBoost classifier... \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.99      0.98        81\n",
            "         1.0       0.75      0.93      0.83        45\n",
            "         2.0       0.60      0.19      0.29        16\n",
            "         3.0       0.00      0.00      0.00         2\n",
            "\n",
            "    accuracy                           0.87       144\n",
            "   macro avg       0.58      0.53      0.52       144\n",
            "weighted avg       0.84      0.87      0.84       144\n",
            "\n",
            "\n",
            "CONFUSION MATRIX for XGBoost classifier... \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[80,  1,  0,  0],\n",
              "       [ 1, 42,  2,  0],\n",
              "       [ 2, 11,  3,  0],\n",
              "       [ 0,  2,  0,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Create a classifier using best params\n",
        "xgb = XGBClassifier(booster='gbtree', objective='multi:softmax', max_depth = 11, n_estimators = 35,\n",
        "                    eta = 0.1, subsample  = 0.7, num_class = 4, random_state=42)\n",
        "# Fit the classifier with the training data\n",
        "xgb.fit(X_train,y_train)\n",
        "# Use trained model to predict output of test dataset\n",
        "y_pred = xgb.predict(X_test)\n",
        "\n",
        "accuracies[5] = accuracy_score(y_test, y_pred)\n",
        "auc_roc[5] = multiclass_roc_auc_score(y_test, y_pred)\n",
        "\n",
        "print('CLASSIFICATION REPORT for XGBoost classifier... \\n')\n",
        "print(classification_report(y_test,y_pred))\n",
        "print('\\nCONFUSION MATRIX for XGBoost classifier... \\n')\n",
        "confusion_matrix(y_test,y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "6lH72PbVNcZa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "b18d7027-b9fa-4c62-8785-801703006a38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d2f55019-b7fc-454b-bc69-a41f08f6af4e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Classifiers</th>\n",
              "      <th>Accuracy score</th>\n",
              "      <th>AUC-ROC Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>0.854167</td>\n",
              "      <td>0.724428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.673611</td>\n",
              "      <td>0.592203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>kNN</td>\n",
              "      <td>0.743056</td>\n",
              "      <td>0.656717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SVM_rbf</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.700449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ANN</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.868056</td>\n",
              "      <td>0.737979</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d2f55019-b7fc-454b-bc69-a41f08f6af4e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d2f55019-b7fc-454b-bc69-a41f08f6af4e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d2f55019-b7fc-454b-bc69-a41f08f6af4e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           Classifiers  Accuracy score AUC-ROC Score\n",
              "0        Random Forest        0.854167      0.724428\n",
              "1  Logistic Regression        0.673611      0.592203\n",
              "2                  kNN        0.743056      0.656717\n",
              "3              SVM_rbf        0.833333      0.700449\n",
              "4                  ANN             NaN              \n",
              "5              XGBoost        0.868056      0.737979"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ],
      "source": [
        "models = {'Classifiers': model_list, 'Accuracy score': accuracies, 'AUC-ROC Score': auc_roc}\n",
        "models_df = pd.DataFrame(models, columns = ['Classifiers', 'Accuracy score', 'AUC-ROC Score'] )\n",
        "models_df"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "ABIDEII_Composite_Phenotypic.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}